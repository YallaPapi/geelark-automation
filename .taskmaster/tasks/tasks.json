{
  "master": {
    "tasks": [
      {
        "id": 11,
        "title": "Project skeleton, configuration, and input/output handling",
        "description": "Set up the Python project, config management, and basic CLI to load inputs (photos, bios, follow list, proxies, API keys) and write CSV output for created accounts.",
        "details": "Implementation details:\n- Use Python 3.11+ with a simple, single-package structure (e.g., `geelark_ig_bot/`).\n- Create `config.py` to load configuration from a `.env` file (using `python-dotenv`) or a `config.yaml` file (using `pyyaml`). Keys: DAISY_SMS_KEY, TWO_CAPTCHA_KEY, ANTHROPIC_KEY, PROXY_ROTATE_URL, GEELARK_DEVICE_ID or connection params, paths for PHOTOS_DIR, BIOS_FILE, FOLLOW_FILE, OUTPUT_CSV.\n- Implement a small `models.py` with dataclasses such as `AccountProfile(photo_path, bio, follow_targets)` and `RunContext(proxy_url, device_id, session_id, logs_path)`.\n- Implement `io_inputs.py`:\n  - Load all image paths from the photos folder (validate file extensions and existence).\n  - Load bios from a text file, one bio per non-empty line.\n  - Load accounts-to-follow from a text file, one username per non-empty line.\n- Implement `io_outputs.py` with function `append_created_account(csv_path, username, password, phone, status, extra=None)` that appends a row; ensure the CSV is created with a header if missing.\n- Implement `main.py` with a CLI (using `argparse`) that supports parameters like `--accounts N`, `--device-id`, `--start-index`, `--output-csv`.\n- Add logging (built-in `logging` module) with INFO for high-level steps and DEBUG for low-level details; log to both console and a rotating file handler.\n- Ensure paths and config values are validated at startup, with clear error messages and non-zero exit codes on failure.\n- Keep architecture minimal: a main loop that calls a `create_single_account(profile: AccountProfile)` function implemented in later tasks.\n\nPseudo-code sketch:\n```python\n# main.py\nfrom config import load_config\nfrom io_inputs import load_photos, load_bios, load_follow_targets\nfrom io_outputs import append_created_account\nfrom workflow import create_single_account\n\nif __name__ == \"__main__\":\n    cfg = load_config()\n    photos = load_photos(cfg.PHOTOS_DIR)\n    bios = load_bios(cfg.BIOS_FILE)\n    follows = load_follow_targets(cfg.FOLLOW_FILE)\n\n    for i in range(cfg.NUM_ACCOUNTS):\n        profile = build_profile(photos, bios, follows, i)\n        result = create_single_account(profile, cfg)\n        append_created_account(\n            cfg.OUTPUT_CSV,\n            result.username,\n            result.password,\n            result.phone,\n            result.status,\n        )\n```",
        "testStrategy": "- Unit test config loading with missing/invalid keys.\n- Unit test input loaders with temporary directories and sample files.\n- Unit test CSV writer: create temp file, append multiple rows, verify header and data.\n- Run a dry-run mode (no device interaction) that uses mock `create_single_account` to verify CLI, logging, and CSV pipeline behave correctly.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Geelark device control abstraction (RPA/ADB/API)",
        "description": "Research and implement a minimal Python abstraction to control Geelark cloud phones (screenshot, tap, type, scroll) using the most reliable available method (RPA, ADB, or API).",
        "details": "Implementation details:\n- Investigate GeeLark’s RPA feature and any documented APIs from their dashboard/help center.[1]\n- Decide on a practical option:\n  - **Option A (preferred, if accessible):** Use GeeLark RPA/Custom tasks via HTTP or WebSocket if they expose an API to trigger actions on a running device (tap, input text, wait), or via a local bridge component.\n  - **Option B:** Connect via ADB over TCP to the cloud phone (if GeeLark exposes an ADB endpoint per phone). Use `adbutils` or `pure-python-adb` for screenshots and input events.\n  - **Option C:** If GeeLark has an official REST API to interact with cloud phones, wrap the relevant endpoints.\n- Define a Python interface `GeelarkDeviceController` in `geelark_device.py` with methods:\n  - `screenshot() -> bytes` (PNG/JPEG data)\n  - `tap(x: int, y: int)`\n  - `type_text(text: str)`\n  - `scroll(direction: Literal[\"up\",\"down\",\"left\",\"right\"], amount: int=500)`\n  - `back()` to press back button\n  - `home()` to go home\n  - `wait(seconds: float)` for simple delays.\n- Implement at least one concrete subclass, e.g., `AdbGeelarkDeviceController` or `RpaGeelarkDeviceController`, depending on what is feasible with GeeLark.\n- Include a simple device discovery/attachment function: `connect_device(device_id_or_host) -> GeelarkDeviceController`.\n- Ensure screenshot capturing is performant (e.g., ADB `exec-out screencap -p`), and images are in a format accepted by Claude Vision.\n\nExample using ADB-style pseudo-code:\n```python\nclass AdbGeelarkDeviceController(GeelarkDeviceController):\n    def __init__(self, serial: str):\n        self.adb = adbutils.AdbDevice(serial=serial)\n\n    def screenshot(self) -> bytes:\n        return self.adb.screencap()\n\n    def tap(self, x, y):\n        self.adb.shell(f\"input tap {x} {y}\")\n\n    def type_text(self, text):\n        safe = text.replace(\" \", \"%s\")\n        self.adb.shell(f\"input text '{safe}'\")\n\n    def scroll(self, direction, amount=500):\n        if direction == \"up\":\n            self.adb.shell(f\"input swipe 500 1000 500 {1000-amount}\")\n        # etc.\n```",
        "testStrategy": "- If ADB is used, test against a local Android emulator: verify that screenshot bytes are non-empty and tapping/types produce visible effects.\n- If GeeLark RPA/API is used, integration test on a disposable cloud phone: tap a known coordinate (e.g., Settings icon) and verify manually.\n- Add a `--test-device` CLI option that runs a quick health-check: take screenshot, tap a test area, log success/failure.\n- Use mocks in unit tests to assert high-level code calls `tap`, `type_text`, etc., with expected parameters.",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Claude Vision screen understanding client",
        "description": "Implement a Python client that sends device screenshots and the current context to Claude Vision, parses its response into actionable steps with coordinates and text.",
        "details": "Implementation details:\n- Use Anthropic’s Python SDK or plain HTTP with API key from config.\n- Define a prompt template that instructs Claude Vision explicitly:\n  - Provide **screen description**.\n  - Provide **next action** in a strict JSON format with fields like `{\"action\": \"tap\"|\"type\"|\"scroll\"|\"done\"|\"wait\",\"coordinates\": {\"x\": int, \"y\": int},\"text\": \"...\", \"reason\": \"...\"}`.\n  - Ask it to always respond with a single JSON object and no extra text.\n  - Instruct it that the goal is to create and fully set up an Instagram account according to the step list (birthday, phone, SMS, username, password, skip optional, photo, bio, creator, follow accounts).\n- Implement `claude_vision.py` with:\n  - `class ClaudeVisionClient:`\n    - `propose_action(image_bytes: bytes, state: dict) -> dict` where `state` includes progress markers (e.g., `has_entered_birthday`, `has_verified_phone`).\n- Implement robust JSON parsing:\n  - Strip any non-JSON prefix/suffix if Claude accidentally adds text.\n  - Validate that required keys exist; if not, log error and request again with a clarifying system message.\n- Include rate limiting/backoff and simple retry for network errors or malformed responses.\n- Maintain a small `state` object that encodes goal progress to share with Claude in the system/user message so it can choose the next step more reliably.\n\nPseudo-code:\n```python\nSYSTEM_PROMPT = \"\"\"You are controlling an Android phone to create a new Instagram account...\"\"\"\n\ndef propose_action(self, img, state):\n    msg = self._build_message(state)\n    resp = self.client.messages.create(\n        model=\"claude-3.5-sonnet\",  # or latest vision-capable model\n        max_tokens=300,\n        temperature=0.1,\n        messages=[\n          {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n          {\"role\": \"user\", \"content\": [\n              {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": base64.b64encode(img).decode()}},\n              {\"type\": \"text\", \"text\": msg},\n          ]},\n        ],\n    )\n    json_str = extract_json(resp)\n    return json.loads(json_str)\n```",
        "testStrategy": "- Unit test prompt-building and JSON parsing with canned Claude-like responses.\n- Add an offline mode that uses a fake vision client returning predetermined actions for known test screenshots to validate the loop without spending API credits.\n- Log each request/response pair to a file (with redaction of secrets) and manually inspect a few runs to ensure action JSON is consistent.",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "DaisySMS integration for phone number and SMS code retrieval",
        "description": "Implement integration with DaisySMS to rent phone numbers and poll for Instagram verification SMS codes.",
        "details": "Implementation details:\n- Review DaisySMS API docs to identify endpoints for:\n  - Requesting a number for a specific service/country.\n  - Checking SMS status and retrieving the code.\n  - Canceling/finishing an activation.\n- Implement `daisysms_client.py` with:\n  - `request_number(service=\"instagram\", country=None) -> Activation` where `Activation` holds `id`, `phone_number`.\n  - `wait_for_sms(activation_id, timeout=300, poll_interval=5) -> str` returning the numeric code.\n  - `cancel_activation(activation_id)` and `finish_activation(activation_id)`.\n- Handle common failure cases: no numbers, timeout waiting for SMS, banned/invalid numbers.\n- Mask phone number in logs for privacy.\n- Provide helper to format phone for entering on the device (e.g., strip `+` if needed, or let Claude decide how to input it given the screenshot).\n\nPseudo-code sketch:\n```python\nclass DaisySmsClient:\n    def request_number(self):\n        # call API, parse JSON\n        return Activation(id=act_id, phone=phone)\n\n    def wait_for_sms(self, act_id, timeout=300):\n        # loop: GET status, parse text, extract 6-digit code via regex\n```",
        "testStrategy": "- Unit test JSON parsing with sample DaisySMS responses.\n- Use a mock HTTP server (e.g., `responses` or `httpretty`) for DaisySMS endpoints to validate retry and timeout behavior.\n- In a staging run, manually request a number and send a test SMS from another phone to verify code extraction logic.\n- Simulate failure modes (no number, timeout, malformed SMS) and confirm the calling workflow handles them gracefully (marks account as failed, logs reason, releases activation).",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "2Captcha integration for solving Instagram captchas",
        "description": "Integrate with 2Captcha to submit Instagram captchas (image or possibly hCaptcha/ReCaptcha) and retrieve solutions when they appear during signup.",
        "details": "Implementation details:\n- Check 2Captcha docs for supported captcha types on Instagram flows (likely image captcha or hCaptcha/ReCaptcha). Implement at least generic image captcha support; leave hooks for sitekey-based captchas if needed.\n- Implement `twocaptcha_client.py` with:\n  - `submit_image_captcha(image_bytes) -> captcha_id`.\n  - `wait_for_solution(captcha_id, timeout=180, poll_interval=5) -> str`.\n- Integrate with the main flow via a simple contract: when Claude identifies a captcha on the screen and indicates an `action: \"captcha\"` (we can define this), capture a high-resolution screenshot and crop if necessary:\n  - Either ask Claude to provide bounding box coordinates, then crop the relevant region before sending to 2Captcha.\n- After receiving the solution string, pass it back to the device using `type_text` or `tap`/`type` sequences as directed by Claude.\n- Implement error handling: if 2Captcha returns an error or times out, mark run as failed and log details.\n\nPseudo-code:\n```python\nclass TwoCaptchaClient:\n    def submit_image_captcha(self, img):\n        # POST multipart/form-data to 2Captcha\n\n    def wait_for_solution(self, cap_id, timeout):\n        # poll /res.php until status=1\n```",
        "testStrategy": "- Unit test polling and response parsing using mocked 2Captcha HTTP endpoints.\n- Manual integration test with a known captcha image to confirm that 2Captcha returns the expected text.\n- Simulate failures such as `ERROR_CAPTCHA_UNSOLVABLE` and ensure workflow either retries with a new captcha or aborts with a clear status.",
        "priority": "medium",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Mobile proxy rotation and network setup",
        "description": "Implement proxy rotation via a mobile proxy rotation URL before each new account and ensure all external calls from the device go through the rotated IP.",
        "details": "Implementation details:\n- Use the provided `PROXY_ROTATE_URL` config: before starting each new account creation, send a simple HTTP GET to this URL and wait a short delay (e.g., 5–10 seconds) for IP to change.\n- If GeeLark supports per-device proxy assignment, ensure the cloud phone is configured to use the mobile proxy; otherwise, rely on proxy at network edge.\n- Implement `proxy.py` with:\n  - `rotate_proxy() -> bool` which returns True on HTTP 2xx, False otherwise.\n- Add logging to record rotation attempts and results.\n- Optionally verify IP change using a cheap `https://api.ipify.org` style service via the device’s browser or host network (config-driven; disabled by default to avoid extra calls).\n- Integrate into `create_single_account` workflow: call `rotate_proxy()` once at the very beginning of each account run.\n\nPseudo-code:\n```python\ndef rotate_proxy(url, timeout=10):\n    try:\n        r = requests.get(url, timeout=timeout)\n        r.raise_for_status()\n        logger.info(\"Proxy rotated\")\n        time.sleep(8)\n        return True\n    except Exception as e:\n        logger.error(f\"Proxy rotation failed: {e}\")\n        return False\n```",
        "testStrategy": "- Unit test `rotate_proxy` with mocked HTTP responses (success, timeout, non-200).\n- In staging, call rotation multiple times and verify IP change manually using an external IP-check service.\n- Add a debug flag to log detected IPs (host-level) before and after rotation for manual verification.",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Core action loop: screenshot → Claude → device actions",
        "description": "Implement the main control loop that repeatedly screenshots the Geelark device, asks Claude Vision what to do, executes the returned action (tap/type/scroll), and tracks progress toward account creation.",
        "details": "Implementation details:\n- Implement `workflow.py` with a function `run_screen_loop(device: GeelarkDeviceController, vision: ClaudeVisionClient, state: dict, max_steps=200) -> state`.\n- Loop behavior:\n  - For each step:\n    - Take screenshot via `device.screenshot()`.\n    - Call `vision.propose_action(image_bytes, state)`.\n    - Parse action JSON and execute:\n      - `action == \"tap\"`: call `device.tap(x, y)`.\n      - `action == \"type\"`: call `device.type_text(text)`.\n      - `action == \"scroll\"`: call `device.scroll(direction, amount)`.\n      - `action == \"wait\"`: call `device.wait(seconds)`.\n      - `action == \"back\"`/`\"home\"`: call corresponding methods.\n      - `action == \"done\"`: break loop and return.\n      - `action == \"captcha\"`: delegate to 2Captcha handler (Task 15) then feed solution back.\n    - Update `state` with any progress hints returned (e.g., `state[\"phase\"] = resp[\"phase\"]`).\n    - Add random small delays (0.5–1.5 s) to mimic human interaction and let UI update.\n- Implement safety guards:\n  - If `max_steps` reached without `done`, mark run as failed.\n  - Detect repeated identical actions (same tap coordinates for many steps) and break to avoid loops.\n- Ensure the state encodes key information for later steps (e.g., whether phone number has been used, SMS verified, username set, account switched to creator, followed 20 accounts).\n\nPseudo-code:\n```python\ndef run_screen_loop(device, vision, state, max_steps=200):\n    for i in range(max_steps):\n        img = device.screenshot()\n        action = vision.propose_action(img, state)\n        if action[\"action\"] == \"done\":\n            state[\"status\"] = \"done\"\n            break\n        execute_action(device, action, state)\n    return state\n```",
        "testStrategy": "- Implement unit tests for `execute_action` using a mock `GeelarkDeviceController` to verify correct calls for each action type.\n- Use an offline fake-vision client (from Task 13 tests) returning a deterministic series of actions to validate that the loop terminates correctly and state progresses.\n- On a test device with Instagram already on a simple form screen, run a short loop and confirm taps and typing correspond roughly to what Claude suggests (manual spot check using logs and video capture).",
        "priority": "high",
        "dependencies": [
          12,
          13,
          15
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Account setup orchestration and Instagram-specific flow",
        "description": "Orchestrate a full Instagram account setup run, coordinating proxy rotation, device control, Claude-driven steps, DaisySMS phone/SMS handling, captchas, and the business logic for username/password, bio, photo, creator switch, and following accounts.",
        "details": "Implementation details:\n- Implement `create_single_account(profile: AccountProfile, cfg) -> AccountResult` in `workflow.py`.\n- High-level sequence:\n  1. Rotate proxy using Task 16.\n  2. Connect to Geelark device (Task 12) and ensure Instagram app is launched (via explicit launch intent or by tapping icon; you can teach Claude to tap the Instagram icon from home screen as part of loop).\n  3. Initialize `state` with:\n     - `target_bio`, `target_photo_path`, `follow_targets`.\n     - Flags: `birthday_entered`, `phone_requested`, `sms_verified`, `username_set`, `password_set`, `creator_switched`, `followed_count`.\n  4. Request DaisySMS number when the flow reaches phone entry stage:\n     - Either pre-request the number before starting, or better, when `state` indicates phone will be needed (e.g., when Claude says \"now enter phone number\").\n     - Store number and activation id in `state`.\n  5. Run `run_screen_loop` until `state[\"status\"] == \"done\"` or error.\n  6. In the loop integration, insert hooks based on `state`:\n     - When a screen expects the phone number, programmatically supply the DaisySMS number (you may give Claude the number in the context so it types it itself).\n     - After submitting phone, start a background `wait_for_sms` and when code is received, provide it to Claude in the next prompt so it can type it.\n     - For username/password, either auto-generate values in Python (e.g., random letters+digits) and provide them to Claude, or let Claude propose them but ensure Python records them in `state` so they can be output to CSV.\n  7. Ensure optional steps (such as contacts, notifications, etc.) are skipped—rely on Claude’s screen understanding but mention this explicitly in the prompt.\n  8. After reaching home feed, direct Claude (via state goal) to:\n     - Add profile photo from gallery: upload `target_photo_path` to the device or ensure the device already has a set of photos (outside of script scope) and instruct Claude accordingly.\n     - Add bio using `target_bio`.\n     - Switch to Creator account via settings (state flag `creator_switched=True` when done).\n     - Follow ~20 accounts from `follow_targets` list (give the list or next target to Claude in context, track `followed_count`).\n- Implement `AccountResult(username, password, phone, status, error_message=None)` dataclass.\n- On any unrecoverable error (DaisySMS/2Captcha failure, loop timeout, device disconnection), set `status=\"failed\"` and include `error_message`.\n",
        "testStrategy": "- Unit test orchestration logic with mocks for DaisySMS, 2Captcha, device controller, and Claude client to ensure correct call ordering and state changes.\n- Implement a dry-run mode that skips actual external calls and produces synthetic `AccountResult` to verify CSV output and control flow.\n- Run an end-to-end test on a single GeeLark device with manual observation, logging all key decisions; verify that a full account is created and appears in Instagram.\n- After a successful single-account run, test a small batch (e.g., 3 accounts) in series to validate that proxy rotation and resource cleanup between runs behave correctly.",
        "priority": "high",
        "dependencies": [
          12,
          13,
          14,
          15,
          16,
          17
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Profile data assignment and username/password generation",
        "description": "Implement logic to map input photos, bios, and follow lists to each new account and deterministically generate secure usernames and passwords.",
        "details": "Implementation details:\n- Implement `profiles.py` with:\n  - `build_profile(photos, bios, follows, index) -> AccountProfile` using round-robin or randomized selection.\n  - `generate_username(index, base=None) -> str` using a configurable pattern (e.g., random adjectives+noun+digits) and allowed Instagram constraints.\n  - `generate_password() -> str` with 12–16 chars including letters, digits, and symbols.\n- Ensure that for each account run, `AccountProfile` includes:\n  - `photo_path`: may be None if fewer photos than accounts; handle gracefully (skip photo step).\n  - `bio`: may be randomly chosen or selected sequentially.\n  - `follow_targets`: either the full list or a subset of ~20 selected per account.\n- Pass generated username and password into `state` to be shared with Claude so it types them when appropriate.\n- Avoid reusing the same username; if Instagram rejects a username, have Claude propose alternatives but keep track in state and update `AccountResult` accordingly.\n\nPseudo-code:\n```python\n@dataclass\nclass AccountProfile:\n    username: str\n    password: str\n    photo_path: Optional[str]\n    bio: Optional[str]\n    follow_targets: list[str]\n```",
        "testStrategy": "- Unit test profile building to ensure fair rotation of bios/photos and correct slicing of follow targets (~20 per account).\n- Unit test username/password generation for uniqueness and complexity constraints.\n- Use a mock Claude client to simulate username rejection; verify that state and `AccountResult` update to the new accepted username.",
        "priority": "medium",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Logging, error handling, and basic scaling for multiple accounts",
        "description": "Harden the script with structured logging, error handling, cleanup between runs, and simple sequential multi-account execution.",
        "details": "Implementation details:\n- Extend logging from Task 11:\n  - Include per-account correlation ID in all logs.\n  - Log key milestones (proxy rotated, number acquired, SMS received, captcha solved, account created, failures).\n- Implement a central exception handler in `main.py` that catches unexpected errors per account, records a failed `AccountResult`, and continues to the next account instead of crashing the whole batch.\n- Add cleanup hooks:\n  - Release DaisySMS activations on error.\n  - Optionally reset Instagram app state between runs (e.g., clear data or log out via Claude instructions at end of run).\n- For scaling:\n  - Keep initial implementation strictly sequential (one account after another) to minimize complexity.\n  - Design the code to allow future parallelization (e.g., by making `create_single_account` stateless other than its arguments and return value), but do not add concurrency yet.\n- Expose a few runtime knobs via CLI/config: `MAX_STEPS`, `SMS_TIMEOUT`, `CAPTCHA_TIMEOUT`, `RETRY_LIMIT`.\n",
        "testStrategy": "- Simulate multiple account runs with mocks where some accounts succeed and others fail; verify that all results are written to CSV and script exits cleanly.\n- Inject failures (e.g., raise exceptions from DaisySMS/2Captcha/Claude clients) and confirm they are caught and logged and do not stop subsequent accounts.\n- Manual multi-account test (2–3 accounts) to verify logs are readable and correlated with account IDs.",
        "priority": "medium",
        "dependencies": [
          18
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-12-10T03:59:28.494Z",
      "updated": "2025-12-10T04:21:12.826Z",
      "description": "Tasks for master context"
    }
  },
  "posting": {
    "tasks": [
      {
        "id": "1",
        "title": "Set up project structure, configuration, and API key management",
        "description": "Initialize a Python project for Geelark Instagram posting automation, including config management for CSV paths, Claude Vision, proxy rotation URL, and 2Captcha keys.",
        "details": "Implementation details:\n- Use Python 3.11+.\n- Create a package structure, e.g. `geelark_ig_bot/` with modules: `config.py`, `csv_io.py`, `geelark_control.py`, `instagram_flow.py`, `logging_utils.py`, `main.py`.\n- Use `python-dotenv` or similar to load secrets from `.env` (ANTHROPIC_API_KEY, CAPTCHA_API_KEY, PROXY_ROTATION_URL, etc.).\n- Define a `Config` dataclass in `config.py` holding: `input_csv_path`, `output_log_csv_path`, `video_root_dir`, `proxy_rotation_url`, `anthropic_api_key`, `captcha_api_key`, `geelark_api_base`, `mvp_mode` (single device vs multi-account).\n- Add a `requirements.txt` including: `requests`, `pandas` or `python-csv` (standard), `python-dotenv`, `anthropic` (official Claude client), and any chosen Geelark control SDK or ADB wrapper.\n- Provide a simple YAML or JSON config file for non-secret settings (file paths, default timeouts, retry counts).\n- Pseudo-code example:\n```python\n# config.py\nfrom dataclasses import dataclass\nimport os\n\n@dataclass\nclass Config:\n    input_csv_path: str\n    output_log_csv_path: str\n    video_root_dir: str\n    proxy_rotation_url: str\n    anthropic_api_key: str\n    captcha_api_key: str | None\n    geelark_api_base: str\n    mvp_mode: bool = True\n\n\ndef load_config() -> Config:\n    return Config(\n        input_csv_path=os.getenv(\"INPUT_CSV\", \"input.csv\"),\n        output_log_csv_path=os.getenv(\"OUTPUT_LOG_CSV\", \"post_log.csv\"),\n        video_root_dir=os.getenv(\"VIDEO_ROOT_DIR\", \"./videos\"),\n        proxy_rotation_url=os.getenv(\"PROXY_ROTATION_URL\", \"\"),\n        anthropic_api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n        captcha_api_key=os.getenv(\"CAPTCHA_API_KEY\"),\n        geelark_api_base=os.getenv(\"GEELARK_API_BASE\", \"http://localhost:8000\"),\n    )\n```",
        "testStrategy": "- Unit test `load_config()` with different environment variable scenarios.\n- Verify that secrets are not hardcoded (only read from env/.env).\n- Run a dry `python -m geelark_ig_bot.main --dry-run` to confirm project imports and config loading work without runtime errors.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:00:11.686Z"
      },
      {
        "id": "2",
        "title": "Implement CSV input parsing and output logging",
        "description": "Create robust utilities to read posting instructions from a CSV and log results to an output CSV log file.",
        "details": "Implementation details:\n- Define required input columns: `account_name`, `video_path`, `caption`.\n- Implement `read_jobs(csv_path: str) -> list[PostJob]` where `PostJob` is a dataclass with `account_name`, `video_path`, `caption`.\n- Validate CSV: check mandatory columns exist; trim whitespace; skip or flag empty rows.\n- Normalize `video_path` by joining with `video_root_dir` if it is not absolute.\n- Implement `append_log_row(log_path, account, video, status, error=None, timestamp=None)` that appends to CSV, creating header if file does not exist.\n- Ensure logs are flushed after every job for crash resilience.\n- Pseudo-code:\n```python\n# csv_io.py\nfrom dataclasses import dataclass\nimport csv, os, datetime\n\n@dataclass\nclass PostJob:\n    account_name: str\n    video_path: str\n    caption: str\n\n\ndef read_jobs(path: str, video_root_dir: str) -> list[PostJob]:\n    jobs = []\n    with open(path, newline='', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            if not row.get('account_name') or not row.get('video_path'):\n                continue\n            vp = row['video_path']\n            if not os.path.isabs(vp):\n                vp = os.path.join(video_root_dir, vp)\n            jobs.append(PostJob(row['account_name'].strip(), vp, row.get('caption', '')))\n    return jobs\n\n\ndef append_log_row(path: str, account: str, video: str, status: str, error: str | None = None):\n    file_exists = os.path.exists(path)\n    with open(path, 'a', newline='', encoding='utf-8') as f:\n        writer = csv.writer(f)\n        if not file_exists:\n            writer.writerow(['timestamp', 'account', 'video', 'status', 'error'])\n        ts = datetime.datetime.utcnow().isoformat()\n        writer.writerow([ts, account, video, status, error or ''])\n```",
        "testStrategy": "- Unit test `read_jobs` with:\n  - Valid CSV.\n  - Missing columns (expect exception or empty list based on design).\n  - Relative vs absolute video paths.\n- Unit test `append_log_row`:\n  - First write creates header.\n  - Subsequent calls append new rows.\n  - Inspect resulting CSV to match expected line count and fields.\n- Perform an end-to-end dry run reading a small sample CSV and writing a sample log.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:00:20.864Z"
      },
      {
        "id": "3",
        "title": "Design Geelark device control abstraction",
        "description": "Create an abstraction layer to control Geelark cloud phones for screenshots, taps, typing, app launching, and file transfer, independent of the underlying mechanism (RPA, ADB, or API).",
        "details": "Implementation details:\n- Define an interface `GeelarkDeviceController` with methods:\n  - `connect(account_name: str) -> DeviceHandle`\n  - `launch_app(device: DeviceHandle, app_id: str)` (e.g. Instagram)\n  - `tap(device, x: int, y: int)`\n  - `type_text(device, text: str)`\n  - `screenshot(device) -> bytes` (PNG/JPEG bytes)\n  - `swipe(device, x1, y1, x2, y2, duration_ms)`\n  - `upload_file(device, local_path: str, remote_path: str) -> str` (returns remote path or URI).\n- Implement an initial MVP adapter that talks to Geelark via whichever is available first (e.g. ADB over TCP or a Geelark HTTP API). For now, define stub methods that raise `NotImplementedError` but with clear signatures.\n- Provide a mapping from `account_name` to `device_id` (config or simple dict) for the MVP single device.\n- Include sensible timeouts and retry wrappers around network calls.\n- Pseudo-code skeleton:\n```python\n# geelark_control.py\nfrom dataclasses import dataclass\n\n@dataclass\nclass DeviceHandle:\n    id: str\n\n\nclass GeelarkDeviceController:\n    def connect(self, account_name: str) -> DeviceHandle:\n        # map account -> device_id (MVP: single device)\n        raise NotImplementedError\n\n    def launch_app(self, device: DeviceHandle, app_id: str):\n        raise NotImplementedError\n\n    def tap(self, device: DeviceHandle, x: int, y: int):\n        raise NotImplementedError\n\n    def type_text(self, device: DeviceHandle, text: str):\n        raise NotImplementedError\n\n    def screenshot(self, device: DeviceHandle) -> bytes:\n        raise NotImplementedError\n\n    def upload_file(self, device: DeviceHandle, local_path: str, remote_path: str) -> str:\n        raise NotImplementedError\n```\n- Later tasks will fill implementations using the chosen low-level mechanism.",
        "testStrategy": "- Unit test that the interface exists and that stub methods raise `NotImplementedError`.\n- Create a fake/mock implementation `MockGeelarkDeviceController` for testing higher-level logic without real devices.\n- Verify that `account_name` to `device_id` mapping works as expected using the MVP single-device configuration.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:00:29.298Z"
      },
      {
        "id": "4",
        "title": "Implement low-level Geelark control (screenshots, taps, typing, file transfer)",
        "description": "Provide a concrete implementation of the Geelark device control abstraction using the chosen RPA/ADB/API mechanism.",
        "details": "Implementation details:\n- Decide a concrete mechanism based on what Geelark exposes:\n  - If Geelark offers an HTTP API: implement calls like `POST /devices/{id}/tap`, `POST /devices/{id}/type`, `GET /devices/{id}/screenshot`, etc.\n  - If using ADB: use `subprocess` to call `adb -s <serial> shell input tap x y`, `input text`, `screencap -p`, and `adb push` for file transfer.\n- Implement `GeelarkDeviceController` methods:\n  - `connect`: resolve `account_name` to a device identifier (e.g. `device_serial`), possibly via config mapping; validate connectivity.\n  - `launch_app`: `adb shell monkey -p com.instagram.android 1` or equivalent API.\n  - `tap`: execute appropriate tap command.\n  - `type_text`: escape special characters for ADB; for longer captions, implement paste via clipboard if device API supports it.\n  - `screenshot`: capture and return raw bytes; ensure correct image format for Claude Vision.\n  - `upload_file`: transfer video from host to device; return the device-side file path.\n- Add minimal rate limiting to avoid overwhelming Geelark/API.\n- Pseudo-code example (ADB-style):\n```python\nimport subprocess, io\n\nclass AdbGeelarkDeviceController(GeelarkDeviceController):\n    def __init__(self, mapping: dict[str, str]):\n        self.mapping = mapping\n\n    def connect(self, account_name: str) -> DeviceHandle:\n        serial = self.mapping.get(account_name) or next(iter(self.mapping.values()))\n        return DeviceHandle(serial)\n\n    def tap(self, device: DeviceHandle, x: int, y: int):\n        subprocess.run([\"adb\", \"-s\", device.id, \"shell\", \"input\", \"tap\", str(x), str(y)], check=True)\n\n    def screenshot(self, device: DeviceHandle) -> bytes:\n        out = subprocess.check_output([\"adb\", \"-s\", device.id, \"exec-out\", \"screencap\", \"-p\"])\n        return out\n\n    def upload_file(self, device: DeviceHandle, local_path: str, remote_path: str) -> str:\n        subprocess.run([\"adb\", \"-s\", device.id, \"push\", local_path, remote_path], check=True)\n        return remote_path\n```",
        "testStrategy": "- If using ADB: run integration tests against a test device or emulator.\n  - Verify `connect` returns a valid handle.\n  - Call `screenshot` and confirm returned bytes decode as an image.\n  - Call `tap` and `type_text` while observing the device screen.\n  - Transfer a small dummy video file and confirm existence on the device.\n- If using HTTP API: use a mock server to validate request payloads, paths, and error handling.\n- Add negative tests: simulate command/API failures and verify that exceptions are raised and propagated up.",
        "priority": "high",
        "dependencies": [
          "3"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:00:38.036Z"
      },
      {
        "id": "5",
        "title": "Integrate proxy rotation before each post",
        "description": "Implement a simple proxy rotation step that hits the configured rotation URL before each posting job.",
        "details": "Implementation details:\n- Add a `rotate_proxy()` function in a `network_utils.py` module.\n- Use `requests.get(config.proxy_rotation_url, timeout=10)` or equivalent; treat non-2xx responses as failures.\n- Add small backoff and retry (e.g. 3 attempts with exponential backoff) because this is a network call.\n- Pseudo-code:\n```python\nimport time, requests\n\ndef rotate_proxy(url: str, retries: int = 3, base_delay: float = 1.0) -> bool:\n    for attempt in range(retries):\n        try:\n            r = requests.get(url, timeout=10)\n            if 200 <= r.status_code < 300:\n                return True\n        except requests.RequestException:\n            pass\n        time.sleep(base_delay * (2 ** attempt))\n    return False\n```\n- Hook `rotate_proxy()` into the main posting loop: call it before connecting to the Geelark device for each row.\n- Log proxy rotation success/failure per job (but continue posting even if rotation fails if that is acceptable per requirements).",
        "testStrategy": "- Unit test `rotate_proxy` using a requests-mock server returning:\n  - 200: expect success on first attempt.\n  - 500: expect retries and final failure.\n  - Network timeout: expect retries and final failure.\n- In an integration-like test, configure a local HTTP server as rotation URL and verify that it is hit once per job in a multi-row CSV.",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:00:56.695Z"
      },
      {
        "id": "6",
        "title": "Implement Claude Vision client for Instagram UI navigation",
        "description": "Create a module that sends device screenshots and minimal context to Claude Vision and receives structured navigation instructions for the Instagram posting flow.",
        "details": "Implementation details:\n- Use the official Anthropic Python SDK (`anthropic` package) and Claude Vision model.\n- Define a `ClaudeNavigator` class with:\n  - `plan_next_action(screenshot_bytes: bytes, context: dict) -> Action` where `Action` is a dataclass describing an operation such as `tap(x,y)`, `type(text)`, `wait(seconds)`, `verify_posted`.\n- Provide a system prompt that explains the device context (Android Instagram app on a cloud phone), the goal (post a Reel/video with a given caption), and a JSON schema for response.\n- Example pseudo-code:\n```python\nfrom anthropic import Anthropic\nimport base64, json\n\n@dataclass\nclass Action:\n    kind: str  # 'tap', 'type', 'wait', 'done', 'error'\n    x: int | None = None\n    y: int | None = None\n    text: str | None = None\n    seconds: float | None = None\n\n\nclass ClaudeNavigator:\n    def __init__(self, api_key: str):\n        self.client = Anthropic(api_key=api_key)\n\n    def plan_next_action(self, screenshot_bytes: bytes, context: dict) -> Action:\n        img_b64 = base64.b64encode(screenshot_bytes).decode('ascii')\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"input_image\",\n                        \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": img_b64},\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": json.dumps(context),\n                    },\n                ],\n            }\n        ]\n        resp = self.client.messages.create(\n            model=\"claude-3-5-sonnet\",  # example vision-capable model\n            max_tokens=512,\n            messages=messages,\n            system=\"You control an Android Instagram app. Respond ONLY with a JSON object describing the next action to create and publish a video post.\",\n        )\n        action_dict = json.loads(resp.content[0].text)\n        return Action(**action_dict)\n```\n- The `context` should include the current step: e.g. `{\"step\": \"open_plus\", \"caption\": \"...\"}`.\n- Keep actions atomic and loop until `kind == 'done'` or an error is detected.",
        "testStrategy": "- Unit test `ClaudeNavigator` parsing: mock Anthropic client responses with known JSON and ensure `Action` is constructed correctly.\n- Add validation on returned actions (e.g. coordinates within screen bounds, non-empty `text` for `type` actions) and test these validators.\n- For manual testing, feed screenshots of Instagram app (from a real device) and confirm that the model returns sensible next-step actions by logging them without executing on device.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:01:05.856Z"
      },
      {
        "id": "7",
        "title": "Orchestrate Instagram posting flow with device control and Claude Vision",
        "description": "Combine CSV jobs, Geelark control, proxy rotation, and Claude Vision navigation to automate the full Instagram posting flow per row, including caption entry and success verification for one device (MVP).",
        "details": "Implementation details:\n- Implement a high-level function `run_post_job(job: PostJob, config: Config, controller: GeelarkDeviceController, navigator: ClaudeNavigator)` that:\n  1) Rotates proxy.\n  2) Connects to the Geelark device for `job.account_name`.\n  3) Ensures the Instagram app is running (`launch_app`).\n  4) Transfers the video file to the device via `upload_file` and records the device path.\n  5) Enters a loop to perform the posting flow:\n     - Take a `screenshot`.\n     - Provide `context` to Claude, including:\n       - `goal`: \"Post the specified video to this Instagram account as a Reel or standard video post.\"\n       - `step_state`: track state such as `{\"video_uploaded\": false, \"caption_pasted\": false}`.\n       - `video_device_path` and `caption`.\n     - Receive `Action` from `ClaudeNavigator`.\n     - Map `Action` to `GeelarkDeviceController` calls (`tap`, `type_text`, etc.).\n     - Track timeouts and max steps (e.g. 30 steps) to avoid infinite loops.\n  6) After `Action.kind == 'done'`, confirm success by having Claude inspect a final screenshot with a `verify_posted` context.\n- Ensure that errors (exceptions, invalid actions, timeouts) raise a `PostJobError` that carries a human-readable message.\n- Pseudo-code skeleton:\n```python\ndef run_post_job(job, config, controller, navigator):\n    rotate_proxy(config.proxy_rotation_url)\n    device = controller.connect(job.account_name)\n    controller.launch_app(device, app_id=\"com.instagram.android\")\n    remote_video_path = controller.upload_file(device, job.video_path, \"/sdcard/Download/post_video.mp4\")\n\n    state = {\"video_uploaded\": False, \"caption_pasted\": False, \"remote_video_path\": remote_video_path}\n    for step in range(30):\n        screenshot = controller.screenshot(device)\n        context = {\"goal\": \"post_video\", \"caption\": job.caption, \"state\": state}\n        action = navigator.plan_next_action(screenshot, context)\n        if action.kind == \"tap\":\n            controller.tap(device, action.x, action.y)\n        elif action.kind == \"type\":\n            controller.type_text(device, action.text)\n        elif action.kind == \"wait\":\n            time.sleep(action.seconds)\n        elif action.kind == \"done\":\n            break\n        else:\n            raise PostJobError(f\"Unknown action: {action.kind}\")\n\n    # final verification screenshot\n    final_shot = controller.screenshot(device)\n    verify_action = navigator.plan_next_action(final_shot, {\"goal\": \"verify_posted\"})\n    if verify_action.kind != \"done\":\n        raise PostJobError(\"Unable to verify post was successful\")\n```\n- Make the orchestrator initially target MVP: one device and single job; then scale to loop over all jobs from CSV in `main.py`.\n- Capture and return a success/failure status and error message to the caller for logging.",
        "testStrategy": "- Implement integration tests in a `--dry-run` mode where `GeelarkDeviceController` is a mock and `ClaudeNavigator` is replaced by a deterministic fake that returns a scripted sequence of actions; verify steps executed in correct order.\n- On a real Geelark device, manually run one job and visually confirm that Instagram opens, video is selected, caption is filled, and post is shared.\n- Test failure paths: simulate `upload_file` failure, invalid actions from navigator, and assert that errors propagate to logging.\n- Verify that the loop stops when max steps are reached and logs an appropriate error.",
        "priority": "high",
        "dependencies": [
          "2",
          "4",
          "5",
          "6"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:01:14.920Z"
      },
      {
        "id": "8",
        "title": "Handle login prompts, captchas, and rate limits",
        "description": "Add edge-case handling for Instagram login requests, captchas via 2Captcha, and rate-limit detection with backoff and retry.",
        "details": "Implementation details:\n- Extend Claude prompts to explicitly ask it to identify when the screen shows:\n  - A login screen.\n  - A captcha challenge.\n  - A rate-limit or \"try again later\" message.\n- In `ClaudeNavigator`, allow an `Action.kind` of `\"login_required\"`, `\"captcha\"`, or `\"rate_limited\"` with additional metadata if needed.\n- Implement logic in the orchestrator:\n  - `login_required`: for MVP, either skip the job and log `login_required`, or if credentials are available in config, allow navigator-guided login by providing `username`/`password` in context.\n  - `captcha`: integrate 2Captcha by:\n    - Taking a screenshot of the captcha area (or whole screen) and sending to 2Captcha's image API.\n    - Polling for the solved text and then issuing `type_text` or `tap` actions accordingly.\n  - `rate_limited`: pause posting for a configurable cooldown (e.g. 10–30 minutes per account/device) before retrying the current job once; if still rate limited, mark as failed and move on.\n- Pseudo-code snippet for 2Captcha integration:\n```python\nimport requests, time\n\nclass CaptchaSolver:\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n\n    def solve_image(self, image_bytes: bytes) -> str:\n        # send\n        resp = requests.post(\"http://2captcha.com/in.php\", data={\n            \"key\": self.api_key,\n            \"method\": \"base64\",\n            \"body\": base64.b64encode(image_bytes).decode('ascii'),\n            \"json\": 1,\n        })\n        captcha_id = resp.json()[\"request\"]\n        # poll result\n        for _ in range(24):\n            r = requests.get(\"http://2captcha.com/res.php\", params={\n                \"key\": self.api_key,\n                \"action\": \"get\",\n                \"id\": captcha_id,\n                \"json\": 1,\n            })\n            data = r.json()\n            if data[\"status\"] == 1:\n                return data[\"request\"]\n            time.sleep(5)\n        raise TimeoutError(\"Captcha solving timed out\")\n```\n- Log all edge-case events distinctly so they can be monitored later.",
        "testStrategy": "- Unit test captcha solver using mocked 2Captcha endpoints with typical success and timeout responses.\n- Extend fake `ClaudeNavigator` in tests to return `login_required`, `captcha`, and `rate_limited` actions and verify that the orchestrator:\n  - For `login_required`, either skips or performs login based on test configuration.\n  - For `captcha`, calls `CaptchaSolver.solve_image` and then attempts to type the solution.\n  - For `rate_limited`, waits the configured cooldown and retries at most once.\n- Manually induce a login-required state on a test account and confirm that it is handled as designed and logged appropriately.",
        "priority": "medium",
        "dependencies": [
          "6",
          "7"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:01:25.731Z"
      },
      {
        "id": "9",
        "title": "Implement main loop, error handling, and structured logging",
        "description": "Create the main entrypoint that iterates over CSV rows, invokes the posting orchestrator per job, and writes structured logs with status, timestamp, and errors.",
        "details": "Implementation details:\n- In `main.py`, implement:\n  - `load_config()`.\n  - Initialize `GeelarkDeviceController`, `ClaudeNavigator`, and optionally `CaptchaSolver`.\n  - Load jobs via `read_jobs(config.input_csv_path, config.video_root_dir)`.\n  - For each job:\n    - Call `run_post_job` inside a `try/except` block.\n    - On success, call `append_log_row(..., status=\"success\")`.\n    - On failure, log `status=\"fail\"` with the exception message.\n- Use Python `logging` module with JSON-ish log format (e.g. `%(asctime)s %(levelname)s %(message)s`) and include job identifiers.\n- Allow CLI flags/env for:\n  - `--mvp` (single job from CSV).\n  - `--max-jobs` to limit for testing.\n- Pseudo-code:\n```python\ndef main():\n    config = load_config()\n    controller = AdbGeelarkDeviceController(mapping=load_account_device_mapping())\n    navigator = ClaudeNavigator(api_key=config.anthropic_api_key)\n    jobs = read_jobs(config.input_csv_path, config.video_root_dir)\n\n    for i, job in enumerate(jobs):\n        try:\n            run_post_job(job, config, controller, navigator)\n            append_log_row(config.output_log_csv_path, job.account_name, job.video_path, \"success\")\n        except Exception as e:\n            append_log_row(config.output_log_csv_path, job.account_name, job.video_path, \"fail\", str(e))\n```\n- Ensure that an exception in one job does not terminate the loop; always continue to next row.\n- Optionally, add a small random delay between jobs to reduce pattern-like behavior and mitigate rate limits.",
        "testStrategy": "- Use a mock controller and navigator to simulate successful and failing jobs; verify that the main loop continues after failures and that the log CSV contains correct rows.\n- Run end-to-end in a test environment with 2–3 dummy jobs, visually inspect logs and confirm that timestamps and statuses are correct.\n- Intentionally raise an exception inside `run_post_job` for one job and confirm that others are still processed.",
        "priority": "high",
        "dependencies": [
          "2",
          "5",
          "7"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:01:36.963Z"
      },
      {
        "id": "10",
        "title": "MVP validation and scaling to multiple accounts/devices",
        "description": "Validate the MVP by successfully posting one video with caption on a single Geelark device, then extend to handle multiple accounts/devices from the spreadsheet.",
        "details": "Implementation details:\n- MVP validation steps:\n  - Configure one `account_name` in the CSV, one `video_path`, and a simple caption.\n  - Map that account to a Geelark device in the controller configuration.\n  - Run the tool and visually confirm that the video is posted with the correct caption.\n  - Confirm that the output log records `success` for this job.\n- Scaling steps:\n  - Extend account-to-device mapping to support many accounts; use a config file like `devices.yaml` with entries `{account_name, device_id}`.\n  - In `connect(account_name)`, look up the correct `device_id` and fall back to a default or raise an error if unmapped.\n  - If Geelark supports parallel control, optionally add a future-ready abstraction to run jobs concurrently (e.g. via a worker pool); for now keep them sequential to minimize complexity.\n  - Ensure that proxy rotation is still called once per job and that rate-limit logic is per account/device.\n- Add documentation (README) describing:\n  - How to prepare the CSV.\n  - How to organize video files.\n  - How to configure API keys and device mappings.\n  - Known edge cases and limitations.",
        "testStrategy": "- For MVP:\n  - Run manual test: verify the real post appears on Instagram from the target account with the expected caption and time.\n  - Check that logs show a single `success` entry with accurate timestamp and video path.\n- For multi-account:\n  - Prepare a CSV with at least 2 accounts mapped to different devices (or sequential runs on same device if that is the Geelark constraint).\n  - Run and verify that each account posts its respective video.\n  - Inspect logs to ensure each row has correct `account`, `video`, and `status`.\n- Perform a small load test with ~10 rows to confirm there are no memory leaks or unhandled exceptions across many iterations.",
        "priority": "medium",
        "dependencies": [
          "4",
          "7",
          "8",
          "9"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:01:47.805Z"
      },
      {
        "id": "11",
        "title": "Fix ADBKeyboard installation on Geelark cloud phones",
        "description": "Pivot to using Appium for Unicode text input on Geelark cloud phones, abandoning the ADBKeyboard approach due to Android 15 incompatibility where the package is hidden at the framework level and cannot be restored via ADB commands.",
        "status": "done",
        "dependencies": [
          "4"
        ],
        "priority": "high",
        "details": "PROBLEM SUMMARY:\n- ADBKeyboard works on Android 12/13 (SDK 31-33) but is blocked on Android 15 (SDK 35) with hidden=true flag at Android framework level\n- All ADB remediation attempts failed: pm uninstall, pm enable, pm unhide, cmd package install-existing all return success but package remains hidden\n- Geelark phones do not provide root access (su returns command not found)\n- ClipboardHelper + KEYCODE_PASTE fallback tested and FAILED - keyboard not visible during paste, text does not appear\n- Only podmindstudio (Android 13) works; reelwisdompod_ and talktrackhub (Android 15) are broken\n\nNEW APPROACH - APPIUM:\nPivot to using Appium for text input, which handles Unicode natively across all Android versions without requiring a custom keyboard IME.\n\nAppium UIAutomator2 driver can:\n- Type text directly into focused fields via send_keys() or mobile:type command\n- Works with Unicode/emojis natively\n- No need for ADBKeyboard, ClipboardHelper, or any IME installation\n- Connects to devices via ADB (same as current setup)\n- Cross-platform Android version support (works on Android 15)\n\nIMPLEMENTATION PLAN:\n1. Set up Appium server (can run locally or on a server)\n   - Install Node.js if not present\n   - npm install -g appium\n   - appium driver install uiautomator2\n\n2. Install Python Appium client:\n   - pip install Appium-Python-Client\n\n3. Connect to Geelark phones via Appium:\n   - Use ADB connection info from Geelark API (same as current flow)\n   - Create Appium driver session with desired capabilities:\n     - platformName: Android\n     - automationName: UiAutomator2\n     - deviceName: {adb_device_id}\n     - noReset: true\n     - appPackage/appActivity for Instagram\n\n4. Update post_reel_smart.py to use Appium:\n   - Create new AppiumController class or add Appium methods to SmartInstagramPoster\n   - Replace type_text() method (lines 225-245) with Appium's send_keys()\n   - Keep ADB for non-typing operations (tap, swipe, screenshot)\n   - Or migrate entirely to Appium for all interactions\n\n5. Testing:\n   - Test on Android 15 device (reelwisdompod_) first\n   - Verify Unicode/emoji typing works correctly\n   - Test full Instagram posting flow\n\nRELEVANT FILES TO MODIFY:\n- post_reel_smart.py: Replace type_text() with Appium-based implementation\n- requirements.txt: Add Appium-Python-Client dependency\n- New file: appium_controller.py (optional, for Appium setup logic)\n\nEXISTING ASSETS:\n- appium-uiautomator2-server.apk already exists in project root\n- package/ directory contains io.appium.settings source (UnicodeIME) but not needed with direct Appium approach\n- ADB connection flow in post_reel_smart.py connect() method can be reused",
        "testStrategy": "- Set up Appium server locally\n- Test Appium connection to reelwisdompod_ (Android 15) device first\n- Create test script that: 1) connects via Appium, 2) opens Instagram, 3) navigates to caption field, 4) types text with emojis using send_keys()\n- Verify text appears correctly in the caption field including Unicode characters and emojis\n- Run full posting flow on Android 15 device\n- Verify same flow still works on Android 13 device (podmindstudio) for backwards compatibility\n- Compare posting success rates before/after migration",
        "subtasks": [
          {
            "id": 3,
            "title": "Complete ADBKeyboard remediation research and document Android 15 blocker",
            "description": "Document the comprehensive ADBKeyboard remediation attempts and confirm that Android 15 hidden=true state is an unresolvable blocker without root access, leading to pivot to Appium.",
            "dependencies": [
              1,
              2
            ],
            "details": "All ADBKeyboard remediation approaches exhausted:\n- pm uninstall/install: Returns success but package remains hidden\n- cmd package install-existing: Returns success but pm path empty\n- pm enable/unhide: Requires root access not available on Geelark\n- Alternative keyboards: Same hidden=true issue affects new installs\n- ClipboardHelper fallback: FAILED - keyboard not visible during paste\n- Root API: Error 43016 indicates phones don't support root\n\nConclusion: ADBKeyboard approach is fundamentally incompatible with Android 15 on Geelark phones. Pivoting to Appium which handles Unicode typing natively without requiring IME installation.",
            "status": "done",
            "testStrategy": "Document all attempted remediation commands and their results. Confirm Android version correlation (SDK 35 = broken, SDK <= 33 = working).",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Set up Appium server and install UiAutomator2 driver",
            "description": "Install and configure Appium server locally with UiAutomator2 driver for Android automation that supports native Unicode text input across all Android versions.",
            "dependencies": [],
            "details": "Installation steps:\n1. Verify Node.js is installed (node --version), install if needed from https://nodejs.org\n2. Install Appium globally: npm install -g appium\n3. Install UiAutomator2 driver: appium driver install uiautomator2\n4. Verify installation: appium driver list (should show uiautomator2)\n5. Start Appium server: appium --allow-insecure chromedriver_autodownload\n6. Verify server is running on http://localhost:4723\n\nServer configuration:\n- Default port: 4723\n- May need to configure ANDROID_HOME environment variable pointing to Android SDK\n- May need to ensure platform-tools (adb) is in PATH\n\nFiles to create:\n- requirements.txt: Add 'Appium-Python-Client>=3.0.0'\n- Optional: appium_setup.py script to verify/start Appium service\n<info added on 2025-12-11T04:22:32.422Z>\nCOMPLETED SETUP STATUS:\n- Appium version: 3.1.2 installed globally via npm\n- UiAutomator2 driver: installed via appium driver install uiautomator2\n- Android SDK: ANDROID_HOME=C:/Users/asus/Downloads/android-sdk with platform-tools symlinked\n- Successfully connected to Geelark cloud phone at 98.98.125.37:20865 running Android 15 (SDK 35)\n- Connection verified via test_appium.py script which captured screenshot (appium_test.png) proving connection works\n- Appium-Python-Client needs to be added to requirements.txt (currently only has python-dotenv, requests, anthropic)\n- Platform version confirmed via driver.capabilities after successful Remote connection to http://127.0.0.1:4723\n</info added on 2025-12-11T04:22:32.422Z>",
            "status": "done",
            "testStrategy": "1) Run 'appium --version' to verify installation\n2) Run 'appium driver list' to verify uiautomator2 is installed\n3) Start Appium server and verify it responds on localhost:4723\n4) Create simple test script that imports appium and verifies client library version",
            "updatedAt": "2025-12-11T04:21:52.916Z",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Appium connection to Geelark cloud phones",
            "description": "Create AppiumController class that connects to Geelark devices via Appium using existing ADB connection info from GeelarkClient, enabling Unicode text input on Android 15.",
            "dependencies": [
              4
            ],
            "details": "Implementation in new file appium_controller.py:\n\nCreate AppiumController class with methods:\n- connect(): Get phone info from GeelarkClient, start phone, enable ADB, connect via Appium with UiAutomator2Options\n- type_text(text): Use driver.switch_to.active_element.send_keys(text) for Unicode support\n- close(): Quit Appium driver session\n\nKey Appium capabilities:\n- platformName: 'Android'\n- automationName: 'UiAutomator2'\n- deviceName: ADB device string (ip:port)\n- noReset: True (preserve app state)\n- newCommandTimeout: 300\n\nIntegration with existing code:\n- Reuse GeelarkClient for phone discovery and ADB setup\n- Reuse ADB connection logic from post_reel_smart.py lines 115-170\n- Add error handling for Appium connection failures",
            "status": "done",
            "testStrategy": "1) Connect to reelwisdompod_ (Android 15) via Appium\n2) Verify driver session is established\n3) Run driver.page_source to confirm UI access\n4) Take screenshot via driver.get_screenshot_as_png()\n5) Verify connection works on both Android 15 and Android 13 devices",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T04:35:32.773Z"
          },
          {
            "id": 6,
            "title": "Update post_reel_smart.py to use Appium for text input",
            "description": "Modify the SmartInstagramPoster class to use Appium's send_keys() for typing captions instead of ADBKeyboard broadcast, while keeping ADB for other operations.",
            "dependencies": [
              4,
              5
            ],
            "details": "Changes to post_reel_smart.py:\n\n1) Add Appium imports at top:\nfrom appium import webdriver\nfrom appium.options.android import UiAutomator2Options\n\n2) Add Appium driver initialization in connect() method\n\n3) Replace type_text() method (lines 225-245) with Appium-based implementation:\n- Use self.appium_driver.switch_to.active_element.send_keys(text)\n- Remove typing_method check since Appium works universally\n- Handle emojis and Unicode natively\n\n4) Add cleanup for Appium driver in disconnect/cleanup\n\n5) Keep existing ADB methods for tap(), swipe(), screenshot, etc.\n\nAlternative: Hybrid approach - try Appium first, fall back to ADBKeyboard if Appium unavailable for Android 13 devices",
            "status": "done",
            "testStrategy": "1) Start Appium server\n2) Run test on Android 15 device (reelwisdompod_) with caption containing emojis\n3) Verify caption appears correctly in Instagram caption field\n4) Run full posting flow and verify success\n5) Run same test on Android 13 device (podmindstudio) for backwards compatibility",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T04:35:36.719Z"
          },
          {
            "id": 7,
            "title": "Add Appium dependencies and update requirements.txt",
            "description": "Add Appium-Python-Client and any other required dependencies to the project requirements file.",
            "dependencies": [],
            "details": "Update requirements.txt to add:\nAppium-Python-Client>=3.0.0\nselenium>=4.0.0\n\nInstallation command: pip install Appium-Python-Client\n\nVerify installation:\nimport appium\nprint(appium.__version__)\n\nNote: Appium-Python-Client depends on selenium, which will be installed automatically.\n\nPreserve existing dependencies:\n- anthropic (for Claude API)\n- requests (for HTTP calls)\n- python-dotenv (for .env loading)",
            "status": "done",
            "testStrategy": "1) Run pip install -r requirements.txt\n2) Verify no dependency conflicts\n3) Test import: python -c \"from appium import webdriver; print('OK')\"\n4) Verify existing imports still work",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T04:22:51.801Z"
          },
          {
            "id": 8,
            "title": "Test full Instagram posting flow with Appium on Android 15",
            "description": "Perform end-to-end testing of the complete Instagram Reel posting workflow using Appium for text input on an Android 15 device to validate the pivot from ADBKeyboard.",
            "dependencies": [
              4,
              5,
              6,
              7
            ],
            "details": "Test procedure:\n\n1) Pre-requisites:\n- Appium server running\n- Android 15 device available (reelwisdompod_ or talktrackhub)\n- Test video file and caption with Unicode/emojis prepared\n\n2) Test execution:\nStart Appium server in terminal 1: appium\nRun posting script in terminal 2: python post_reel_smart.py reelwisdompod_ test_video.mp4 \"Test caption with emojis 🎉✨🔥\"\n\n3) Verification steps:\n- Phone connects successfully\n- Instagram app opens\n- Video upload works (existing ADB-based file transfer)\n- Caption field is focused\n- Appium types caption including emojis correctly\n- Post is shared successfully\n- Verify post appears on Instagram with correct caption\n\n4) Performance comparison:\n- Time to type caption: Appium vs ADBKeyboard\n- Overall posting time\n- Success rate over multiple posts",
            "status": "pending",
            "testStrategy": "1) Execute full posting flow on Android 15 device with emoji-rich caption\n2) Verify caption appears correctly on published post\n3) Repeat test 3-5 times to verify consistency\n4) Test on Android 13 device for backwards compatibility\n5) Run batch_post.py with mix of Android versions to verify multi-device support",
            "parentId": "undefined"
          },
          {
            "id": 1,
            "title": "Research Android package manager ghost package and signature mismatch behaviors (cloud phones)",
            "description": "Investigate how Android handles ghost/orphaned package entries and INSTALL_FAILED_UPDATE_INCOMPATIBLE errors, especially on non-rootable or cloud-hosted devices like Geelark, and document feasible ADB-only remedies.",
            "dependencies": [],
            "details": "Use Perplexity to search Android developer docs, StackOverflow, and XDA for: (1) causes and fixes of INSTALL_FAILED_UPDATE_INCOMPATIBLE when pm uninstall fails; (2) techniques to clear or bypass ghost/orphaned packages without root (e.g., user 0 uninstall, package clear, disabling users, testharness, or resetting app state); (3) behavior differences for system apps vs. user apps in /system/app and /system/priv-app. Summarize which approaches are viable when you only have adb shell and no root, and call out any device-OEM-specific caveats relevant to cloud/virtual devices.\n<info added on 2025-12-11T02:49:23.733Z>\nBased on the codebase analysis and research findings, here is the new text to append:\n\nResearch findings for ADB-only ghost package remediation on Geelark cloud phones:\n\n1) Ghost package removal without root: Use `pm uninstall --user 0 com.android.adbkeyboard` (do NOT use -k flag as it keeps data and leaves ghost state). This removes the package for the current user even when standard pm uninstall fails with DELETE_FAILED_INTERNAL_ERROR.\n\n2) Restoring orphaned system apps: If ADBKeyboard was previously a system app (like on podmindstudio at /system/app/AdbKeyboard/AdbKeyboard.apk), use `cmd package install-existing com.android.adbkeyboard` to restore it from the system image.\n\n3) Alternative for DELETE_FAILED_INTERNAL_ERROR: Try `pm disable-user --user 0 com.android.adbkeyboard` first to disable the ghost entry before attempting uninstall.\n\n4) Detecting ghost packages: Compare output of `pm list packages` (installed) vs `pm list packages -u` (includes uninstalled-but-retained). Packages appearing only in -u output are ghosts.\n\n5) Fallback typing without ADBKeyboard: The codebase already has ClipboardHelper (setup_clipboard_helper.py) which sets clipboard via `am start -n com.geelark.clipboard/.CopyActivity -a com.geelark.clipboard.COPY --es base64 <b64text>`. After setting clipboard, use `input keyevent 279` (KEYCODE_PASTE) to paste content. This approach supports Unicode and emojis without requiring ADBKeyboard.\n\n6) Current setup_adbkeyboard.py (line 102) uses basic `pm uninstall` which fails on ghost packages. Fix requires updating to use `pm uninstall --user 0` approach.\n\nSources: XDA Forums, bayton.org, droidwin.com\n</info added on 2025-12-11T02:49:23.733Z>",
            "status": "done",
            "testStrategy": null,
            "updatedAt": "2025-12-11T02:49:43.751Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Probe Geelark cloud phones for ADBKeyboard package state and system app presence",
            "description": "Systematically inspect all relevant Geelark devices to understand current ADBKeyboard installation state, including ghost entries and potential system app copies.",
            "dependencies": [
              1
            ],
            "details": "On each Geelark phone (podmindstudio, miccliparchive, reelwisdompod_, talktrackhub), run a scripted adb diagnostic sequence: (1) `pm list packages | grep adbkeyboard`; (2) `pm list packages -s` and `-3` to see if it’s system or user; (3) `pm path com.android.adbkeyboard`; (4) `cmd package resolve-activity` and `dumpsys package com.android.adbkeyboard` to detect ghost entries or disabled states; (5) search filesystem for the APK (e.g., `/system/app`, `/system/priv-app`, `/product/app`) using `ls` patterns where allowed; (6) check `settings get secure default_input_method` and `ime list -a` to see if the IME is registered but disabled. Capture outputs in logs per device and infer whether each device has a system app copy, a broken/ghost entry, or no trace at all.\n<info added on 2025-12-11T02:52:30.810Z>\nDiagnosis Results:\n\n1) podmindstudio: INSTALLED and working - System app located at /system/app/AdbKeyboard/AdbKeyboard.apk. IME properly set to com.android.adbkeyboard/.AdbIME. No remediation needed.\n\n2) miccliparchive: GHOST PACKAGE - APK exists in /system/app but package uninstalled for user 0. Current IME set to Google keyboard (com.google.android.inputmethod.latin). Package appears in `pm list packages -u` but not in `pm list packages`. Remediation: Use `cmd package install-existing com.android.adbkeyboard` to restore system app for current user, then set IME.\n\n3) reelwisdompod_: GHOST PACKAGE - APK exists in /system/app but package uninstalled for user 0. IME setting still points to ADBKeyboard but keyboard non-functional since package not installed for user. Remediation: Same as miccliparchive - use `cmd package install-existing com.android.adbkeyboard` to restore.\n\n4) talktrackhub: NOT INSTALLED - Clean slate, no ADBKeyboard APK anywhere on the filesystem. No ghost package entries. Remediation options: (a) Copy APK from podmindstudio via `adb pull/push` and install, or (b) Use clipboard-based text input as fallback.\n\nFix Strategy for setup_adbkeyboard.py: Add detection logic to differentiate ghost package vs clean slate states. For ghost packages (miccliparchive, reelwisdompod_), use `cmd package install-existing com.android.adbkeyboard` instead of standard pm install. For clean installs (talktrackhub), either pull APK from working phone or use local ADBKeyboard.apk with pm install.\n</info added on 2025-12-11T02:52:30.810Z>",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-11T02:52:49.620Z"
          }
        ],
        "updatedAt": "2025-12-12T02:01:58.948Z"
      },
      {
        "id": "12",
        "title": "Investigate and fix empty Appium page_source on Android 15 Instagram sessions",
        "description": "Debug and instrument the Appium-based Android 15 setup so that page_source / dump_ui() returns a non-empty, correctly structured UI hierarchy for the Instagram app, and determine whether issues stem from app launch, hierarchy generation, or XML parsing.",
        "details": "Implementation plan:\n\n1. **Set up a focused Android 15/Appium debug harness**\n- Create a standalone Python script or test module (e.g. `debug/appium_source_debug.py`) that:\n  - Connects to the same Geelark Android 15 device configuration used in Task 11.\n  - Uses the same Appium capabilities (platformName, platformVersion, deviceName/UDID, automationName=UiAutomator2, appPackage/appActivity for Instagram, noReset, etc.).\n  - Logs all capabilities and the Appium server version at startup for reproducibility.\n- Ensure the harness is runnable independently from the main posting flow to speed up iteration.\n\n2. **Compare Appium page_source vs. uiautomator dump formats**\n- Use Appium’s `driver.page_source` and log the raw return value to a file (e.g. `artifacts/appium_source_raw.xml`) for multiple states: before Instagram launch, after launch, and after navigating to a known screen.[2]\n- On the same device and screen, use `adb shell uiautomator dump /sdcard/view.xml && adb pull /sdcard/view.xml artifacts/uiautomator_view.xml` and compare:\n  - Root element tag names and attributes (`hierarchy`, `node`, bounds, text, resource-id, content-desc).\n  - Character encoding and XML declaration.\n  - Presence/absence of expected views (e.g., Instagram home feed, buttons, bottom nav).\n- Document differences in a short markdown note (`docs/appium_vs_uiautomator.md`), highlighting any fields Appium normalizes or omits and confirming that Appium is returning **application hierarchy XML**, not a raw uiautomator dump.[2]\n\n3. **Verify that Instagram is truly launching and in foreground**\n- From the debug harness, add explicit steps:\n  - Call `driver.start_activity(appPackage, appActivity)` (or equivalent) and wait for a few seconds.\n  - Use `adb shell dumpsys window windows | grep -E 'mCurrentFocus|mFocusedApp'` to verify that the Instagram activity is in the foreground; log this output.\n  - Capture a screenshot via Appium (`driver.get_screenshot_as_png()`) and save to `artifacts/instagram_launch.png`; visually confirm the app is open.\n- If Appium connects but Instagram is not foregrounded, log this and add retries/explicit waits (e.g. wait for known accessibility id or resource-id) before calling `page_source`.\n\n4. **Instrument the page_source / dump_ui() call itself**\n- Wrap `driver.page_source` and any `dump_ui()` helper used in Task 11 in a small utility (e.g. `debug/get_hierarchy.py`) that:\n  - Measures call latency.\n  - Catches and logs exceptions.\n  - Logs the length of the returned XML string and the first 500–1000 characters.\n- Add verbose Appium server logging (log level `debug`) for these calls, capturing:\n  - The `Get Page Source` requests and responses.\n  - Any UiAutomator2/Android errors when traversing the hierarchy.\n- If `page_source` returns an empty hierarchy but no exception, investigate whether this is a known limitation with background apps, webviews, or Android 15 specifics.[1][6]\n\n5. **Check for webview / context or invisible-element issues**\n- Enumerate contexts using `driver.contexts` and log them; if a `WEBVIEW_` context exists for Instagram, switch contexts and compare `page_source` results to the native context.\n- Confirm whether the expected elements are off-screen or lazily created (e.g., lists or RecyclerViews)[3]; scroll a small amount and re-fetch `page_source` to see if the hierarchy populates.\n- Ensure that the harness requests **native context** when expecting native XML, and document how Instagram’s UI composition (native vs webview) affects what Appium can see.[4]\n\n6. **Rule out XML parsing issues in our code**\n- If Appium returns non-empty XML but our `dump_ui()` / parser reports no nodes, add unit-level diagnostics:\n  - Create a minimal parser module (e.g. `ui_parsing/xml_utils.py`) that loads the raw Appium XML using both `xml.etree.ElementTree` and `lxml` (if available) to handle any namespace/encoding quirks.\n  - Log any parsing errors, invalid characters, or namespace prefixes.\n  - Add defensive parsing: strip BOMs, normalize encoding to UTF‑8, and handle default namespaces.\n- Implement a small CLI (`python -m ui_parsing.debug_parse artifacts/appium_source_raw.xml`) that prints root tag, number of nodes, and a few sample attributes to quickly validate parsing.\n\n7. **Constrain work to Android 15 devices**\n- Ensure the harness inspects the device’s SDK level from `adb shell getprop ro.build.version.sdk` and asserts it is 35 (Android 15); otherwise, exit with a clear message.\n- If needed, parameterize the target device but keep the scope of this task to documenting and resolving the Android 15 behavior (other OS versions can be future work).\n\n8. **Output and documentation**\n- Produce a short troubleshooting doc `docs/android15_appium_empty_source.md` summarizing:\n  - Root cause(s): app not foregrounded, context mismatch, Android 15 UiAutomator behavior, or XML parsing bug.\n  - The final, recommended way to:\n    - Confirm Instagram is open.\n    - Fetch reliable page source.\n    - Parse and inspect the hierarchy.\n  - Any Appium capabilities or flags that improved results (e.g., waitForIdleTimeout, disableWindowAnimation, etc., if changed).\n- Expose any reusable utilities (e.g., `get_page_source_debug()`, `assert_instagram_foreground()`) in a `debug_utils` module so other tasks (like Task 11 and orchestrator work) can reuse them.\n",
        "testStrategy": "1. **Environment and connectivity sanity checks**\n- Run the debug harness against an Android 15 Geelark device and verify:\n  - Appium session is created without errors.\n  - Device SDK level is detected as 35; the script exits with an error on non‑15 devices.\n\n2. **Instagram launch verification**\n- Execute the harness with Instagram launch enabled and confirm:\n  - `dumpsys window` logs show an Instagram activity in `mCurrentFocus`/`mFocusedApp`.\n  - The saved screenshot clearly shows Instagram in the foreground.\n\n3. **Page source vs uiautomator comparison**\n- On the same screen, generate both `artifacts/appium_source_raw.xml` and `artifacts/uiautomator_view.xml`.\n- Manually inspect or script-compare them to confirm:\n  - Non-empty XML in both files.\n  - Similar numbers of nodes and presence of expected Instagram UI elements.\n\n4. **XML parsing validation**\n- Run the XML parser CLI against `appium_source_raw.xml` and verify it prints:\n  - Correct root element name.\n  - A positive node count (> 0).\n  - At least a few nodes with sensible attributes (e.g., text/resource-id not all empty).\n- Intentionally corrupt the XML file (e.g., truncate it) and confirm the parser reports clear parsing errors instead of silently returning zero nodes.\n\n5. **Context and visibility behavior tests**\n- From the harness, log `driver.contexts` and switch between native and any webview context, calling `page_source` in each and confirming non-empty output where expected.\n- Scroll within Instagram and re-run `page_source`, verifying the hierarchy updates and that elements entering/leaving the visible region appear/disappear from the XML.\n\n6. **Regression guard for empty source condition**\n- Add an automated check in the harness that fails if `page_source` length is below a small threshold (e.g., < 1 KB) while Instagram is reported as foreground.\n- Run the harness multiple times (at least 5) and confirm the check consistently passes on Android 15.\n\n7. **Documentation review**\n- Have a team member follow `docs/android15_appium_empty_source.md` on a fresh environment and verify they can reproduce the debug steps and obtain non-empty page source and parsed node counts without additional help.",
        "status": "done",
        "dependencies": [
          "4",
          "11"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:02:09.371Z"
      },
      {
        "id": "13",
        "title": "Apply Appium stability fixes with extended timeouts and crash recovery",
        "description": "Improve Appium connection reliability in post_reel_smart.py by adding missing timeout capabilities, increasing existing timeouts, implementing phone restart logic for UiAutomator2 crashes, and creating a typed exception for startup failures.",
        "details": "## Implementation Details\n\n### 1. Create typed UiAutomatorStartupError exception (top of file, after imports ~line 35)\n\n```python\nclass UiAutomatorStartupError(Exception):\n    \"\"\"Raised when UiAutomator2 fails to start on the device\"\"\"\n    pass\n```\n\n### 2. Update connect_appium() function (lines 730-763) with new capabilities\n\nAdd these capabilities to the `options` object:\n\n```python\ndef connect_appium(self, retries=3):\n    \"\"\"Connect Appium driver - REQUIRED for automation to work\"\"\"\n    print(\"Connecting Appium driver...\")\n\n    options = UiAutomator2Options()\n    options.platform_name = \"Android\"\n    options.automation_name = \"UiAutomator2\"\n    options.device_name = self.device\n    options.udid = self.device\n    options.no_reset = True\n    options.new_command_timeout = 60\n    \n    # Extended timeouts for stability (Android 15 devices need longer)\n    options.set_capability(\"appium:uiautomator2ServerLaunchTimeout\", 90000)  # NEW: 90s (was missing, defaulted to 30s)\n    options.set_capability(\"appium:uiautomator2ServerInstallTimeout\", 120000)  # INCREASED: 120s (was 60s)\n    options.set_capability(\"appium:adbExecTimeout\", 120000)  # INCREASED: 120s (was 30s)\n    options.set_capability(\"appium:androidDeviceReadyTimeout\", 60000)  # NEW: 60s device ready wait\n\n    last_error = None\n    for attempt in range(retries):\n        try:\n            self.appium_driver = webdriver.Remote(\n                command_executor=APPIUM_SERVER,\n                options=options\n            )\n            platform_ver = self.appium_driver.capabilities.get('platformVersion', 'unknown')\n            print(f\"  Appium connected! (Android {platform_ver})\")\n            return True\n        except Exception as e:\n            last_error = e\n            print(f\"  Appium connection failed (attempt {attempt + 1}/{retries}): {e}\")\n            self.appium_driver = None\n            \n            # Check if UiAutomator2 crashed - may need phone restart\n            if self.is_uiautomator2_crash(e):\n                print(f\"  [RECOVERY] UiAutomator2 crash detected, attempting phone restart...\")\n                self._restart_phone_for_recovery()\n            \n            if attempt < retries - 1:\n                print(f\"  Retrying in 15 seconds...\")  # INCREASED: 15s (was 5s)\n                time.sleep(15)\n\n    # All retries failed - raise typed exception\n    raise UiAutomatorStartupError(f\"Appium connection failed after {retries} attempts: {last_error}\")\n```\n\n### 3. Add phone restart recovery method (new method in SmartInstagramPoster class)\n\nAdd this method after `reconnect_appium()` (around line 84):\n\n```python\ndef _restart_phone_for_recovery(self):\n    \"\"\"Restart the Geelark phone to recover from UiAutomator2 crash\"\"\"\n    if not self.phone_id:\n        print(\"    Cannot restart phone - phone_id not set\")\n        return False\n    \n    try:\n        print(\"    Stopping phone...\")\n        self.client.stop_phone(self.phone_id)\n        time.sleep(5)\n        \n        print(\"    Starting phone...\")\n        self.client.start_phone(self.phone_id)\n        \n        # Wait for phone to boot (similar to connect() logic)\n        print(\"    Waiting for phone to boot...\")\n        for i in range(60):\n            time.sleep(2)\n            status_result = self.client.get_phone_status([self.phone_id])\n            items = status_result.get(\"successDetails\", [])\n            if items and items[0].get(\"status\") == 0:\n                print(f\"    Phone ready after restart! (took ~{(i+1)*2}s)\")\n                break\n            if i % 5 == 0:\n                print(f\"    Booting... ({(i+1)*2}s)\")\n        else:\n            print(\"    Warning: Phone boot timeout after restart\")\n            return False\n        \n        # Re-enable ADB after restart\n        time.sleep(3)\n        print(\"    Re-enabling ADB...\")\n        self.client.enable_adb(self.phone_id)\n        time.sleep(5)\n        \n        # Reconnect ADB\n        adb_info = self.client.get_adb_info(self.phone_id)\n        self.device = f\"{adb_info['ip']}:{adb_info['port']}\"\n        password = adb_info['pwd']\n        \n        import subprocess\n        subprocess.run([ADB_PATH, \"connect\", self.device], capture_output=True)\n        self.adb(f\"glogin {password}\")\n        time.sleep(3)\n        \n        print(\"    Phone restart recovery complete\")\n        return True\n        \n    except Exception as e:\n        print(f\"    Phone restart failed: {e}\")\n        return False\n```\n\n### 4. Update reconnect_appium() to use new exception (line 74-84)\n\n```python\ndef reconnect_appium(self):\n    \"\"\"Reconnect Appium driver after UiAutomator2 crash\"\"\"\n    print(\"  [RECOVERY] Reconnecting Appium driver...\")\n    try:\n        if self.appium_driver:\n            self.appium_driver.quit()\n    except:\n        pass\n    self.appium_driver = None\n    time.sleep(2)\n    try:\n        return self.connect_appium()\n    except UiAutomatorStartupError:\n        # If reconnect also fails, try phone restart\n        if self._restart_phone_for_recovery():\n            return self.connect_appium()\n        raise\n```\n\n### Summary of Changes\n\n| Item | Before | After |\n|------|--------|-------|\n| `uiautomator2ServerLaunchTimeout` | Missing (30s default) | 90000ms |\n| `uiautomator2ServerInstallTimeout` | 60000ms | 120000ms |\n| `adbExecTimeout` | 30000ms | 120000ms |\n| `androidDeviceReadyTimeout` | Missing | 60000ms |\n| Retry sleep | 5s | 15s |\n| Phone restart on crash | Not implemented | Implemented |\n| Typed exception | Generic Exception | UiAutomatorStartupError |\n\n### Files Modified\n- `post_reel_smart.py`: Add exception class, update `connect_appium()`, add `_restart_phone_for_recovery()`, update `reconnect_appium()`",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Tests for Exception Class\n- Verify `UiAutomatorStartupError` can be raised and caught\n- Verify it inherits from `Exception`\n- Verify error message is preserved correctly\n\n### 2. Timeout Configuration Tests\n- Start Appium with a mock device and verify the capabilities are set correctly:\n  - `uiautomator2ServerLaunchTimeout == 90000`\n  - `uiautomator2ServerInstallTimeout == 120000`\n  - `adbExecTimeout == 120000`\n  - `androidDeviceReadyTimeout == 60000`\n- Log the capabilities object before connection to verify values\n\n### 3. Retry Logic Tests\n- Mock Appium connection failures and verify:\n  - Retry happens 3 times\n  - Sleep between retries is 15 seconds (measure with time.time())\n  - `UiAutomatorStartupError` is raised after all retries fail\n\n### 4. Phone Restart Recovery Tests\n- Mock `is_uiautomator2_crash()` to return `True`\n- Verify `_restart_phone_for_recovery()` is called\n- Mock GeelarkClient methods (`stop_phone`, `start_phone`, `get_phone_status`, `enable_adb`, `get_adb_info`)\n- Verify the correct sequence of recovery calls\n\n### 5. Integration Test with Real Device\n```bash\n# Test on a Geelark Android 15 device\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\n\nposter = SmartInstagramPoster('test_phone_name')\nposter.connect()\n\n# Verify capabilities by checking the driver\ncaps = poster.appium_driver.capabilities\nprint(f'Platform: {caps.get(\\\"platformVersion\\\")}')\nprint(f'Appium connected successfully with extended timeouts')\n\nposter.cleanup()\n\"\n```\n\n### 6. Crash Recovery Simulation\n- Force a UiAutomator2 crash by killing the server process\n- Verify the recovery logic kicks in:\n  ```bash\n  # In a separate terminal while test is running:\n  adb shell \"pkill -f uiautomator\"\n  ```\n- Observe that phone restart and Appium reconnection occur\n\n### 7. End-to-End Test\n- Run full posting flow: `python post_reel_smart.py <phone> <video> <caption>`\n- Monitor logs for timeout-related errors\n- Verify no more \"30s timeout\" errors appear\n- Verify successful connection even under slow network conditions\n\n### 8. Regression Testing\n- Run the existing test suite to ensure no regressions\n- Verify `posting_scheduler.py` still works with the updated `connect_appium()`\n- Test with multiple concurrent phones to verify stability",
        "status": "done",
        "dependencies": [
          "11",
          "12"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:02:20.479Z"
      },
      {
        "id": "14",
        "title": "Analyze overnight scheduler run results (Dec 11-12)",
        "description": "Review batch_results_20251211*.csv files and scheduler logs to compute comprehensive metrics including success rates, error patterns, time correlations, and priority fixes needed.",
        "details": "## Implementation Details\n\n### 1. Create analysis script `analyze_scheduler_results.py`\n\n```python\nimport os\nimport csv\nimport json\nfrom datetime import datetime\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple, Optional\nimport statistics\n\nclass SchedulerAnalyzer:\n    def __init__(self, csv_pattern: str = \"batch_results_20251211*.csv\"):\n        self.csv_pattern = csv_pattern\n        self.records = []\n        \n    def load_data(self):\n        \"\"\"Load all matching CSV files\"\"\"\n        import glob\n        for filepath in glob.glob(self.csv_pattern):\n            with open(filepath, 'r', encoding='utf-8') as f:\n                reader = csv.DictReader(f)\n                for row in reader:\n                    row['source_file'] = filepath\n                    row['timestamp_parsed'] = datetime.fromisoformat(row['timestamp']) if row.get('timestamp') else None\n                    self.records.append(row)\n```\n\n### 2. Metric Calculations\n\n#### Success Rate by Account\n```python\ndef success_rate_by_account(self) -> Dict[str, dict]:\n    \"\"\"Calculate success/fail/error counts per phone/account\"\"\"\n    by_account = defaultdict(lambda: {'success': 0, 'failed': 0, 'error': 0, 'total': 0})\n    for r in self.records:\n        account = r.get('phone', 'unknown')\n        status = r.get('status', 'unknown')\n        by_account[account][status] = by_account[account].get(status, 0) + 1\n        by_account[account]['total'] += 1\n    # Calculate rates\n    for acc, data in by_account.items():\n        data['success_rate'] = data['success'] / data['total'] * 100 if data['total'] > 0 else 0\n    return dict(sorted(by_account.items(), key=lambda x: x[1]['success_rate']))\n```\n\n#### Success Rate by Hour\n```python\ndef success_rate_by_hour(self) -> Dict[int, dict]:\n    \"\"\"Calculate success rates grouped by hour of day\"\"\"\n    by_hour = defaultdict(lambda: {'success': 0, 'total': 0})\n    for r in self.records:\n        if r.get('timestamp_parsed'):\n            hour = r['timestamp_parsed'].hour\n            by_hour[hour]['total'] += 1\n            if r.get('status') == 'success':\n                by_hour[hour]['success'] += 1\n    for hour, data in by_hour.items():\n        data['success_rate'] = data['success'] / data['total'] * 100 if data['total'] > 0 else 0\n    return dict(sorted(by_hour.items()))\n```\n\n#### Error Type Classification\n```python\ndef classify_errors(self) -> Dict[str, List[dict]]:\n    \"\"\"Categorize errors by type based on error message patterns\"\"\"\n    error_patterns = {\n        'upload_timeout': ['Upload timeout', 'status: 1'],\n        'uiautomator_crash': ['UiAutomator2', 'instrumentation process is not running', 'crashed'],\n        'adb_timeout': ['timed out after', 'adb.exe'],\n        'connection_failed': ['connection', 'offline', 'refused'],\n        'instagram_blocked': ['action blocked', 'suspended', 'captcha'],\n    }\n    \n    classified = defaultdict(list)\n    for r in self.records:\n        if r.get('status') in ['error', 'failed']:\n            error_msg = r.get('error', '')\n            error_type = 'unknown'\n            for etype, patterns in error_patterns.items():\n                if any(p.lower() in error_msg.lower() for p in patterns):\n                    error_type = etype\n                    break\n            classified[error_type].append(r)\n    return dict(classified)\n```\n\n#### Average Attempts Before Success\n```python\ndef avg_attempts_before_success(self) -> dict:\n    \"\"\"Calculate average attempts needed for successful posts.\n    Requires correlation with scheduler_state.json for attempt tracking.\"\"\"\n    # Load from scheduler_state.json if available\n    state_file = \"scheduler_state.json\"\n    attempts_data = []\n    try:\n        with open(state_file, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        for job in data.get('jobs', []):\n            if job.get('status') == 'success':\n                attempts_data.append(job.get('attempts', 1))\n    except Exception:\n        pass\n    \n    if attempts_data:\n        return {\n            'mean': statistics.mean(attempts_data),\n            'median': statistics.median(attempts_data),\n            'max': max(attempts_data),\n            'samples': len(attempts_data)\n        }\n    return {'error': 'No attempt data available'}\n```\n\n#### Phones with Highest Failure Rates\n```python\ndef phones_by_failure_rate(self, min_attempts: int = 2) -> List[Tuple[str, float, int]]:\n    \"\"\"Return phones sorted by failure rate (highest first)\"\"\"\n    rates = self.success_rate_by_account()\n    failures = []\n    for phone, data in rates.items():\n        if data['total'] >= min_attempts:\n            failure_rate = 100 - data['success_rate']\n            failures.append((phone, failure_rate, data['total']))\n    return sorted(failures, key=lambda x: -x[1])\n```\n\n#### Time Patterns in Failures\n```python\ndef failure_time_patterns(self) -> dict:\n    \"\"\"Analyze when failures occur - time of day, day of week, gaps between attempts\"\"\"\n    failures_by_hour = defaultdict(int)\n    failures_by_minute_bucket = defaultdict(int)  # 10-min buckets\n    \n    for r in self.records:\n        if r.get('status') in ['error', 'failed'] and r.get('timestamp_parsed'):\n            ts = r['timestamp_parsed']\n            failures_by_hour[ts.hour] += 1\n            bucket = ts.hour * 6 + ts.minute // 10\n            failures_by_minute_bucket[bucket] += 1\n    \n    return {\n        'by_hour': dict(failures_by_hour),\n        'peak_failure_hour': max(failures_by_hour.items(), key=lambda x: x[1]) if failures_by_hour else None,\n        'failure_distribution': failures_by_minute_bucket\n    }\n```\n\n#### Video Size Correlation (placeholder - needs video file access)\n```python\ndef video_size_correlation(self, video_folder: str = \"chunk_01c\") -> dict:\n    \"\"\"Correlate video file sizes with success/failure rates.\n    Requires access to video files to get sizes.\"\"\"\n    # Map shortcodes to file sizes\n    shortcode_sizes = {}\n    success_sizes = []\n    fail_sizes = []\n    \n    # Walk video folder to build size map\n    for root, dirs, files in os.walk(video_folder):\n        for f in files:\n            if f.endswith('.mp4'):\n                shortcode = f.replace('.mp4', '')\n                path = os.path.join(root, f)\n                shortcode_sizes[shortcode] = os.path.getsize(path)\n    \n    for r in self.records:\n        shortcode = r.get('shortcode', '')\n        if shortcode in shortcode_sizes:\n            size_mb = shortcode_sizes[shortcode] / (1024 * 1024)\n            if r.get('status') == 'success':\n                success_sizes.append(size_mb)\n            else:\n                fail_sizes.append(size_mb)\n    \n    return {\n        'avg_success_size_mb': statistics.mean(success_sizes) if success_sizes else 0,\n        'avg_fail_size_mb': statistics.mean(fail_sizes) if fail_sizes else 0,\n        'success_samples': len(success_sizes),\n        'fail_samples': len(fail_sizes),\n    }\n```\n\n### 3. Report Generator\n\n```python\ndef generate_report(self) -> str:\n    \"\"\"Generate a comprehensive markdown report\"\"\"\n    report = []\n    report.append(\"# Scheduler Run Analysis Report - Dec 11, 2025\\n\")\n    \n    # Overall stats\n    total = len(self.records)\n    success = sum(1 for r in self.records if r.get('status') == 'success')\n    report.append(f\"## Overall Statistics\")\n    report.append(f\"- Total attempts: {total}\")\n    report.append(f\"- Successful: {success} ({success/total*100:.1f}%)\")\n    report.append(f\"- Failed/Error: {total - success}\")\n    \n    # Add each metric section...\n    # (success by account, by hour, error types, etc.)\n    \n    # Priority Fixes section\n    report.append(\"\\n## Priority Fixes Needed\")\n    errors = self.classify_errors()\n    if errors.get('upload_timeout'):\n        report.append(\"1. **Upload Timeout** - Increase upload timeout beyond 180s or implement chunked upload\")\n    if errors.get('uiautomator_crash'):\n        report.append(\"2. **UiAutomator2 Crashes** - Implement phone restart recovery per Task 13\")\n    \n    return \"\\n\".join(report)\n```\n\n### 4. CLI Interface\n\n```python\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser(description='Analyze scheduler results')\n    parser.add_argument('--date', default='20251211', help='Date pattern YYYYMMDD')\n    parser.add_argument('--output', default='scheduler_analysis_report.md', help='Output report file')\n    parser.add_argument('--json', action='store_true', help='Output raw data as JSON')\n    \n    args = parser.parse_args()\n    \n    analyzer = SchedulerAnalyzer(f\"batch_results_{args.date}*.csv\")\n    analyzer.load_data()\n    \n    if args.json:\n        data = {\n            'by_account': analyzer.success_rate_by_account(),\n            'by_hour': analyzer.success_rate_by_hour(),\n            'errors': analyzer.classify_errors(),\n            'failure_patterns': analyzer.failure_time_patterns(),\n        }\n        print(json.dumps(data, indent=2, default=str))\n    else:\n        report = analyzer.generate_report()\n        with open(args.output, 'w', encoding='utf-8') as f:\n            f.write(report)\n        print(f\"Report saved to {args.output}\")\n```\n\n### 5. Files to Read\n\n- `batch_results_20251211*.csv` - All CSV files from Dec 11 runs\n- `scheduler_state.json` - For attempt counts and job metadata\n- `geelark_batch.log` - For detailed error stack traces and phase timing\n- `chunk_01c/` - Video folder for file size analysis",
        "testStrategy": "## Test Strategy\n\n### 1. Data Loading Tests\n- Verify all Dec 11 CSV files are found and loaded (expect ~14 files based on glob results)\n- Confirm all expected columns are present: shortcode, phone, status, error, timestamp\n- Test handling of empty error fields and malformed timestamps\n\n### 2. Metric Calculation Validation\n- **Success rate by account**: Cross-reference with manual count from sample CSV files\n- **Success rate by hour**: Verify hour extraction from ISO timestamps (e.g., \"2025-12-11T18:22:34\" → hour 18)\n- **Error classification**: Test pattern matching against known error strings:\n  - \"Upload timeout after 180s (last status: 1)\" → upload_timeout\n  - \"UiAutomator2 server...instrumentation process is not running\" → uiautomator_crash\n  - \"timed out after 30 seconds\" → adb_timeout\n\n### 3. Report Verification\n- Run analysis and verify report includes all 7 requested metrics\n- Compare overall success count with sum across all CSVs\n- Verify phones with highest failure rates list shows accounts that appear in error records\n\n### 4. Edge Cases\n- Test with empty CSV files\n- Test with single-record files\n- Test when scheduler_state.json is unavailable or malformed\n- Test when video folder doesn't exist (video size correlation should gracefully report 0 samples)\n\n### 5. Manual Spot-Check\n```bash\n# Quick validation commands\npython analyze_scheduler_results.py --json | jq '.by_account | length'\n# Should return number of unique accounts\n\npython analyze_scheduler_results.py --json | jq '.errors | keys'\n# Should show error type categories found\n```\n\n### 6. Cross-Reference with Raw Data\n- Compare report findings with direct CSV inspection\n- Verify error messages in report match actual error strings from CSVs\n- Confirm time patterns align with file timestamps on batch_results_*.csv files",
        "status": "pending",
        "dependencies": [
          "2",
          "9"
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": "15",
        "title": "Integrate reliability features into posting_scheduler worker loop",
        "description": "Wire up existing but unused reliability mechanisms (Appium health checks, account cooldown backoff) into the scheduler's worker loop, add a heartbeat thread to keep the lock file fresh, and classify infrastructure errors to trigger account-level backoff.",
        "details": "## Current State Analysis\n\nThe codebase already has several reliability features that are **implemented but NOT wired up**:\n\n1. **Single-instance lock** (lines 30-111): Fully working but uses static lock file without heartbeat\n2. **Appium health checks** (lines 114-170): `check_appium_health()` and `restart_appium()` exist but never called\n3. **Account cooldown** (lines 435-471): `is_on_cooldown()` and `record_post(is_infra_error)` exist but:\n   - `is_on_cooldown()` is NOT checked in `get_next_job()` (line 661-703)\n   - `record_post()` is always called with just `False`, never passing `is_infra_error=True` (line 815)\n\n## Implementation Details\n\n### 1. Add Heartbeat Thread for Lock Freshness\n\nUpdate the lock file with a timestamp periodically so other instances can detect truly stale locks:\n\n```python\n# Add to PostingScheduler.__init__()\nself.heartbeat_thread: Optional[threading.Thread] = None\nself.heartbeat_interval = 30  # seconds\n\n# Add heartbeat method\ndef _heartbeat_loop(self):\n    \"\"\"Periodically update lock file to prove we're still alive\"\"\"\n    while self.running:\n        try:\n            if os.path.exists(LOCK_FILE):\n                with open(LOCK_FILE, 'r') as f:\n                    lock_data = json.load(f)\n                if lock_data.get('pid') == os.getpid():\n                    lock_data['last_heartbeat'] = datetime.now().isoformat()\n                    with open(LOCK_FILE, 'w') as f:\n                        json.dump(lock_data, f)\n        except Exception as e:\n            logger.warning(f\"Heartbeat error: {e}\")\n        time.sleep(self.heartbeat_interval)\n```\n\nUpdate `acquire_lock()` to check heartbeat staleness:\n```python\n# In acquire_lock(), after is_process_running check:\nlast_heartbeat = lock_data.get('last_heartbeat')\nif last_heartbeat:\n    hb_time = datetime.fromisoformat(last_heartbeat)\n    stale_threshold = timedelta(minutes=2)  # 2 minutes without heartbeat = stale\n    if datetime.now() - hb_time > stale_threshold:\n        print(f\"[LOCK] Lock heartbeat stale ({hb_time}). Taking over.\")\n        # Proceed to take over\n```\n\n### 2. Integrate Appium Health Check into Worker Loop\n\nIn `_worker_loop()`, before processing a job:\n\n```python\ndef _worker_loop(self):\n    \"\"\"Main worker loop\"\"\"\n    self._log(\"Worker started\")\n    \n    # Track consecutive Appium failures for restart logic\n    appium_consecutive_failures = 0\n    max_appium_failures_before_restart = 3\n    \n    while self.running:\n        if self.paused:\n            time.sleep(1)\n            continue\n        \n        # Check Appium health before each job\n        if not check_appium_health():\n            self._log(\"[APPIUM] Health check failed\")\n            appium_consecutive_failures += 1\n            \n            if appium_consecutive_failures >= max_appium_failures_before_restart:\n                self._log(\"[APPIUM] Attempting auto-restart...\")\n                if restart_appium():\n                    appium_consecutive_failures = 0\n                else:\n                    self._log(\"[APPIUM] Restart failed, waiting 60s...\")\n                    time.sleep(60)\n                    continue\n            else:\n                time.sleep(10)\n                continue\n        else:\n            appium_consecutive_failures = 0  # Reset on success\n        \n        job = self.get_next_job()\n        # ... rest of loop\n```\n\n### 3. Integrate Account Cooldown into get_next_job()\n\nUpdate `get_next_job()` to filter out accounts on cooldown:\n\n```python\ndef get_next_job(self) -> Optional[PostJob]:\n    \"\"\"Get next job that's ready to post\"\"\"\n    accounts_posted_today = get_accounts_posted_today()\n    \n    # Filter: can post today AND not on cooldown\n    available_accounts = [\n        acc for acc in self.accounts.values()\n        if acc.can_post_today(self.posts_per_account_per_day)\n        and acc.name not in accounts_posted_today\n        and not acc.is_on_cooldown()  # ADD THIS LINE\n    ]\n    # ... rest of method unchanged\n```\n\n### 4. Classify Infrastructure Errors in execute_job()\n\nUpdate the error handling in `execute_job()` to detect infrastructure errors:\n\n```python\n# In execute_job(), in the except block (around line 783):\nexcept Exception as e:\n    error_msg = str(e)\n    error_type_name = type(e).__name__\n    \n    # Classify infrastructure errors\n    infra_error_patterns = [\n        'ADB', 'adb', 'device offline', 'glogin', 'phone not running',\n        'Appium', 'appium', 'UiAutomator', 'WebDriver', \n        'connection refused', 'timeout', 'Timeout'\n    ]\n    is_infra_error = any(pattern in error_msg for pattern in infra_error_patterns) or \\\n                     any(pattern in error_type_name for pattern in infra_error_patterns)\n    \n    job.last_error = f\"[{phase}] {error_type_name}: {error_msg}\"\n    \n    # ... existing error handling ...\n    \n    # Pass is_infra_error to trigger backoff\n    self.accounts[job.account].record_post(False, is_infra_error=is_infra_error)\n```\n\n### 5. Start Heartbeat Thread in start()\n\n```python\ndef start(self):\n    \"\"\"Start the scheduler\"\"\"\n    if self.running:\n        return\n    \n    # ... existing phone cleanup ...\n    \n    self.running = True\n    self.paused = False\n    \n    # Start heartbeat thread\n    self.heartbeat_thread = threading.Thread(target=self._heartbeat_loop, daemon=True)\n    self.heartbeat_thread.start()\n    self._log(\"[HEARTBEAT] Started heartbeat thread\")\n    \n    # Start worker thread\n    self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)\n    self.worker_thread.start()\n    self._log(\"Scheduler started\")\n```\n\n### 6. Add Account Cooldown Status to get_stats()\n\n```python\ndef get_stats(self) -> dict:\n    \"\"\"Get current statistics\"\"\"\n    accounts_on_cooldown = [acc.name for acc in self.accounts.values() if acc.is_on_cooldown()]\n    \n    return {\n        # ... existing stats ...\n        'accounts_on_cooldown': accounts_on_cooldown,\n    }\n```\n\n## Files to Modify\n\n- `posting_scheduler.py`: All changes concentrated in this single file",
        "testStrategy": "## Test Strategy\n\n### 1. Single-Instance Lock with Heartbeat Tests\n\n**Test stale lock detection:**\n```bash\n# Create a stale lock file manually\necho '{\"pid\": 99999, \"started\": \"2024-01-01T00:00:00\", \"last_heartbeat\": \"2024-01-01T00:00:00\"}' > scheduler.lock\n\n# Run scheduler - should take over the stale lock\npython posting_scheduler.py --status\n# Expected: \"Lock heartbeat stale\" message, then acquires lock\n```\n\n**Test heartbeat updates:**\n```bash\n# Start scheduler in background\npython posting_scheduler.py --add-folder chunk_test --add-accounts test1 --run &\n\n# Check lock file updates every 30s\nwatch -n 10 'cat scheduler.lock | python -m json.tool | grep last_heartbeat'\n# Expected: last_heartbeat timestamp updates every ~30 seconds\n```\n\n**Test duplicate instance prevention:**\n```bash\n# Terminal 1: Start scheduler\npython posting_scheduler.py --run\n\n# Terminal 2: Try to start another\npython posting_scheduler.py --run\n# Expected: \"[LOCK ERROR] Another scheduler instance is already running!\"\n```\n\n### 2. Appium Health Check Integration Tests\n\n**Test health check detection:**\n```bash\n# Stop Appium server\ntaskkill /F /IM node.exe\n\n# Run scheduler - should detect Appium down\npython posting_scheduler.py --run\n# Expected: \"[APPIUM] Health check failed\" messages\n```\n\n**Test auto-restart:**\n```bash\n# With Appium stopped, scheduler should attempt restart after 3 failures\n# Expected log sequence:\n# [APPIUM] Health check failed (1)\n# [APPIUM] Health check failed (2) \n# [APPIUM] Health check failed (3)\n# [APPIUM] Attempting auto-restart...\n# [APPIUM] Server ready on port 4723\n```\n\n### 3. Account Cooldown Integration Tests\n\n**Test cooldown filtering in get_next_job:**\n```python\n# Unit test\nscheduler = PostingScheduler()\nscheduler.add_account(\"test1\")\nscheduler.accounts[\"test1\"].cooldown_until = (datetime.now() + timedelta(minutes=10)).isoformat()\n\n# get_next_job should not return jobs for test1\njob = scheduler.get_next_job()\nassert job is None or job.account != \"test1\"\n```\n\n**Test infrastructure error classification:**\n```python\n# Simulate infra error in execute_job\n# After 3 consecutive failures, account should be on cooldown\nassert scheduler.accounts[\"test1\"].is_on_cooldown() == True\nassert scheduler.accounts[\"test1\"].consecutive_failures >= 3\n```\n\n### 4. Status Command Verification\n\n```bash\npython posting_scheduler.py --status\n# Expected output includes:\n# - Lock status with last_heartbeat timestamp\n# - Accounts on cooldown list (if any)\n# - Appium health status\n```\n\n### 5. Error Log Verification\n\nAfter a test run with simulated failures:\n```bash\ngrep \"is_infra_error\" geelark_batch.log\n# Should show infrastructure errors being correctly classified\n\ngrep \"on cooldown\" geelark_batch.log  \n# Should show accounts being put on cooldown after consecutive failures\n```\n\n### 6. Integration Test with Real Posting\n\n```bash\n# Run with a small test batch\npython posting_scheduler.py --add-folder chunk_test --add-accounts phone1 --run\n\n# Monitor logs for:\n# 1. Heartbeat updates\n# 2. Appium health checks before each job\n# 3. Proper cooldown behavior if failures occur\n# 4. Clean shutdown releasing lock\n```",
        "status": "done",
        "dependencies": [
          "9",
          "11",
          "13"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-12T01:42:53.832Z"
      },
      {
        "id": "16",
        "title": "Ensure ANDROID_HOME / ANDROID_SDK_ROOT Are Recognized by Appium Server",
        "description": "Make Appium reliably detect the Android SDK by standardizing how ANDROID_HOME and ANDROID_SDK_ROOT are set, exported, and propagated into the Appium server process across all deployment environments.",
        "details": "## Goal\nGuarantee that when the Appium server is started (locally, via scripts, or inside workers/containers), it always has valid access to the Android SDK through **ANDROID_HOME** and/or **ANDROID_SDK_ROOT**, so errors like “Neither ANDROID_HOME nor ANDROID_SDK_ROOT environment variable was exported” do not occur.[7][8]\n\n## High-Level Approach\n1. **Standardize environment variable configuration** for Android SDK on all supported OSes (Linux/macOS; Windows only if relevant).\n2. **Ensure variables are set in a *non-interactive* context** (systemd services, cron, Docker, background workers), not just in interactive shells.[7]\n3. **Unify Appium startup** through a single entry point (Python helper or shell script) that validates and, if needed, sets or maps ANDROID_SDK_ROOT/ANDROID_HOME before launching the server.\n4. **Add diagnostics** so misconfiguration is obvious in logs.\n\n## Implementation Steps\n\n### 1. Discover Current SDK Paths and Usage\n- Inspect how Appium is currently started:\n  - Python wrapper (e.g., `post_reel_smart.py` / scheduler), direct `appium` CLI, Docker, or a service unit.\n  - Note whether `appium` is started via `subprocess` in Python.\n- On at least one working dev machine and one production-like host:\n  - Run `echo $ANDROID_HOME` and `echo $ANDROID_SDK_ROOT` (or `set` on Windows) to see what is set.[4][6]\n  - Run `sdkmanager --list` from the same shell that starts Appium to confirm SDK accessibility.\n  - If using Android Studio, open SDK Manager and capture the **Android SDK Location** to use as canonical ANDROID_HOME.[5][6]\n\n### 2. Standard OS-Level Environment Setup (Best Practices)\nFollow current best-practice patterns for SDK env configuration so that Appium’s CLI sees them by default.[2][4][5][6]\n\n**Linux/macOS:**\n- In the system or service user profile, set (example):\n  ```bash\n  export ANDROID_HOME=\"$HOME/Android/Sdk\"\n  export ANDROID_SDK_ROOT=\"$ANDROID_HOME\"\n  export PATH=\"$PATH:$ANDROID_HOME/emulator:$ANDROID_HOME/platform-tools:$ANDROID_HOME/tools:$ANDROID_HOME/tools/bin\"\n  ```[4][5][6]\n- Add to the appropriate file for non-interactive shells (e.g., `/etc/profile.d/android-sdk.sh` or the service user’s `.profile`), not just `.bashrc`.\n\n**Windows (if used for Appium host):**\n- In *System Properties → Environment Variables*:\n  - Add **ANDROID_HOME** and/or **ANDROID_SDK_ROOT** pointing to the SDK directory (e.g., `C:\\Users\\<User>\\AppData\\Local\\Android\\Sdk`).[2][3][5][6]\n  - Add to **PATH**:\n    - `%ANDROID_HOME%\\emulator`\n    - `%ANDROID_HOME%\\platform-tools`\n    - `%ANDROID_HOME%\\tools`\n    - `%ANDROID_HOME%\\tools\\bin`[2][5]\n- Reboot or restart relevant services after setting system variables.[5]\n\nDocument the canonical SDK path and env configuration in `docs/appium_env.md` so all environments can be made consistent.\n\n### 3. Central Appium Launcher With Env Validation\nCreate a central launcher responsible for starting Appium with a guaranteed-good environment.\n\n**Option A – Shell wrapper (for CLI/containers):**\n- Add a script `scripts/start_appium.sh`:\n  ```bash\n  #!/usr/bin/env bash\n  set -euo pipefail\n\n  # 1. Infer or normalize SDK env\n  if [[ -z \"${ANDROID_HOME:-}\" && -n \"${ANDROID_SDK_ROOT:-}\" ]]; then\n    export ANDROID_HOME=\"$ANDROID_SDK_ROOT\"\n  elif [[ -z \"${ANDROID_SDK_ROOT:-}\" && -n \"${ANDROID_HOME:-}\" ]]; then\n    export ANDROID_SDK_ROOT=\"$ANDROID_HOME\"\n  fi\n\n  # 2. Fallback: attempt to detect SDK in common locations (optional)\n  if [[ -z \"${ANDROID_HOME:-}\" ]]; then\n    for candidate in \"$HOME/Android/Sdk\" \\\n                    \"$HOME/Library/Android/sdk\" \\\n                    \"/usr/local/android-sdk\"; do\n      if [[ -d \"$candidate/platform-tools\" ]]; then\n        export ANDROID_HOME=\"$candidate\"\n        export ANDROID_SDK_ROOT=\"$candidate\"\n        break\n      fi\n    done\n  fi\n\n  # 3. Validate\n  if [[ -z \"${ANDROID_HOME:-}\" || ! -d \"$ANDROID_HOME/platform-tools\" ]]; then\n    echo \"[FATAL] ANDROID_HOME/ANDROID_SDK_ROOT not set or invalid. Please install Android SDK and configure env vars.\" >&2\n    exit 1\n  fi\n\n  export PATH=\"$PATH:$ANDROID_HOME/emulator:$ANDROID_HOME/platform-tools:$ANDROID_HOME/tools:$ANDROID_HOME/tools/bin\"\n\n  echo \"[INFO] Using ANDROID_HOME=$ANDROID_HOME\" >&2\n  echo \"[INFO] Using ANDROID_SDK_ROOT=${ANDROID_SDK_ROOT:-$ANDROID_HOME}\" >&2\n\n  # 4. Finally run Appium\n  exec appium \"$@\"\n  ```\n- Ensure all automation (scheduler, local dev docs, CI, systemd unit) uses this script instead of invoking `appium` directly.\n\n**Option B – Python-side launcher (if Appium is started from Python):**\n- Implement a helper (e.g., in a shared module `appium_env.py`):\n  ```python\n  import os\n  import shutil\n  import subprocess\n\n  class AndroidEnvError(RuntimeError):\n      pass\n\n  def ensure_android_env() -> dict:\n      env = os.environ.copy()\n      home = env.get(\"ANDROID_HOME\")\n      root = env.get(\"ANDROID_SDK_ROOT\")\n\n      if not home and root:\n          home = root\n          env[\"ANDROID_HOME\"] = root\n      elif not root and home:\n          root = home\n          env[\"ANDROID_SDK_ROOT\"] = home\n\n      if not home:\n          # Optional: probe common locations\n          for candidate in [\n              os.path.expanduser(\"~/Android/Sdk\"),\n              os.path.expanduser(\"~/Library/Android/sdk\"),\n              \"/usr/local/android-sdk\",\n          ]:\n              if os.path.isdir(os.path.join(candidate, \"platform-tools\")):\n                  home = root = candidate\n                  env[\"ANDROID_HOME\"] = candidate\n                  env[\"ANDROID_SDK_ROOT\"] = candidate\n                  break\n\n      if not home or not os.path.isdir(os.path.join(home, \"platform-tools\")):\n          raise AndroidEnvError(\n              \"ANDROID_HOME/ANDROID_SDK_ROOT not set or invalid; install Android SDK and configure env vars.\"\n          )\n\n      pt = os.path.join(home, \"platform-tools\")\n      emulator = os.path.join(home, \"emulator\")\n      tools = os.path.join(home, \"tools\")\n      tools_bin = os.path.join(tools, \"bin\")\n      extra = os.pathsep.join(p for p in [pt, emulator, tools, tools_bin] if os.path.isdir(p))\n      if extra:\n          env[\"PATH\"] = env.get(\"PATH\", \"\") + os.pathsep + extra\n\n      return env\n\n  def start_appium_server(args: list[str]) -> subprocess.Popen:\n      env = ensure_android_env()\n      appium_cmd = shutil.which(\"appium\") or \"appium\"\n      return subprocess.Popen([appium_cmd, *args], env=env)\n  ```\n- Refactor all places that start Appium (e.g., utilities used by Task 11 and 13 flows) to use `start_appium_server` instead of raw `subprocess.Popen`.\n\n### 4. Integrate with Existing Reliability / Health Logic\n- In the same place where Appium health checks and restarts are wired (Task 15) and connection stability is being improved (Task 13), ensure the restart path *also* uses the standardized launcher so restarted servers see the correct env.\n- When an Appium startup or health check fails due to env problems (e.g., server logs mention missing `adb` or ANDROID_HOME), log a distinct error code / message so future analysis (Task 14) can differentiate env configuration problems from device/Appium bugs.\n\n### 5. Diagnostics and Logging\n- At Appium startup, log the detected **ANDROID_HOME**, **ANDROID_SDK_ROOT**, and whether `adb` is found on PATH (e.g., `which adb` / `where adb`).\n- Optionally, run a lightweight `adb version` and `adb devices` check immediately after starting the server and log the output to quickly spot SDK vs. device issues.[4][6]\n- Update developer/ops documentation with:\n  - Required env vars and their purpose.\n  - Example configuration snippets for each OS.\n  - How to run `appium-doctor --android` to validate setup before running tests.[1][2][6]\n\n### 6. CI / Container Integration (If Applicable)\n- For Docker images, bake the SDK and env variables into the image:\n  ```dockerfile\n  ENV ANDROID_HOME=/opt/android-sdk \\\n      ANDROID_SDK_ROOT=/opt/android-sdk\n  ENV PATH=\"$PATH:$ANDROID_HOME/emulator:$ANDROID_HOME/platform-tools:$ANDROID_HOME/tools:$ANDROID_HOME/tools/bin\"\n  ```\n- Ensure the CI job that runs mobile tests uses either the shell or Python launcher above.\n\n## Notes / Best Practices\n- Prefer **ANDROID_SDK_ROOT** (more modern) but keep **ANDROID_HOME** for compatibility; set both to the same directory.[7]\n- Always ensure that at least `platform-tools` and `emulator` are on PATH for Appium Android testing.[2][4][5][6]\n- When changing system environment variables on Windows, restart services or the whole machine so Appium inherits them.[5]\n",
        "testStrategy": "1. **Env Sanity Checks**\n- On each supported OS:\n  - Open a shell configured the same way the Appium server is started (service user, CI container, or scheduler process).\n  - Run `echo $ANDROID_HOME` / `echo $ANDROID_SDK_ROOT` (or `set ANDROID_` on Windows) and confirm they point to the actual SDK directory.\n  - Run `adb version` and confirm it succeeds.\n  - Run `appium-doctor --android` and verify there are no Android SDK-related errors.[1][2][6]\n\n2. **Launcher-Level Tests (Shell Wrapper)**\n- Temporarily unset ANDROID_HOME/ANDROID_SDK_ROOT, then:\n  - Create a mock SDK directory at a common default path with a dummy `platform-tools` folder.\n  - Run `scripts/start_appium.sh --log-level debug` and confirm:\n    - The script discovers the SDK and sets ANDROID_HOME/ANDROID_SDK_ROOT (check printed logs).\n    - `adb` from the mock SDK is picked up (check `which adb` output if added).\n- Set only ANDROID_HOME and confirm the wrapper mirrors it to ANDROID_SDK_ROOT and logs both.\n- Set only ANDROID_SDK_ROOT and confirm the wrapper mirrors it to ANDROID_HOME.\n- Intentionally point ANDROID_HOME to a non-existent directory and verify the script exits non‑zero with a clear fatal error message.\n\n3. **Launcher-Level Tests (Python Helper, if implemented)**\n- Unit-test `ensure_android_env()` using `monkeypatch`/`os.environ` manipulation:\n  - Case: both vars absent, no SDK dirs → expect `AndroidEnvError`.\n  - Case: only ANDROID_HOME set → expect ANDROID_SDK_ROOT to be added and PATH extended.\n  - Case: only ANDROID_SDK_ROOT set → expect ANDROID_HOME to be added and PATH extended.\n  - Case: neither set but a test SDK directory exists in a probed path → expect both vars to be set to that directory.\n- Unit-test `start_appium_server()` by stubbing `subprocess.Popen` and asserting it receives an `env` with properly set ANDROID_HOME/ANDROID_SDK_ROOT and PATH.\n\n4. **Integration Test with Appium and Device**\n- From the worker/scheduler context that will run real jobs:\n  - Start Appium using the new launcher (shell or Python).\n  - Check the Appium server logs to confirm:\n    - ANDROID_HOME/ANDROID_SDK_ROOT values are logged as expected.\n    - No warnings like \"Neither ANDROID_HOME nor ANDROID_SDK_ROOT environment variable was exported\" appear.[8]\n  - Run a minimal Android session (e.g., from Task 11’s test harness):\n    - Create an Appium session to a real or cloud Android device.\n    - Verify the session initializes, `adb devices` lists the device, and a simple `driver.get_page_source()` succeeds.\n\n5. **Failure-Mode Regression Test**\n- Temporarily misconfigure env (e.g., unset ANDROID_HOME in the service config) and start Appium through the new launcher:\n  - Confirm the launcher fails fast with a clear error instead of starting a broken server.\n  - Ensure higher-level reliability/health logic (from Task 13 and Task 15) logs an explicit env-configuration error category and does not enter an infinite restart loop.\n\n6. **Documentation Validation**\n- Follow the updated `docs/appium_env.md` from a clean machine:\n  - Configure the SDK and env exactly as documented.\n  - Start Appium using the documented command.\n  - Confirm that an Android session can be created without additional manual tweaks, demonstrating the docs are accurate and sufficient.",
        "status": "done",
        "dependencies": [
          "11",
          "13"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-12T01:54:53.017Z"
      },
      {
        "id": "17",
        "title": "Reduce UiAutomator2 launch timeout from 90s to 10s",
        "description": "Optimize the Appium UiAutomator2 server launch timeout based on the observation that instrumentation either starts immediately (~1s) or times out completely - there is no middle ground, so waiting 90 seconds on failure wastes time unnecessarily.",
        "details": "## Background\n\nAnalysis documented in `geelark_uiautomator2_timeout_report.txt` reveals a **binary behavior pattern** for UiAutomator2 initialization on Geelark cloud phones:\n\n- **Success case**: Instrumentation starts in ~1 second (observed: 1104ms)\n- **Failure case**: Times out after the full timeout period (previously 90s)\n- **No middle ground**: There are no cases where initialization takes 30s, 50s, or any intermediate time\n\nThis means the previous 90-second timeout was wasteful - if UiAutomator2 doesn't start within a few seconds, it won't start at all until retry.\n\n## Implementation\n\nIn `post_reel_smart.py`, update the `connect_appium()` method (around line 743):\n\n### Before (Task 13 implementation):\n```python\noptions.set_capability(\"appium:uiautomator2ServerLaunchTimeout\", 90000)  # 90s\n```\n\n### After:\n```python\noptions.set_capability(\"appium:uiautomator2ServerLaunchTimeout\", 10000)  # 10s for launch - binary: works in ~1s or not at all\n```\n\n## Rationale\n\n1. **Time savings**: Failed attempts now waste 10s instead of 90s (80s saved per failure)\n2. **Faster retry cycle**: With 15s delay between retries, a full 3-attempt cycle takes:\n   - Before: 90s + 15s + 90s + 15s + 90s = 300s (5 minutes)\n   - After: 10s + 15s + 10s + 15s + 10s = 60s (1 minute)\n3. **No false negatives**: 10s is still generous given the observed ~1s success time\n4. **Buffer for edge cases**: 10s provides 10x buffer over the ~1s typical success time\n\n## Other timeouts remain unchanged\n\nThe following timeouts in `connect_appium()` should NOT be reduced as they serve different purposes:\n\n- `newCommandTimeout: 120` - For slow cloud phone operations during the session\n- `adbExecTimeout: 120000` - For slow ADB commands over network tunnels\n- `uiautomator2ServerInstallTimeout: 120000` - First-time APK installation can be slow\n- `androidDeviceReadyTimeout: 60` - Device boot/ready detection\n\nOnly `uiautomator2ServerLaunchTimeout` exhibits the binary behavior pattern.",
        "testStrategy": "## Test Strategy\n\n### 1. Verify timeout value is correctly set\n\nRun a quick Appium session and check the capabilities:\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nposter = SmartInstagramPoster('test_phone')\n# Connect to a phone and check capabilities\nposter.connect()\ncaps = poster.appium_driver.capabilities\nprint(f'Launch timeout: {caps.get(\\\"uiautomator2ServerLaunchTimeout\\\", \\\"not set\\\")}')\nposter.cleanup()\n\"\n```\n\n### 2. Timing verification on success\n\nStart a cloud phone and time the Appium connection:\n```bash\ntime python -c \"\nfrom post_reel_smart import SmartInstagramPoster\nposter = SmartInstagramPoster('podclipcrafters')\nposter.connect()\nprint('Connected successfully')\nposter.cleanup()\n\"\n```\n\nExpected: Total connection time should be well under 60 seconds on success.\n\n### 3. Timing verification on failure\n\nSimulate a failure scenario by connecting to an invalid device:\n```bash\ntimeout 20 python -c \"\nfrom appium import webdriver\nfrom appium.options.android import UiAutomator2Options\noptions = UiAutomator2Options()\noptions.device_name = 'invalid:12345'\noptions.udid = 'invalid:12345'\noptions.set_capability('appium:uiautomator2ServerLaunchTimeout', 10000)\ndriver = webdriver.Remote('http://127.0.0.1:4723', options=options)\n\" 2>&1 | grep -i timeout\n```\n\nExpected: Should timeout within ~15 seconds (10s timeout + overhead), not 90+ seconds.\n\n### 4. Full retry cycle timing\n\nRun a posting operation to a phone that may have intermittent connectivity:\n```bash\ntime python posting_scheduler.py --add-accounts podclipcrafters --add-folder test_videos --run --max-accounts 1\n```\n\nMonitor `geelark_batch.log` for retry timing. Expected:\n- If first attempt fails, retry should start within 25-30 seconds (10s timeout + 15s delay)\n- Full 3-attempt cycle should complete in under 2 minutes even with all failures\n\n### 5. Regression test - no false negatives\n\nRun the scheduler on 5-10 phones overnight and compare:\n- Success rate should remain the same or improve (not decrease)\n- Average time per attempt should decrease significantly",
        "status": "done",
        "dependencies": [
          "13"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:04:02.231Z"
      },
      {
        "id": "18",
        "title": "Fix Appium Connection Failures with Device-Ready Checks and Thread Cleanup",
        "description": "Resolve Appium connection instability by ensuring device readiness before glogin execution, fixing ThreadPoolExecutor cleanup to prevent orphaned sessions, and managing stale ADB connections through robust connect() flow improvements.",
        "details": "Implement comprehensive fixes for the three identified Appium connection failure root causes following Appium best practices for ADB stability and resource management[1][2][6].\n\n## 1. Device Readiness Check Before glogin (Primary Fix)\n\n**Current Problem**: glogin executes before ADB reports device as 'device' status, causing connection failures[1].\n\n**Implementation**:\n```python\n# In connect_appium() or connect() flow (~lines 730+ from Task 13)\nimport subprocess\nimport time\n\nfrom typing import Optional\n\ndef wait_for_device_ready(udid: str, timeout: int = 60) -> bool:\n    \"\"\"Wait for device to report 'device' status in ADB\"\"\"\n    start_time = time.time()\n    while time.time() - start_time < timeout:\n        result = subprocess.run(\n            ['adb', '-s', udid, 'get-state'],\n            capture_output=True, text=True, timeout=5\n        )\n        if result.returncode == 0 and 'device' in result.stdout.strip().lower():\n            return True\n        time.sleep(2)\n    return False\n\ndef safe_glogin(udid: str) -> None:\n    \"\"\"Only run glogin after device is confirmed ready\"\"\"\n    if not wait_for_device_ready(udid):\n        raise UiAutomatorStartupError(f\"Device {udid} never reached 'device' state\")\n    # Run glogin subprocess here (existing logic)\n    subprocess.run(['glogin', udid], check=True)\n```\n\n**Integration**: Call `safe_glogin(self.device_udid)` **before** Appium driver initialization in `connect_appium()`.\n\n## 2. Fix/Remove ThreadPoolExecutor Wrapper\n\n**Current Problem**: ThreadPoolExecutor timeouts leave orphaned Appium sessions/threads[6].\n\n**Best Practice**: Use context managers for guaranteed cleanup. Remove ThreadPoolExecutor wrapper entirely[6].\n\n**Implementation**:\n```python\n# REPLACE ThreadPoolExecutor wrapper pattern with direct context-managed sessions\n\nclass AppiumSessionManager:\n    def __enter__(self):\n        self.driver = self.connect_appium(retries=3)\n        return self.driver\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Guaranteed cleanup - even on exceptions/timeouts\"\"\"\n        if hasattr(self, 'driver') and self.driver:\n            try:\n                self.driver.quit()\n                # Force ADB session cleanup\n                subprocess.run(['adb', 'kill-server'])\n                subprocess.run(['adb', 'start-server'])\n            except Exception as e:\n                logger.warning(f\"Cleanup failed: {e}\")\n\n# Usage in posting logic:\nwith AppiumSessionManager() as driver:\n    # All automation here\n    pass  # Auto-cleanup guaranteed\n```\n\n## 3. Stale ADB Connection Management\n\n**Implementation**:\n```python\ndef refresh_adb_connection(udid: Optional[str] = None) -> None:\n    \"\"\"Kill/restart ADB server to clear stale connections[1][2]\"\"\"\n    subprocess.run(['adb', 'kill-server'])\n    time.sleep(2)\n    subprocess.run(['adb', 'start-server'])\n    if udid:\n        # Wait for specific device\n        wait_for_device_ready(udid)\n\n# Call refresh_adb_connection() at start of connect_appium() and on UiAutomatorStartupError\n```\n\n## 4. Updated connect_appium() Flow\n```python\ndef connect_appium(self, retries=3):\n    for attempt in range(retries):\n        try:\n            refresh_adb_connection(self.device_udid)\n            safe_glogin(self.device_udid)\n            \n            # Existing Appium connection logic with 10s UiAutomator2 timeout (Task 17)\n            options = UiAutomator2Options()\n            options.set_capability('uiautomator2ServerLaunchTimeout', 10000)  # 10s\n            self.driver = u2.connect(options)\n            return self.driver\n        except (UiAutomatorStartupError, Exception) as e:\n            logger.warning(f\"Attempt {attempt+1} failed: {e}\")\n            if attempt == retries - 1:\n                raise\n            time.sleep(5)\n```\n\n**Dependencies**: Builds directly on Task 13 (connect_appium() structure, UiAutomatorStartupError), Task 17 (10s timeout), Task 16 (ADB env vars).[1][2]",
        "testStrategy": "**Comprehensive Test Strategy** (Critical for production stability)\n\n### 1. Device Readiness Tests\n```bash\n# Test 1: Simulate offline→online transition\nadb disconnect <udid>\nsleep 5\n# Start device connection\npython -m pytest test_appium_connect.py::test_wait_device_ready\n```\n- Verify `wait_for_device_ready()` polls correctly\n- Confirm `safe_glogin()` blocks until 'device' state\n- Test 60s timeout raises `UiAutomatorStartupError`\n\n### 2. ThreadPoolExecutor Replacement Tests\n- Create unit test simulating timeout during session\n```python\nfrom contextlib import contextmanager\n\n@contextmanager\ndef failing_appium():\n    yield\n    raise TimeoutError(\"Simulated timeout\")\n\n# Verify __exit__ still executes cleanup\n```\n- Confirm `driver.quit()` and `adb kill-server` called even on exceptions\n\n### 3. End-to-End Connection Tests\n```bash\n# Test full connect() flow 50x\nfor i in {1..50}; do\n    python test_appium_stability.py --device <udid> || echo \"FAIL $i\"\ndone\n```\n**Success Criteria**:\n- 100% success rate (0 connection failures)\n- No orphaned Appium processes (`ps aux | grep appium`)\n- No stale ADB connections (`adb devices -l` shows clean list)\n\n### 4. ADB Stale Connection Tests\n```bash\n# Force stale connections\nadb kill-server\nadb start-server\n# Run multiple parallel sessions\npytest test_adb_cleanup.py -n auto\n```\n- Verify `refresh_adb_connection()` restores clean state\n- Confirm no 'offline' devices after cleanup\n\n### 5. Integration with Existing Codebase\n- Run full scheduler loop (Task 15) for 2+ hours\n- Monitor `batch_results_*.csv` for zero Appium connection errors\n- Validate no lockfile/heartbeat issues (Task 15)\n\n**Tools**:\n- `lsof -i :5037` (ADB port conflicts)\n- `ps aux | grep -E 'appium|glogin'` (orphaned processes)\n- Appium logs with `--log-level debug`",
        "status": "done",
        "dependencies": [
          "13",
          "16",
          "17"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T03:15:41.147Z"
      },
      {
        "id": "19",
        "title": "Investigate and Fix Appium Multi-Instance Crashes in Parallel Posting Scheduler",
        "description": "Research, design, and implement a robust parallel Appium execution strategy so that running 2+ concurrent sessions for the posting scheduler does not trigger UiAutomator2 instrumentation startup timeouts or cross-instance interference.",
        "details": "## Goals\n- Understand why **2+ simultaneous Appium sessions** cause `instrumentation process cannot be initialized within 10000ms` / UiAutomator2 startup failures when the posting scheduler runs in parallel.\n- Implement a **multi-instance–safe Appium orchestration layer** (ports, ADB, capabilities, process isolation, and retry policy) for stable parallel phone automation.\n- Ensure the design is compatible with existing connection improvements from Tasks 13, 16, 17, and 18.\n\n## Research & Design\n1. **Gather context from current codebase**\n   - Review `post_reel_smart.py`, the scheduler worker code, and any Appium orchestration helpers to understand:\n     - How drivers are created (where `connect_appium` / `connect()` is called).\n     - How many workers can run concurrently and how phones are assigned.\n     - Whether **multiple sessions can target the same device** or if there is a 1:1 mapping.\n     - Current handling of **ports, systemPort, wdaLocalPort (if iOS in future), and Appium server endpoints**.\n   - Document the current assumptions about **single-instance vs multi-instance** operation.\n\n2. **Research Appium parallel best practices (Android focus)**\n   - Summarize current recommendations for **parallel Android execution**:\n     - Use **one Appium server instance per device** or, if sharing a server, ensure unique `systemPort` / bootstrap ports to avoid conflicts.[2][5][7]\n     - Avoid having **two sessions on the same device at the same time**; enforce a per-device session lock.[3][7]\n     - Ensure **separate tmp/log directories** per server instance to avoid file-lock races.[1][5]\n     - Use delays or staggered startup when opening multiple sessions to avoid concurrent heavy ADB load and connection resets.[2][6][8]\n   - Check guidance around UiAutomator2 startup issues and instrumentation conflicts when multiple devices are attached and multiple sessions start simultaneously.[2][6]\n   - Record concrete recommendations (ports, capabilities, server process layout) to feed into implementation.\n\n3. **Identify likely root causes for the 10s instrumentation timeout in parallel**\n   - Hypothesize (and later validate) major categories:\n     - **Port and resource conflicts**:\n       - Multiple sessions using the same `systemPort` or appium internal ports.[2][5][7]\n       - Shared Appium server instance not isolating device traffic properly under parallel load.[3][6]\n     - **ADB contention**:\n       - Multiple heavy `adb shell` / `uiautomator` launches racing at startup causing delays beyond 10s limit.[2][6]\n     - **Process lifecycle and cleanup**:\n       - Orphaned UiAutomator2 / instrumentation processes from prior sessions after crashes (even after Task 18) causing subsequent startups to hang.\n     - **Scheduler-level concurrency**:\n       - All workers attempting `connect()` at exactly the same time on cold devices, amplifying the timing window for instrumentation startup.\n   - Plan to validate each via logs and targeted experiments.\n\n## Implementation Plan\n\n### 1. Introduce a Parallel-Oriented Session Orchestrator\n- Create a module/class, e.g. `parallel_appium_manager.py`, responsible for **safe multi-instance coordination**:\n  - Maintain a **registry of phones** with:\n    - Device identifier/UDID.\n    - Appium server endpoint (host, port).\n    - Unique `systemPort` (and any other per-device ports) if sharing a server.\n    - Current session status (idle, connecting, busy, error).\n  - Provide methods:\n    - `acquire_device(for_job_id)` – atomically reserves a device for a worker.\n    - `release_device(device_id, status)` – releases and optionally marks health.\n    - `connect_driver(device_id, retries=...)` – wrapper around existing `connect()` that adds **parallel-aware policies**:\n      - Enforce **one active session per device**.\n      - Apply a **startup jitter** (small randomized sleep) when multiple workers start concurrently.\n      - Limit the number of **simultaneous fresh instrumentation startups** (e.g., at most N devices doing initial UiAutomator2 startup at once).\n\n### 2. Port & Server Layout Hardening\n- Decide and implement one of these strategies (config-driven so it can be changed later):\n  1. **One Appium server per device** (recommended for simplicity and isolation):[2][5][7]\n     - Ensure each server is started with:\n       - Unique `--port`.\n       - Unique `--tmp` directory per device.[1]\n     - Keep or add a tiny **Appium server manager** that:\n       - Starts/stops per-device servers.\n       - Ensures environment (`ANDROID_HOME` from Task 16) is correctly set.\n  2. **Single Appium server, multiple devices**:\n     - Guarantee **unique `systemPort` per device** in desired capabilities to avoid port collision.[2][5][7]\n     - Persist this mapping somewhere (config file or code constant) and validate on startup.\n- In both cases:\n  - Make the **per-device port configuration explicit** (no implicit defaults).\n  - Fail fast on startup if two devices would share the same `systemPort` or Appium port.\n\n### 3. Strengthen UiAutomator2 Startup & Timeout Handling for Parallelism\n- Build on Task 13 and Task 17:\n  - Keep the **10s UiAutomator2 timeout** but add **parallel-aware retry logic**:\n    - On `instrumentation process cannot be initialized within 10000ms`:\n      - Immediately perform a targeted **cleanup routine** on that device:\n        - Kill stray `uiautomator` / `io.appium.uiautomator2.server` processes (via `adb shell ps` + `kill` or `am force-stop`).\n        - Clear any stuck instrumentation states.\n      - Backoff with jitter (e.g., 2–5 seconds) before retrying to reduce synchronized retries.\n    - Limit number of retries and, after failure, mark device as **temporarily unhealthy** for the scheduler.\n  - Ensure any instrumentation failure raises the existing `UiAutomatorStartupError`, but now tagged with **device id and parallel context** (e.g., number of concurrent connections at that time) for observability.\n\n### 4. Device-Ready & ADB Stability in Parallel (Builds on Task 18)\n- Reuse and extend the **device-ready checks** and **ADB connect() flow improvements** from Task 18 for multi-instance:\n  - Before starting UiAutomator2 on a device, assert:\n    - Device is in `device` state.\n    - `adb -s <udid> shell echo ping` returns quickly (sanity check).\n  - If multiple devices are going from `offline` to `device` simultaneously:\n    - Randomize or sequence **Appium session creation** so not all devices attempt heavy instrumentation launch at the same instant.\n- Add logging/metrics per device:\n  - Time-to-ready: from scheduler worker start → device-ready → instrumentation ready.\n  - Count and rate of instrumentation timeouts under parallel load.\n\n### 5. Scheduler Integration & Concurrency Controls\n- Integrate the new orchestrator with the **posting_scheduler worker loop** (Task 15):\n  - Ensure that **each worker acquires a device lock** from `parallel_appium_manager` before calling `SmartInstagramPoster.connect()`.\n  - Guarantee no worker is allowed to open a second session on a device that is already in use.\n  - If the device pool is exhausted, define behavior:\n    - Either block until device frees up, or\n    - Immediately requeue / reschedule the job.\n- Add **configurable limits**:\n  - `MAX_PARALLEL_DEVICES` – how many devices the scheduler tries to use at the same time.\n  - `MAX_SIMULTANEOUS_STARTUPS` – cap number of devices that can be in `connecting` state concurrently.\n\n### 6. Observability & Diagnostics\n- Enhance logs to make parallel issues diagnosable:\n  - Include **device id, appium port, systemPort, worker id, and session id** in all connection-related logs.\n  - Log explicit messages on:\n    - Session acquisition/release.\n    - Instrumentation startup, timeout, and retries.\n    - ADB cleanup operations.\n- Optionally add a lightweight **debug script**:\n  - `debug/run_parallel_connect_test.py` that spins up N parallel connection attempts (without running full posting logic) to reproduce and stress-test parallel startup behavior.\n\n## Non-Goals / Constraints\n- Do not attempt to support **multiple sessions on the same physical device** concurrently; enforce one active session per device as a hard invariant, in line with Appium/WDA expectations.[3][7]\n- Avoid introducing global sleeps; prefer **bounded jitter + limits** to maintain throughput.\n",
        "testStrategy": "### 1. Unit & Integration Tests for Orchestrator\n1. Write unit tests for `parallel_appium_manager`:\n   - **Device lock behavior**:\n     - Simulate multiple threads calling `acquire_device()`; assert only one obtains the same device at a time.\n   - **Port configuration validation**:\n     - Provide a configuration with duplicate `systemPort`; assert initialization fails fast with clear error.\n   - **Health marking**:\n     - Simulate a device failing to connect repeatedly; verify it gets marked unhealthy and is not handed out to new workers.\n\n2. Add integration tests (can be slow / optional in CI):\n   - Use a small number of mock or stubbed devices to simulate the scheduler requesting devices in parallel and verify that no concurrent session is created for the same device.\n\n### 2. Parallel Connection Load Tests (On Real or Cloud Devices)\n1. **Two-device baseline test**:\n   - Attach 2 Android devices (or cloud device slots).\n   - Start 2 scheduler workers, each configured to use a different device via the orchestrator.\n   - Run at least 20–30 short posting jobs.\n   - Verify:\n     - No `instrumentation process cannot be initialized within 10000ms` errors.\n     - No cross-device interference (e.g., logs for device A show only device A’s UDID and ports).\n\n2. **Scaled parallel test (3–4 devices, if available)**:\n   - Configure 3–4 devices with unique Appium ports and `systemPort` values.\n   - Run the debug script `run_parallel_connect_test.py` to:\n     - Start N parallel connection attempts.\n     - Repeat several times.\n   - Collect metrics:\n     - Connection success rate.\n     - Average and max time from connect start to session ready.\n     - Count of retries and any instrumentation timeouts.\n\n3. **Cold-start storm test**:\n   - Power-cycle or adb-reboot all devices.\n   - Immediately start the scheduler with multiple workers so all try to run at once.\n   - Confirm that:\n     - Startup jitter and `MAX_SIMULTANEOUS_STARTUPS` prevent mass failures.\n     - Timeouts, if any, are contained and recovered via retry logic.\n\n### 3. Instrumentation Timeout Handling Tests\n1. **Forced timeout simulation** (where possible):\n   - Temporarily configure an unrealistically low UiAutomator2 timeout to force failures, or inject a stub around instrumentation start to simulate a timeout.\n   - Verify that:\n     - Cleanup routine runs (killing stray processes),\n     - Retry is attempted with backoff,\n     - After configured retries, device is marked unhealthy and surfaced to logs.\n\n2. **Regression tests vs single-instance behavior**:\n   - Run the scheduler in **single-device mode** (only one device configured) and ensure:\n     - No regression in connection reliability vs results from Tasks 13, 17, and 18.\n\n### 4. Scheduler-Level End-to-End Tests\n1. **End-to-end posting test, 2+ workers**:\n   - Configure the scheduler with 2–3 workers and 2–3 devices.\n   - Run a realistic batch of posting jobs.\n   - Verify:\n     - All jobs complete or fail only for non-infrastructure reasons (e.g., account problems),\n     - No long hangs at the first post,\n     - Logs show orderly acquire→connect→post→release flow per device.\n\n2. **Failure-injection tests**:\n   - Manually disconnect a device mid-run (e.g., `adb disconnect` or physically unplug, if safe) and observe:\n     - The affected session fails fast with clear logs,\n     - Other devices continue working without instrumentation crashes,\n     - The device is not re-used until it is healthy again.\n\n3. **Metrics/log review**:\n   - After each test run, inspect logs to ensure:\n     - Per-device port mapping is consistent.\n     - No overlapping sessions per device.\n     - Instrumentation timeout count is zero or within expected bounds and always followed by proper retry/cleanup.\n",
        "status": "done",
        "dependencies": [
          "13",
          "16",
          "17",
          "18"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T06:05:31.230Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-12-12T06:05:31.231Z",
      "taskCount": 19,
      "completedCount": 18,
      "tags": [
        "posting"
      ]
    }
  }
}
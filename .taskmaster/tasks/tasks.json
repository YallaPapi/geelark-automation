{
  "master": {
    "tasks": [
      {
        "id": 11,
        "title": "Project skeleton, configuration, and input/output handling",
        "description": "Set up the Python project, config management, and basic CLI to load inputs (photos, bios, follow list, proxies, API keys) and write CSV output for created accounts.",
        "details": "Implementation details:\n- Use Python 3.11+ with a simple, single-package structure (e.g., `geelark_ig_bot/`).\n- Create `config.py` to load configuration from a `.env` file (using `python-dotenv`) or a `config.yaml` file (using `pyyaml`). Keys: DAISY_SMS_KEY, TWO_CAPTCHA_KEY, ANTHROPIC_KEY, PROXY_ROTATE_URL, GEELARK_DEVICE_ID or connection params, paths for PHOTOS_DIR, BIOS_FILE, FOLLOW_FILE, OUTPUT_CSV.\n- Implement a small `models.py` with dataclasses such as `AccountProfile(photo_path, bio, follow_targets)` and `RunContext(proxy_url, device_id, session_id, logs_path)`.\n- Implement `io_inputs.py`:\n  - Load all image paths from the photos folder (validate file extensions and existence).\n  - Load bios from a text file, one bio per non-empty line.\n  - Load accounts-to-follow from a text file, one username per non-empty line.\n- Implement `io_outputs.py` with function `append_created_account(csv_path, username, password, phone, status, extra=None)` that appends a row; ensure the CSV is created with a header if missing.\n- Implement `main.py` with a CLI (using `argparse`) that supports parameters like `--accounts N`, `--device-id`, `--start-index`, `--output-csv`.\n- Add logging (built-in `logging` module) with INFO for high-level steps and DEBUG for low-level details; log to both console and a rotating file handler.\n- Ensure paths and config values are validated at startup, with clear error messages and non-zero exit codes on failure.\n- Keep architecture minimal: a main loop that calls a `create_single_account(profile: AccountProfile)` function implemented in later tasks.\n\nPseudo-code sketch:\n```python\n# main.py\nfrom config import load_config\nfrom io_inputs import load_photos, load_bios, load_follow_targets\nfrom io_outputs import append_created_account\nfrom workflow import create_single_account\n\nif __name__ == \"__main__\":\n    cfg = load_config()\n    photos = load_photos(cfg.PHOTOS_DIR)\n    bios = load_bios(cfg.BIOS_FILE)\n    follows = load_follow_targets(cfg.FOLLOW_FILE)\n\n    for i in range(cfg.NUM_ACCOUNTS):\n        profile = build_profile(photos, bios, follows, i)\n        result = create_single_account(profile, cfg)\n        append_created_account(\n            cfg.OUTPUT_CSV,\n            result.username,\n            result.password,\n            result.phone,\n            result.status,\n        )\n```",
        "testStrategy": "- Unit test config loading with missing/invalid keys.\n- Unit test input loaders with temporary directories and sample files.\n- Unit test CSV writer: create temp file, append multiple rows, verify header and data.\n- Run a dry-run mode (no device interaction) that uses mock `create_single_account` to verify CLI, logging, and CSV pipeline behave correctly.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Geelark device control abstraction (RPA/ADB/API)",
        "description": "Research and implement a minimal Python abstraction to control Geelark cloud phones (screenshot, tap, type, scroll) using the most reliable available method (RPA, ADB, or API).",
        "details": "Implementation details:\n- Investigate GeeLark’s RPA feature and any documented APIs from their dashboard/help center.[1]\n- Decide on a practical option:\n  - **Option A (preferred, if accessible):** Use GeeLark RPA/Custom tasks via HTTP or WebSocket if they expose an API to trigger actions on a running device (tap, input text, wait), or via a local bridge component.\n  - **Option B:** Connect via ADB over TCP to the cloud phone (if GeeLark exposes an ADB endpoint per phone). Use `adbutils` or `pure-python-adb` for screenshots and input events.\n  - **Option C:** If GeeLark has an official REST API to interact with cloud phones, wrap the relevant endpoints.\n- Define a Python interface `GeelarkDeviceController` in `geelark_device.py` with methods:\n  - `screenshot() -> bytes` (PNG/JPEG data)\n  - `tap(x: int, y: int)`\n  - `type_text(text: str)`\n  - `scroll(direction: Literal[\"up\",\"down\",\"left\",\"right\"], amount: int=500)`\n  - `back()` to press back button\n  - `home()` to go home\n  - `wait(seconds: float)` for simple delays.\n- Implement at least one concrete subclass, e.g., `AdbGeelarkDeviceController` or `RpaGeelarkDeviceController`, depending on what is feasible with GeeLark.\n- Include a simple device discovery/attachment function: `connect_device(device_id_or_host) -> GeelarkDeviceController`.\n- Ensure screenshot capturing is performant (e.g., ADB `exec-out screencap -p`), and images are in a format accepted by Claude Vision.\n\nExample using ADB-style pseudo-code:\n```python\nclass AdbGeelarkDeviceController(GeelarkDeviceController):\n    def __init__(self, serial: str):\n        self.adb = adbutils.AdbDevice(serial=serial)\n\n    def screenshot(self) -> bytes:\n        return self.adb.screencap()\n\n    def tap(self, x, y):\n        self.adb.shell(f\"input tap {x} {y}\")\n\n    def type_text(self, text):\n        safe = text.replace(\" \", \"%s\")\n        self.adb.shell(f\"input text '{safe}'\")\n\n    def scroll(self, direction, amount=500):\n        if direction == \"up\":\n            self.adb.shell(f\"input swipe 500 1000 500 {1000-amount}\")\n        # etc.\n```",
        "testStrategy": "- If ADB is used, test against a local Android emulator: verify that screenshot bytes are non-empty and tapping/types produce visible effects.\n- If GeeLark RPA/API is used, integration test on a disposable cloud phone: tap a known coordinate (e.g., Settings icon) and verify manually.\n- Add a `--test-device` CLI option that runs a quick health-check: take screenshot, tap a test area, log success/failure.\n- Use mocks in unit tests to assert high-level code calls `tap`, `type_text`, etc., with expected parameters.",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Claude Vision screen understanding client",
        "description": "Implement a Python client that sends device screenshots and the current context to Claude Vision, parses its response into actionable steps with coordinates and text.",
        "details": "Implementation details:\n- Use Anthropic’s Python SDK or plain HTTP with API key from config.\n- Define a prompt template that instructs Claude Vision explicitly:\n  - Provide **screen description**.\n  - Provide **next action** in a strict JSON format with fields like `{\"action\": \"tap\"|\"type\"|\"scroll\"|\"done\"|\"wait\",\"coordinates\": {\"x\": int, \"y\": int},\"text\": \"...\", \"reason\": \"...\"}`.\n  - Ask it to always respond with a single JSON object and no extra text.\n  - Instruct it that the goal is to create and fully set up an Instagram account according to the step list (birthday, phone, SMS, username, password, skip optional, photo, bio, creator, follow accounts).\n- Implement `claude_vision.py` with:\n  - `class ClaudeVisionClient:`\n    - `propose_action(image_bytes: bytes, state: dict) -> dict` where `state` includes progress markers (e.g., `has_entered_birthday`, `has_verified_phone`).\n- Implement robust JSON parsing:\n  - Strip any non-JSON prefix/suffix if Claude accidentally adds text.\n  - Validate that required keys exist; if not, log error and request again with a clarifying system message.\n- Include rate limiting/backoff and simple retry for network errors or malformed responses.\n- Maintain a small `state` object that encodes goal progress to share with Claude in the system/user message so it can choose the next step more reliably.\n\nPseudo-code:\n```python\nSYSTEM_PROMPT = \"\"\"You are controlling an Android phone to create a new Instagram account...\"\"\"\n\ndef propose_action(self, img, state):\n    msg = self._build_message(state)\n    resp = self.client.messages.create(\n        model=\"claude-3.5-sonnet\",  # or latest vision-capable model\n        max_tokens=300,\n        temperature=0.1,\n        messages=[\n          {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n          {\"role\": \"user\", \"content\": [\n              {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": base64.b64encode(img).decode()}},\n              {\"type\": \"text\", \"text\": msg},\n          ]},\n        ],\n    )\n    json_str = extract_json(resp)\n    return json.loads(json_str)\n```",
        "testStrategy": "- Unit test prompt-building and JSON parsing with canned Claude-like responses.\n- Add an offline mode that uses a fake vision client returning predetermined actions for known test screenshots to validate the loop without spending API credits.\n- Log each request/response pair to a file (with redaction of secrets) and manually inspect a few runs to ensure action JSON is consistent.",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "DaisySMS integration for phone number and SMS code retrieval",
        "description": "Implement integration with DaisySMS to rent phone numbers and poll for Instagram verification SMS codes.",
        "details": "Implementation details:\n- Review DaisySMS API docs to identify endpoints for:\n  - Requesting a number for a specific service/country.\n  - Checking SMS status and retrieving the code.\n  - Canceling/finishing an activation.\n- Implement `daisysms_client.py` with:\n  - `request_number(service=\"instagram\", country=None) -> Activation` where `Activation` holds `id`, `phone_number`.\n  - `wait_for_sms(activation_id, timeout=300, poll_interval=5) -> str` returning the numeric code.\n  - `cancel_activation(activation_id)` and `finish_activation(activation_id)`.\n- Handle common failure cases: no numbers, timeout waiting for SMS, banned/invalid numbers.\n- Mask phone number in logs for privacy.\n- Provide helper to format phone for entering on the device (e.g., strip `+` if needed, or let Claude decide how to input it given the screenshot).\n\nPseudo-code sketch:\n```python\nclass DaisySmsClient:\n    def request_number(self):\n        # call API, parse JSON\n        return Activation(id=act_id, phone=phone)\n\n    def wait_for_sms(self, act_id, timeout=300):\n        # loop: GET status, parse text, extract 6-digit code via regex\n```",
        "testStrategy": "- Unit test JSON parsing with sample DaisySMS responses.\n- Use a mock HTTP server (e.g., `responses` or `httpretty`) for DaisySMS endpoints to validate retry and timeout behavior.\n- In a staging run, manually request a number and send a test SMS from another phone to verify code extraction logic.\n- Simulate failure modes (no number, timeout, malformed SMS) and confirm the calling workflow handles them gracefully (marks account as failed, logs reason, releases activation).",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "2Captcha integration for solving Instagram captchas",
        "description": "Integrate with 2Captcha to submit Instagram captchas (image or possibly hCaptcha/ReCaptcha) and retrieve solutions when they appear during signup.",
        "details": "Implementation details:\n- Check 2Captcha docs for supported captcha types on Instagram flows (likely image captcha or hCaptcha/ReCaptcha). Implement at least generic image captcha support; leave hooks for sitekey-based captchas if needed.\n- Implement `twocaptcha_client.py` with:\n  - `submit_image_captcha(image_bytes) -> captcha_id`.\n  - `wait_for_solution(captcha_id, timeout=180, poll_interval=5) -> str`.\n- Integrate with the main flow via a simple contract: when Claude identifies a captcha on the screen and indicates an `action: \"captcha\"` (we can define this), capture a high-resolution screenshot and crop if necessary:\n  - Either ask Claude to provide bounding box coordinates, then crop the relevant region before sending to 2Captcha.\n- After receiving the solution string, pass it back to the device using `type_text` or `tap`/`type` sequences as directed by Claude.\n- Implement error handling: if 2Captcha returns an error or times out, mark run as failed and log details.\n\nPseudo-code:\n```python\nclass TwoCaptchaClient:\n    def submit_image_captcha(self, img):\n        # POST multipart/form-data to 2Captcha\n\n    def wait_for_solution(self, cap_id, timeout):\n        # poll /res.php until status=1\n```",
        "testStrategy": "- Unit test polling and response parsing using mocked 2Captcha HTTP endpoints.\n- Manual integration test with a known captcha image to confirm that 2Captcha returns the expected text.\n- Simulate failures such as `ERROR_CAPTCHA_UNSOLVABLE` and ensure workflow either retries with a new captcha or aborts with a clear status.",
        "priority": "medium",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Mobile proxy rotation and network setup",
        "description": "Implement proxy rotation via a mobile proxy rotation URL before each new account and ensure all external calls from the device go through the rotated IP.",
        "details": "Implementation details:\n- Use the provided `PROXY_ROTATE_URL` config: before starting each new account creation, send a simple HTTP GET to this URL and wait a short delay (e.g., 5–10 seconds) for IP to change.\n- If GeeLark supports per-device proxy assignment, ensure the cloud phone is configured to use the mobile proxy; otherwise, rely on proxy at network edge.\n- Implement `proxy.py` with:\n  - `rotate_proxy() -> bool` which returns True on HTTP 2xx, False otherwise.\n- Add logging to record rotation attempts and results.\n- Optionally verify IP change using a cheap `https://api.ipify.org` style service via the device’s browser or host network (config-driven; disabled by default to avoid extra calls).\n- Integrate into `create_single_account` workflow: call `rotate_proxy()` once at the very beginning of each account run.\n\nPseudo-code:\n```python\ndef rotate_proxy(url, timeout=10):\n    try:\n        r = requests.get(url, timeout=timeout)\n        r.raise_for_status()\n        logger.info(\"Proxy rotated\")\n        time.sleep(8)\n        return True\n    except Exception as e:\n        logger.error(f\"Proxy rotation failed: {e}\")\n        return False\n```",
        "testStrategy": "- Unit test `rotate_proxy` with mocked HTTP responses (success, timeout, non-200).\n- In staging, call rotation multiple times and verify IP change manually using an external IP-check service.\n- Add a debug flag to log detected IPs (host-level) before and after rotation for manual verification.",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Core action loop: screenshot → Claude → device actions",
        "description": "Implement the main control loop that repeatedly screenshots the Geelark device, asks Claude Vision what to do, executes the returned action (tap/type/scroll), and tracks progress toward account creation.",
        "details": "Implementation details:\n- Implement `workflow.py` with a function `run_screen_loop(device: GeelarkDeviceController, vision: ClaudeVisionClient, state: dict, max_steps=200) -> state`.\n- Loop behavior:\n  - For each step:\n    - Take screenshot via `device.screenshot()`.\n    - Call `vision.propose_action(image_bytes, state)`.\n    - Parse action JSON and execute:\n      - `action == \"tap\"`: call `device.tap(x, y)`.\n      - `action == \"type\"`: call `device.type_text(text)`.\n      - `action == \"scroll\"`: call `device.scroll(direction, amount)`.\n      - `action == \"wait\"`: call `device.wait(seconds)`.\n      - `action == \"back\"`/`\"home\"`: call corresponding methods.\n      - `action == \"done\"`: break loop and return.\n      - `action == \"captcha\"`: delegate to 2Captcha handler (Task 15) then feed solution back.\n    - Update `state` with any progress hints returned (e.g., `state[\"phase\"] = resp[\"phase\"]`).\n    - Add random small delays (0.5–1.5 s) to mimic human interaction and let UI update.\n- Implement safety guards:\n  - If `max_steps` reached without `done`, mark run as failed.\n  - Detect repeated identical actions (same tap coordinates for many steps) and break to avoid loops.\n- Ensure the state encodes key information for later steps (e.g., whether phone number has been used, SMS verified, username set, account switched to creator, followed 20 accounts).\n\nPseudo-code:\n```python\ndef run_screen_loop(device, vision, state, max_steps=200):\n    for i in range(max_steps):\n        img = device.screenshot()\n        action = vision.propose_action(img, state)\n        if action[\"action\"] == \"done\":\n            state[\"status\"] = \"done\"\n            break\n        execute_action(device, action, state)\n    return state\n```",
        "testStrategy": "- Implement unit tests for `execute_action` using a mock `GeelarkDeviceController` to verify correct calls for each action type.\n- Use an offline fake-vision client (from Task 13 tests) returning a deterministic series of actions to validate that the loop terminates correctly and state progresses.\n- On a test device with Instagram already on a simple form screen, run a short loop and confirm taps and typing correspond roughly to what Claude suggests (manual spot check using logs and video capture).",
        "priority": "high",
        "dependencies": [
          12,
          13,
          15
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Account setup orchestration and Instagram-specific flow",
        "description": "Orchestrate a full Instagram account setup run, coordinating proxy rotation, device control, Claude-driven steps, DaisySMS phone/SMS handling, captchas, and the business logic for username/password, bio, photo, creator switch, and following accounts.",
        "details": "Implementation details:\n- Implement `create_single_account(profile: AccountProfile, cfg) -> AccountResult` in `workflow.py`.\n- High-level sequence:\n  1. Rotate proxy using Task 16.\n  2. Connect to Geelark device (Task 12) and ensure Instagram app is launched (via explicit launch intent or by tapping icon; you can teach Claude to tap the Instagram icon from home screen as part of loop).\n  3. Initialize `state` with:\n     - `target_bio`, `target_photo_path`, `follow_targets`.\n     - Flags: `birthday_entered`, `phone_requested`, `sms_verified`, `username_set`, `password_set`, `creator_switched`, `followed_count`.\n  4. Request DaisySMS number when the flow reaches phone entry stage:\n     - Either pre-request the number before starting, or better, when `state` indicates phone will be needed (e.g., when Claude says \"now enter phone number\").\n     - Store number and activation id in `state`.\n  5. Run `run_screen_loop` until `state[\"status\"] == \"done\"` or error.\n  6. In the loop integration, insert hooks based on `state`:\n     - When a screen expects the phone number, programmatically supply the DaisySMS number (you may give Claude the number in the context so it types it itself).\n     - After submitting phone, start a background `wait_for_sms` and when code is received, provide it to Claude in the next prompt so it can type it.\n     - For username/password, either auto-generate values in Python (e.g., random letters+digits) and provide them to Claude, or let Claude propose them but ensure Python records them in `state` so they can be output to CSV.\n  7. Ensure optional steps (such as contacts, notifications, etc.) are skipped—rely on Claude’s screen understanding but mention this explicitly in the prompt.\n  8. After reaching home feed, direct Claude (via state goal) to:\n     - Add profile photo from gallery: upload `target_photo_path` to the device or ensure the device already has a set of photos (outside of script scope) and instruct Claude accordingly.\n     - Add bio using `target_bio`.\n     - Switch to Creator account via settings (state flag `creator_switched=True` when done).\n     - Follow ~20 accounts from `follow_targets` list (give the list or next target to Claude in context, track `followed_count`).\n- Implement `AccountResult(username, password, phone, status, error_message=None)` dataclass.\n- On any unrecoverable error (DaisySMS/2Captcha failure, loop timeout, device disconnection), set `status=\"failed\"` and include `error_message`.\n",
        "testStrategy": "- Unit test orchestration logic with mocks for DaisySMS, 2Captcha, device controller, and Claude client to ensure correct call ordering and state changes.\n- Implement a dry-run mode that skips actual external calls and produces synthetic `AccountResult` to verify CSV output and control flow.\n- Run an end-to-end test on a single GeeLark device with manual observation, logging all key decisions; verify that a full account is created and appears in Instagram.\n- After a successful single-account run, test a small batch (e.g., 3 accounts) in series to validate that proxy rotation and resource cleanup between runs behave correctly.",
        "priority": "high",
        "dependencies": [
          12,
          13,
          14,
          15,
          16,
          17
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Profile data assignment and username/password generation",
        "description": "Implement logic to map input photos, bios, and follow lists to each new account and deterministically generate secure usernames and passwords.",
        "details": "Implementation details:\n- Implement `profiles.py` with:\n  - `build_profile(photos, bios, follows, index) -> AccountProfile` using round-robin or randomized selection.\n  - `generate_username(index, base=None) -> str` using a configurable pattern (e.g., random adjectives+noun+digits) and allowed Instagram constraints.\n  - `generate_password() -> str` with 12–16 chars including letters, digits, and symbols.\n- Ensure that for each account run, `AccountProfile` includes:\n  - `photo_path`: may be None if fewer photos than accounts; handle gracefully (skip photo step).\n  - `bio`: may be randomly chosen or selected sequentially.\n  - `follow_targets`: either the full list or a subset of ~20 selected per account.\n- Pass generated username and password into `state` to be shared with Claude so it types them when appropriate.\n- Avoid reusing the same username; if Instagram rejects a username, have Claude propose alternatives but keep track in state and update `AccountResult` accordingly.\n\nPseudo-code:\n```python\n@dataclass\nclass AccountProfile:\n    username: str\n    password: str\n    photo_path: Optional[str]\n    bio: Optional[str]\n    follow_targets: list[str]\n```",
        "testStrategy": "- Unit test profile building to ensure fair rotation of bios/photos and correct slicing of follow targets (~20 per account).\n- Unit test username/password generation for uniqueness and complexity constraints.\n- Use a mock Claude client to simulate username rejection; verify that state and `AccountResult` update to the new accepted username.",
        "priority": "medium",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Logging, error handling, and basic scaling for multiple accounts",
        "description": "Harden the script with structured logging, error handling, cleanup between runs, and simple sequential multi-account execution.",
        "details": "Implementation details:\n- Extend logging from Task 11:\n  - Include per-account correlation ID in all logs.\n  - Log key milestones (proxy rotated, number acquired, SMS received, captcha solved, account created, failures).\n- Implement a central exception handler in `main.py` that catches unexpected errors per account, records a failed `AccountResult`, and continues to the next account instead of crashing the whole batch.\n- Add cleanup hooks:\n  - Release DaisySMS activations on error.\n  - Optionally reset Instagram app state between runs (e.g., clear data or log out via Claude instructions at end of run).\n- For scaling:\n  - Keep initial implementation strictly sequential (one account after another) to minimize complexity.\n  - Design the code to allow future parallelization (e.g., by making `create_single_account` stateless other than its arguments and return value), but do not add concurrency yet.\n- Expose a few runtime knobs via CLI/config: `MAX_STEPS`, `SMS_TIMEOUT`, `CAPTCHA_TIMEOUT`, `RETRY_LIMIT`.\n",
        "testStrategy": "- Simulate multiple account runs with mocks where some accounts succeed and others fail; verify that all results are written to CSV and script exits cleanly.\n- Inject failures (e.g., raise exceptions from DaisySMS/2Captcha/Claude clients) and confirm they are caught and logged and do not stop subsequent accounts.\n- Manual multi-account test (2–3 accounts) to verify logs are readable and correlated with account IDs.",
        "priority": "medium",
        "dependencies": [
          18
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-12-10T03:59:28.494Z",
      "updated": "2025-12-10T04:21:12.826Z",
      "description": "Tasks for master context"
    }
  },
  "posting": {
    "tasks": [
      {
        "id": "1",
        "title": "Set up project structure, configuration, and API key management",
        "description": "Initialize a Python project for Geelark Instagram posting automation, including config management for CSV paths, Claude Vision, proxy rotation URL, and 2Captcha keys.",
        "details": "Implementation details:\n- Use Python 3.11+.\n- Create a package structure, e.g. `geelark_ig_bot/` with modules: `config.py`, `csv_io.py`, `geelark_control.py`, `instagram_flow.py`, `logging_utils.py`, `main.py`.\n- Use `python-dotenv` or similar to load secrets from `.env` (ANTHROPIC_API_KEY, CAPTCHA_API_KEY, PROXY_ROTATION_URL, etc.).\n- Define a `Config` dataclass in `config.py` holding: `input_csv_path`, `output_log_csv_path`, `video_root_dir`, `proxy_rotation_url`, `anthropic_api_key`, `captcha_api_key`, `geelark_api_base`, `mvp_mode` (single device vs multi-account).\n- Add a `requirements.txt` including: `requests`, `pandas` or `python-csv` (standard), `python-dotenv`, `anthropic` (official Claude client), and any chosen Geelark control SDK or ADB wrapper.\n- Provide a simple YAML or JSON config file for non-secret settings (file paths, default timeouts, retry counts).\n- Pseudo-code example:\n```python\n# config.py\nfrom dataclasses import dataclass\nimport os\n\n@dataclass\nclass Config:\n    input_csv_path: str\n    output_log_csv_path: str\n    video_root_dir: str\n    proxy_rotation_url: str\n    anthropic_api_key: str\n    captcha_api_key: str | None\n    geelark_api_base: str\n    mvp_mode: bool = True\n\n\ndef load_config() -> Config:\n    return Config(\n        input_csv_path=os.getenv(\"INPUT_CSV\", \"input.csv\"),\n        output_log_csv_path=os.getenv(\"OUTPUT_LOG_CSV\", \"post_log.csv\"),\n        video_root_dir=os.getenv(\"VIDEO_ROOT_DIR\", \"./videos\"),\n        proxy_rotation_url=os.getenv(\"PROXY_ROTATION_URL\", \"\"),\n        anthropic_api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n        captcha_api_key=os.getenv(\"CAPTCHA_API_KEY\"),\n        geelark_api_base=os.getenv(\"GEELARK_API_BASE\", \"http://localhost:8000\"),\n    )\n```",
        "testStrategy": "- Unit test `load_config()` with different environment variable scenarios.\n- Verify that secrets are not hardcoded (only read from env/.env).\n- Run a dry `python -m geelark_ig_bot.main --dry-run` to confirm project imports and config loading work without runtime errors.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:00:11.686Z"
      },
      {
        "id": "2",
        "title": "Implement CSV input parsing and output logging",
        "description": "Create robust utilities to read posting instructions from a CSV and log results to an output CSV log file.",
        "details": "Implementation details:\n- Define required input columns: `account_name`, `video_path`, `caption`.\n- Implement `read_jobs(csv_path: str) -> list[PostJob]` where `PostJob` is a dataclass with `account_name`, `video_path`, `caption`.\n- Validate CSV: check mandatory columns exist; trim whitespace; skip or flag empty rows.\n- Normalize `video_path` by joining with `video_root_dir` if it is not absolute.\n- Implement `append_log_row(log_path, account, video, status, error=None, timestamp=None)` that appends to CSV, creating header if file does not exist.\n- Ensure logs are flushed after every job for crash resilience.\n- Pseudo-code:\n```python\n# csv_io.py\nfrom dataclasses import dataclass\nimport csv, os, datetime\n\n@dataclass\nclass PostJob:\n    account_name: str\n    video_path: str\n    caption: str\n\n\ndef read_jobs(path: str, video_root_dir: str) -> list[PostJob]:\n    jobs = []\n    with open(path, newline='', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            if not row.get('account_name') or not row.get('video_path'):\n                continue\n            vp = row['video_path']\n            if not os.path.isabs(vp):\n                vp = os.path.join(video_root_dir, vp)\n            jobs.append(PostJob(row['account_name'].strip(), vp, row.get('caption', '')))\n    return jobs\n\n\ndef append_log_row(path: str, account: str, video: str, status: str, error: str | None = None):\n    file_exists = os.path.exists(path)\n    with open(path, 'a', newline='', encoding='utf-8') as f:\n        writer = csv.writer(f)\n        if not file_exists:\n            writer.writerow(['timestamp', 'account', 'video', 'status', 'error'])\n        ts = datetime.datetime.utcnow().isoformat()\n        writer.writerow([ts, account, video, status, error or ''])\n```",
        "testStrategy": "- Unit test `read_jobs` with:\n  - Valid CSV.\n  - Missing columns (expect exception or empty list based on design).\n  - Relative vs absolute video paths.\n- Unit test `append_log_row`:\n  - First write creates header.\n  - Subsequent calls append new rows.\n  - Inspect resulting CSV to match expected line count and fields.\n- Perform an end-to-end dry run reading a small sample CSV and writing a sample log.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:00:20.864Z"
      },
      {
        "id": "3",
        "title": "Design Geelark device control abstraction",
        "description": "Create an abstraction layer to control Geelark cloud phones for screenshots, taps, typing, app launching, and file transfer, independent of the underlying mechanism (RPA, ADB, or API).",
        "details": "Implementation details:\n- Define an interface `GeelarkDeviceController` with methods:\n  - `connect(account_name: str) -> DeviceHandle`\n  - `launch_app(device: DeviceHandle, app_id: str)` (e.g. Instagram)\n  - `tap(device, x: int, y: int)`\n  - `type_text(device, text: str)`\n  - `screenshot(device) -> bytes` (PNG/JPEG bytes)\n  - `swipe(device, x1, y1, x2, y2, duration_ms)`\n  - `upload_file(device, local_path: str, remote_path: str) -> str` (returns remote path or URI).\n- Implement an initial MVP adapter that talks to Geelark via whichever is available first (e.g. ADB over TCP or a Geelark HTTP API). For now, define stub methods that raise `NotImplementedError` but with clear signatures.\n- Provide a mapping from `account_name` to `device_id` (config or simple dict) for the MVP single device.\n- Include sensible timeouts and retry wrappers around network calls.\n- Pseudo-code skeleton:\n```python\n# geelark_control.py\nfrom dataclasses import dataclass\n\n@dataclass\nclass DeviceHandle:\n    id: str\n\n\nclass GeelarkDeviceController:\n    def connect(self, account_name: str) -> DeviceHandle:\n        # map account -> device_id (MVP: single device)\n        raise NotImplementedError\n\n    def launch_app(self, device: DeviceHandle, app_id: str):\n        raise NotImplementedError\n\n    def tap(self, device: DeviceHandle, x: int, y: int):\n        raise NotImplementedError\n\n    def type_text(self, device: DeviceHandle, text: str):\n        raise NotImplementedError\n\n    def screenshot(self, device: DeviceHandle) -> bytes:\n        raise NotImplementedError\n\n    def upload_file(self, device: DeviceHandle, local_path: str, remote_path: str) -> str:\n        raise NotImplementedError\n```\n- Later tasks will fill implementations using the chosen low-level mechanism.",
        "testStrategy": "- Unit test that the interface exists and that stub methods raise `NotImplementedError`.\n- Create a fake/mock implementation `MockGeelarkDeviceController` for testing higher-level logic without real devices.\n- Verify that `account_name` to `device_id` mapping works as expected using the MVP single-device configuration.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:00:29.298Z"
      },
      {
        "id": "4",
        "title": "Implement low-level Geelark control (screenshots, taps, typing, file transfer)",
        "description": "Provide a concrete implementation of the Geelark device control abstraction using the chosen RPA/ADB/API mechanism.",
        "details": "Implementation details:\n- Decide a concrete mechanism based on what Geelark exposes:\n  - If Geelark offers an HTTP API: implement calls like `POST /devices/{id}/tap`, `POST /devices/{id}/type`, `GET /devices/{id}/screenshot`, etc.\n  - If using ADB: use `subprocess` to call `adb -s <serial> shell input tap x y`, `input text`, `screencap -p`, and `adb push` for file transfer.\n- Implement `GeelarkDeviceController` methods:\n  - `connect`: resolve `account_name` to a device identifier (e.g. `device_serial`), possibly via config mapping; validate connectivity.\n  - `launch_app`: `adb shell monkey -p com.instagram.android 1` or equivalent API.\n  - `tap`: execute appropriate tap command.\n  - `type_text`: escape special characters for ADB; for longer captions, implement paste via clipboard if device API supports it.\n  - `screenshot`: capture and return raw bytes; ensure correct image format for Claude Vision.\n  - `upload_file`: transfer video from host to device; return the device-side file path.\n- Add minimal rate limiting to avoid overwhelming Geelark/API.\n- Pseudo-code example (ADB-style):\n```python\nimport subprocess, io\n\nclass AdbGeelarkDeviceController(GeelarkDeviceController):\n    def __init__(self, mapping: dict[str, str]):\n        self.mapping = mapping\n\n    def connect(self, account_name: str) -> DeviceHandle:\n        serial = self.mapping.get(account_name) or next(iter(self.mapping.values()))\n        return DeviceHandle(serial)\n\n    def tap(self, device: DeviceHandle, x: int, y: int):\n        subprocess.run([\"adb\", \"-s\", device.id, \"shell\", \"input\", \"tap\", str(x), str(y)], check=True)\n\n    def screenshot(self, device: DeviceHandle) -> bytes:\n        out = subprocess.check_output([\"adb\", \"-s\", device.id, \"exec-out\", \"screencap\", \"-p\"])\n        return out\n\n    def upload_file(self, device: DeviceHandle, local_path: str, remote_path: str) -> str:\n        subprocess.run([\"adb\", \"-s\", device.id, \"push\", local_path, remote_path], check=True)\n        return remote_path\n```",
        "testStrategy": "- If using ADB: run integration tests against a test device or emulator.\n  - Verify `connect` returns a valid handle.\n  - Call `screenshot` and confirm returned bytes decode as an image.\n  - Call `tap` and `type_text` while observing the device screen.\n  - Transfer a small dummy video file and confirm existence on the device.\n- If using HTTP API: use a mock server to validate request payloads, paths, and error handling.\n- Add negative tests: simulate command/API failures and verify that exceptions are raised and propagated up.",
        "priority": "high",
        "dependencies": [
          "3"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:00:38.036Z"
      },
      {
        "id": "5",
        "title": "Integrate proxy rotation before each post",
        "description": "Implement a simple proxy rotation step that hits the configured rotation URL before each posting job.",
        "details": "Implementation details:\n- Add a `rotate_proxy()` function in a `network_utils.py` module.\n- Use `requests.get(config.proxy_rotation_url, timeout=10)` or equivalent; treat non-2xx responses as failures.\n- Add small backoff and retry (e.g. 3 attempts with exponential backoff) because this is a network call.\n- Pseudo-code:\n```python\nimport time, requests\n\ndef rotate_proxy(url: str, retries: int = 3, base_delay: float = 1.0) -> bool:\n    for attempt in range(retries):\n        try:\n            r = requests.get(url, timeout=10)\n            if 200 <= r.status_code < 300:\n                return True\n        except requests.RequestException:\n            pass\n        time.sleep(base_delay * (2 ** attempt))\n    return False\n```\n- Hook `rotate_proxy()` into the main posting loop: call it before connecting to the Geelark device for each row.\n- Log proxy rotation success/failure per job (but continue posting even if rotation fails if that is acceptable per requirements).",
        "testStrategy": "- Unit test `rotate_proxy` using a requests-mock server returning:\n  - 200: expect success on first attempt.\n  - 500: expect retries and final failure.\n  - Network timeout: expect retries and final failure.\n- In an integration-like test, configure a local HTTP server as rotation URL and verify that it is hit once per job in a multi-row CSV.",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:00:56.695Z"
      },
      {
        "id": "6",
        "title": "Implement Claude Vision client for Instagram UI navigation",
        "description": "Create a module that sends device screenshots and minimal context to Claude Vision and receives structured navigation instructions for the Instagram posting flow.",
        "details": "Implementation details:\n- Use the official Anthropic Python SDK (`anthropic` package) and Claude Vision model.\n- Define a `ClaudeNavigator` class with:\n  - `plan_next_action(screenshot_bytes: bytes, context: dict) -> Action` where `Action` is a dataclass describing an operation such as `tap(x,y)`, `type(text)`, `wait(seconds)`, `verify_posted`.\n- Provide a system prompt that explains the device context (Android Instagram app on a cloud phone), the goal (post a Reel/video with a given caption), and a JSON schema for response.\n- Example pseudo-code:\n```python\nfrom anthropic import Anthropic\nimport base64, json\n\n@dataclass\nclass Action:\n    kind: str  # 'tap', 'type', 'wait', 'done', 'error'\n    x: int | None = None\n    y: int | None = None\n    text: str | None = None\n    seconds: float | None = None\n\n\nclass ClaudeNavigator:\n    def __init__(self, api_key: str):\n        self.client = Anthropic(api_key=api_key)\n\n    def plan_next_action(self, screenshot_bytes: bytes, context: dict) -> Action:\n        img_b64 = base64.b64encode(screenshot_bytes).decode('ascii')\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"input_image\",\n                        \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": img_b64},\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": json.dumps(context),\n                    },\n                ],\n            }\n        ]\n        resp = self.client.messages.create(\n            model=\"claude-3-5-sonnet\",  # example vision-capable model\n            max_tokens=512,\n            messages=messages,\n            system=\"You control an Android Instagram app. Respond ONLY with a JSON object describing the next action to create and publish a video post.\",\n        )\n        action_dict = json.loads(resp.content[0].text)\n        return Action(**action_dict)\n```\n- The `context` should include the current step: e.g. `{\"step\": \"open_plus\", \"caption\": \"...\"}`.\n- Keep actions atomic and loop until `kind == 'done'` or an error is detected.",
        "testStrategy": "- Unit test `ClaudeNavigator` parsing: mock Anthropic client responses with known JSON and ensure `Action` is constructed correctly.\n- Add validation on returned actions (e.g. coordinates within screen bounds, non-empty `text` for `type` actions) and test these validators.\n- For manual testing, feed screenshots of Instagram app (from a real device) and confirm that the model returns sensible next-step actions by logging them without executing on device.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:01:05.856Z"
      },
      {
        "id": "7",
        "title": "Orchestrate Instagram posting flow with device control and Claude Vision",
        "description": "Combine CSV jobs, Geelark control, proxy rotation, and Claude Vision navigation to automate the full Instagram posting flow per row, including caption entry and success verification for one device (MVP).",
        "details": "Implementation details:\n- Implement a high-level function `run_post_job(job: PostJob, config: Config, controller: GeelarkDeviceController, navigator: ClaudeNavigator)` that:\n  1) Rotates proxy.\n  2) Connects to the Geelark device for `job.account_name`.\n  3) Ensures the Instagram app is running (`launch_app`).\n  4) Transfers the video file to the device via `upload_file` and records the device path.\n  5) Enters a loop to perform the posting flow:\n     - Take a `screenshot`.\n     - Provide `context` to Claude, including:\n       - `goal`: \"Post the specified video to this Instagram account as a Reel or standard video post.\"\n       - `step_state`: track state such as `{\"video_uploaded\": false, \"caption_pasted\": false}`.\n       - `video_device_path` and `caption`.\n     - Receive `Action` from `ClaudeNavigator`.\n     - Map `Action` to `GeelarkDeviceController` calls (`tap`, `type_text`, etc.).\n     - Track timeouts and max steps (e.g. 30 steps) to avoid infinite loops.\n  6) After `Action.kind == 'done'`, confirm success by having Claude inspect a final screenshot with a `verify_posted` context.\n- Ensure that errors (exceptions, invalid actions, timeouts) raise a `PostJobError` that carries a human-readable message.\n- Pseudo-code skeleton:\n```python\ndef run_post_job(job, config, controller, navigator):\n    rotate_proxy(config.proxy_rotation_url)\n    device = controller.connect(job.account_name)\n    controller.launch_app(device, app_id=\"com.instagram.android\")\n    remote_video_path = controller.upload_file(device, job.video_path, \"/sdcard/Download/post_video.mp4\")\n\n    state = {\"video_uploaded\": False, \"caption_pasted\": False, \"remote_video_path\": remote_video_path}\n    for step in range(30):\n        screenshot = controller.screenshot(device)\n        context = {\"goal\": \"post_video\", \"caption\": job.caption, \"state\": state}\n        action = navigator.plan_next_action(screenshot, context)\n        if action.kind == \"tap\":\n            controller.tap(device, action.x, action.y)\n        elif action.kind == \"type\":\n            controller.type_text(device, action.text)\n        elif action.kind == \"wait\":\n            time.sleep(action.seconds)\n        elif action.kind == \"done\":\n            break\n        else:\n            raise PostJobError(f\"Unknown action: {action.kind}\")\n\n    # final verification screenshot\n    final_shot = controller.screenshot(device)\n    verify_action = navigator.plan_next_action(final_shot, {\"goal\": \"verify_posted\"})\n    if verify_action.kind != \"done\":\n        raise PostJobError(\"Unable to verify post was successful\")\n```\n- Make the orchestrator initially target MVP: one device and single job; then scale to loop over all jobs from CSV in `main.py`.\n- Capture and return a success/failure status and error message to the caller for logging.",
        "testStrategy": "- Implement integration tests in a `--dry-run` mode where `GeelarkDeviceController` is a mock and `ClaudeNavigator` is replaced by a deterministic fake that returns a scripted sequence of actions; verify steps executed in correct order.\n- On a real Geelark device, manually run one job and visually confirm that Instagram opens, video is selected, caption is filled, and post is shared.\n- Test failure paths: simulate `upload_file` failure, invalid actions from navigator, and assert that errors propagate to logging.\n- Verify that the loop stops when max steps are reached and logs an appropriate error.",
        "priority": "high",
        "dependencies": [
          "2",
          "4",
          "5",
          "6"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:01:14.920Z"
      },
      {
        "id": "8",
        "title": "Handle login prompts, captchas, and rate limits",
        "description": "Add edge-case handling for Instagram login requests, captchas via 2Captcha, and rate-limit detection with backoff and retry.",
        "details": "Implementation details:\n- Extend Claude prompts to explicitly ask it to identify when the screen shows:\n  - A login screen.\n  - A captcha challenge.\n  - A rate-limit or \"try again later\" message.\n- In `ClaudeNavigator`, allow an `Action.kind` of `\"login_required\"`, `\"captcha\"`, or `\"rate_limited\"` with additional metadata if needed.\n- Implement logic in the orchestrator:\n  - `login_required`: for MVP, either skip the job and log `login_required`, or if credentials are available in config, allow navigator-guided login by providing `username`/`password` in context.\n  - `captcha`: integrate 2Captcha by:\n    - Taking a screenshot of the captcha area (or whole screen) and sending to 2Captcha's image API.\n    - Polling for the solved text and then issuing `type_text` or `tap` actions accordingly.\n  - `rate_limited`: pause posting for a configurable cooldown (e.g. 10–30 minutes per account/device) before retrying the current job once; if still rate limited, mark as failed and move on.\n- Pseudo-code snippet for 2Captcha integration:\n```python\nimport requests, time\n\nclass CaptchaSolver:\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n\n    def solve_image(self, image_bytes: bytes) -> str:\n        # send\n        resp = requests.post(\"http://2captcha.com/in.php\", data={\n            \"key\": self.api_key,\n            \"method\": \"base64\",\n            \"body\": base64.b64encode(image_bytes).decode('ascii'),\n            \"json\": 1,\n        })\n        captcha_id = resp.json()[\"request\"]\n        # poll result\n        for _ in range(24):\n            r = requests.get(\"http://2captcha.com/res.php\", params={\n                \"key\": self.api_key,\n                \"action\": \"get\",\n                \"id\": captcha_id,\n                \"json\": 1,\n            })\n            data = r.json()\n            if data[\"status\"] == 1:\n                return data[\"request\"]\n            time.sleep(5)\n        raise TimeoutError(\"Captcha solving timed out\")\n```\n- Log all edge-case events distinctly so they can be monitored later.",
        "testStrategy": "- Unit test captcha solver using mocked 2Captcha endpoints with typical success and timeout responses.\n- Extend fake `ClaudeNavigator` in tests to return `login_required`, `captcha`, and `rate_limited` actions and verify that the orchestrator:\n  - For `login_required`, either skips or performs login based on test configuration.\n  - For `captcha`, calls `CaptchaSolver.solve_image` and then attempts to type the solution.\n  - For `rate_limited`, waits the configured cooldown and retries at most once.\n- Manually induce a login-required state on a test account and confirm that it is handled as designed and logged appropriately.",
        "priority": "medium",
        "dependencies": [
          "6",
          "7"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:01:25.731Z"
      },
      {
        "id": "9",
        "title": "Implement main loop, error handling, and structured logging",
        "description": "Create the main entrypoint that iterates over CSV rows, invokes the posting orchestrator per job, and writes structured logs with status, timestamp, and errors.",
        "details": "Implementation details:\n- In `main.py`, implement:\n  - `load_config()`.\n  - Initialize `GeelarkDeviceController`, `ClaudeNavigator`, and optionally `CaptchaSolver`.\n  - Load jobs via `read_jobs(config.input_csv_path, config.video_root_dir)`.\n  - For each job:\n    - Call `run_post_job` inside a `try/except` block.\n    - On success, call `append_log_row(..., status=\"success\")`.\n    - On failure, log `status=\"fail\"` with the exception message.\n- Use Python `logging` module with JSON-ish log format (e.g. `%(asctime)s %(levelname)s %(message)s`) and include job identifiers.\n- Allow CLI flags/env for:\n  - `--mvp` (single job from CSV).\n  - `--max-jobs` to limit for testing.\n- Pseudo-code:\n```python\ndef main():\n    config = load_config()\n    controller = AdbGeelarkDeviceController(mapping=load_account_device_mapping())\n    navigator = ClaudeNavigator(api_key=config.anthropic_api_key)\n    jobs = read_jobs(config.input_csv_path, config.video_root_dir)\n\n    for i, job in enumerate(jobs):\n        try:\n            run_post_job(job, config, controller, navigator)\n            append_log_row(config.output_log_csv_path, job.account_name, job.video_path, \"success\")\n        except Exception as e:\n            append_log_row(config.output_log_csv_path, job.account_name, job.video_path, \"fail\", str(e))\n```\n- Ensure that an exception in one job does not terminate the loop; always continue to next row.\n- Optionally, add a small random delay between jobs to reduce pattern-like behavior and mitigate rate limits.",
        "testStrategy": "- Use a mock controller and navigator to simulate successful and failing jobs; verify that the main loop continues after failures and that the log CSV contains correct rows.\n- Run end-to-end in a test environment with 2–3 dummy jobs, visually inspect logs and confirm that timestamps and statuses are correct.\n- Intentionally raise an exception inside `run_post_job` for one job and confirm that others are still processed.",
        "priority": "high",
        "dependencies": [
          "2",
          "5",
          "7"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:01:36.963Z"
      },
      {
        "id": "10",
        "title": "MVP validation and scaling to multiple accounts/devices",
        "description": "Validate the MVP by successfully posting one video with caption on a single Geelark device, then extend to handle multiple accounts/devices from the spreadsheet.",
        "details": "Implementation details:\n- MVP validation steps:\n  - Configure one `account_name` in the CSV, one `video_path`, and a simple caption.\n  - Map that account to a Geelark device in the controller configuration.\n  - Run the tool and visually confirm that the video is posted with the correct caption.\n  - Confirm that the output log records `success` for this job.\n- Scaling steps:\n  - Extend account-to-device mapping to support many accounts; use a config file like `devices.yaml` with entries `{account_name, device_id}`.\n  - In `connect(account_name)`, look up the correct `device_id` and fall back to a default or raise an error if unmapped.\n  - If Geelark supports parallel control, optionally add a future-ready abstraction to run jobs concurrently (e.g. via a worker pool); for now keep them sequential to minimize complexity.\n  - Ensure that proxy rotation is still called once per job and that rate-limit logic is per account/device.\n- Add documentation (README) describing:\n  - How to prepare the CSV.\n  - How to organize video files.\n  - How to configure API keys and device mappings.\n  - Known edge cases and limitations.",
        "testStrategy": "- For MVP:\n  - Run manual test: verify the real post appears on Instagram from the target account with the expected caption and time.\n  - Check that logs show a single `success` entry with accurate timestamp and video path.\n- For multi-account:\n  - Prepare a CSV with at least 2 accounts mapped to different devices (or sequential runs on same device if that is the Geelark constraint).\n  - Run and verify that each account posts its respective video.\n  - Inspect logs to ensure each row has correct `account`, `video`, and `status`.\n- Perform a small load test with ~10 rows to confirm there are no memory leaks or unhandled exceptions across many iterations.",
        "priority": "medium",
        "dependencies": [
          "4",
          "7",
          "8",
          "9"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:01:47.805Z"
      },
      {
        "id": "11",
        "title": "Fix ADBKeyboard installation on Geelark cloud phones",
        "description": "Pivot to using Appium for Unicode text input on Geelark cloud phones, abandoning the ADBKeyboard approach due to Android 15 incompatibility where the package is hidden at the framework level and cannot be restored via ADB commands.",
        "status": "done",
        "dependencies": [
          "4"
        ],
        "priority": "high",
        "details": "PROBLEM SUMMARY:\n- ADBKeyboard works on Android 12/13 (SDK 31-33) but is blocked on Android 15 (SDK 35) with hidden=true flag at Android framework level\n- All ADB remediation attempts failed: pm uninstall, pm enable, pm unhide, cmd package install-existing all return success but package remains hidden\n- Geelark phones do not provide root access (su returns command not found)\n- ClipboardHelper + KEYCODE_PASTE fallback tested and FAILED - keyboard not visible during paste, text does not appear\n- Only podmindstudio (Android 13) works; reelwisdompod_ and talktrackhub (Android 15) are broken\n\nNEW APPROACH - APPIUM:\nPivot to using Appium for text input, which handles Unicode natively across all Android versions without requiring a custom keyboard IME.\n\nAppium UIAutomator2 driver can:\n- Type text directly into focused fields via send_keys() or mobile:type command\n- Works with Unicode/emojis natively\n- No need for ADBKeyboard, ClipboardHelper, or any IME installation\n- Connects to devices via ADB (same as current setup)\n- Cross-platform Android version support (works on Android 15)\n\nIMPLEMENTATION PLAN:\n1. Set up Appium server (can run locally or on a server)\n   - Install Node.js if not present\n   - npm install -g appium\n   - appium driver install uiautomator2\n\n2. Install Python Appium client:\n   - pip install Appium-Python-Client\n\n3. Connect to Geelark phones via Appium:\n   - Use ADB connection info from Geelark API (same as current flow)\n   - Create Appium driver session with desired capabilities:\n     - platformName: Android\n     - automationName: UiAutomator2\n     - deviceName: {adb_device_id}\n     - noReset: true\n     - appPackage/appActivity for Instagram\n\n4. Update post_reel_smart.py to use Appium:\n   - Create new AppiumController class or add Appium methods to SmartInstagramPoster\n   - Replace type_text() method (lines 225-245) with Appium's send_keys()\n   - Keep ADB for non-typing operations (tap, swipe, screenshot)\n   - Or migrate entirely to Appium for all interactions\n\n5. Testing:\n   - Test on Android 15 device (reelwisdompod_) first\n   - Verify Unicode/emoji typing works correctly\n   - Test full Instagram posting flow\n\nRELEVANT FILES TO MODIFY:\n- post_reel_smart.py: Replace type_text() with Appium-based implementation\n- requirements.txt: Add Appium-Python-Client dependency\n- New file: appium_controller.py (optional, for Appium setup logic)\n\nEXISTING ASSETS:\n- appium-uiautomator2-server.apk already exists in project root\n- package/ directory contains io.appium.settings source (UnicodeIME) but not needed with direct Appium approach\n- ADB connection flow in post_reel_smart.py connect() method can be reused",
        "testStrategy": "- Set up Appium server locally\n- Test Appium connection to reelwisdompod_ (Android 15) device first\n- Create test script that: 1) connects via Appium, 2) opens Instagram, 3) navigates to caption field, 4) types text with emojis using send_keys()\n- Verify text appears correctly in the caption field including Unicode characters and emojis\n- Run full posting flow on Android 15 device\n- Verify same flow still works on Android 13 device (podmindstudio) for backwards compatibility\n- Compare posting success rates before/after migration",
        "subtasks": [
          {
            "id": 3,
            "title": "Complete ADBKeyboard remediation research and document Android 15 blocker",
            "description": "Document the comprehensive ADBKeyboard remediation attempts and confirm that Android 15 hidden=true state is an unresolvable blocker without root access, leading to pivot to Appium.",
            "dependencies": [
              1,
              2
            ],
            "details": "All ADBKeyboard remediation approaches exhausted:\n- pm uninstall/install: Returns success but package remains hidden\n- cmd package install-existing: Returns success but pm path empty\n- pm enable/unhide: Requires root access not available on Geelark\n- Alternative keyboards: Same hidden=true issue affects new installs\n- ClipboardHelper fallback: FAILED - keyboard not visible during paste\n- Root API: Error 43016 indicates phones don't support root\n\nConclusion: ADBKeyboard approach is fundamentally incompatible with Android 15 on Geelark phones. Pivoting to Appium which handles Unicode typing natively without requiring IME installation.",
            "status": "done",
            "testStrategy": "Document all attempted remediation commands and their results. Confirm Android version correlation (SDK 35 = broken, SDK <= 33 = working).",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Set up Appium server and install UiAutomator2 driver",
            "description": "Install and configure Appium server locally with UiAutomator2 driver for Android automation that supports native Unicode text input across all Android versions.",
            "dependencies": [],
            "details": "Installation steps:\n1. Verify Node.js is installed (node --version), install if needed from https://nodejs.org\n2. Install Appium globally: npm install -g appium\n3. Install UiAutomator2 driver: appium driver install uiautomator2\n4. Verify installation: appium driver list (should show uiautomator2)\n5. Start Appium server: appium --allow-insecure chromedriver_autodownload\n6. Verify server is running on http://localhost:4723\n\nServer configuration:\n- Default port: 4723\n- May need to configure ANDROID_HOME environment variable pointing to Android SDK\n- May need to ensure platform-tools (adb) is in PATH\n\nFiles to create:\n- requirements.txt: Add 'Appium-Python-Client>=3.0.0'\n- Optional: appium_setup.py script to verify/start Appium service\n<info added on 2025-12-11T04:22:32.422Z>\nCOMPLETED SETUP STATUS:\n- Appium version: 3.1.2 installed globally via npm\n- UiAutomator2 driver: installed via appium driver install uiautomator2\n- Android SDK: ANDROID_HOME=C:/Users/asus/Downloads/android-sdk with platform-tools symlinked\n- Successfully connected to Geelark cloud phone at 98.98.125.37:20865 running Android 15 (SDK 35)\n- Connection verified via test_appium.py script which captured screenshot (appium_test.png) proving connection works\n- Appium-Python-Client needs to be added to requirements.txt (currently only has python-dotenv, requests, anthropic)\n- Platform version confirmed via driver.capabilities after successful Remote connection to http://127.0.0.1:4723\n</info added on 2025-12-11T04:22:32.422Z>",
            "status": "done",
            "testStrategy": "1) Run 'appium --version' to verify installation\n2) Run 'appium driver list' to verify uiautomator2 is installed\n3) Start Appium server and verify it responds on localhost:4723\n4) Create simple test script that imports appium and verifies client library version",
            "updatedAt": "2025-12-11T04:21:52.916Z",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Appium connection to Geelark cloud phones",
            "description": "Create AppiumController class that connects to Geelark devices via Appium using existing ADB connection info from GeelarkClient, enabling Unicode text input on Android 15.",
            "dependencies": [
              4
            ],
            "details": "Implementation in new file appium_controller.py:\n\nCreate AppiumController class with methods:\n- connect(): Get phone info from GeelarkClient, start phone, enable ADB, connect via Appium with UiAutomator2Options\n- type_text(text): Use driver.switch_to.active_element.send_keys(text) for Unicode support\n- close(): Quit Appium driver session\n\nKey Appium capabilities:\n- platformName: 'Android'\n- automationName: 'UiAutomator2'\n- deviceName: ADB device string (ip:port)\n- noReset: True (preserve app state)\n- newCommandTimeout: 300\n\nIntegration with existing code:\n- Reuse GeelarkClient for phone discovery and ADB setup\n- Reuse ADB connection logic from post_reel_smart.py lines 115-170\n- Add error handling for Appium connection failures",
            "status": "done",
            "testStrategy": "1) Connect to reelwisdompod_ (Android 15) via Appium\n2) Verify driver session is established\n3) Run driver.page_source to confirm UI access\n4) Take screenshot via driver.get_screenshot_as_png()\n5) Verify connection works on both Android 15 and Android 13 devices",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T04:35:32.773Z"
          },
          {
            "id": 6,
            "title": "Update post_reel_smart.py to use Appium for text input",
            "description": "Modify the SmartInstagramPoster class to use Appium's send_keys() for typing captions instead of ADBKeyboard broadcast, while keeping ADB for other operations.",
            "dependencies": [
              4,
              5
            ],
            "details": "Changes to post_reel_smart.py:\n\n1) Add Appium imports at top:\nfrom appium import webdriver\nfrom appium.options.android import UiAutomator2Options\n\n2) Add Appium driver initialization in connect() method\n\n3) Replace type_text() method (lines 225-245) with Appium-based implementation:\n- Use self.appium_driver.switch_to.active_element.send_keys(text)\n- Remove typing_method check since Appium works universally\n- Handle emojis and Unicode natively\n\n4) Add cleanup for Appium driver in disconnect/cleanup\n\n5) Keep existing ADB methods for tap(), swipe(), screenshot, etc.\n\nAlternative: Hybrid approach - try Appium first, fall back to ADBKeyboard if Appium unavailable for Android 13 devices",
            "status": "done",
            "testStrategy": "1) Start Appium server\n2) Run test on Android 15 device (reelwisdompod_) with caption containing emojis\n3) Verify caption appears correctly in Instagram caption field\n4) Run full posting flow and verify success\n5) Run same test on Android 13 device (podmindstudio) for backwards compatibility",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T04:35:36.719Z"
          },
          {
            "id": 7,
            "title": "Add Appium dependencies and update requirements.txt",
            "description": "Add Appium-Python-Client and any other required dependencies to the project requirements file.",
            "dependencies": [],
            "details": "Update requirements.txt to add:\nAppium-Python-Client>=3.0.0\nselenium>=4.0.0\n\nInstallation command: pip install Appium-Python-Client\n\nVerify installation:\nimport appium\nprint(appium.__version__)\n\nNote: Appium-Python-Client depends on selenium, which will be installed automatically.\n\nPreserve existing dependencies:\n- anthropic (for Claude API)\n- requests (for HTTP calls)\n- python-dotenv (for .env loading)",
            "status": "done",
            "testStrategy": "1) Run pip install -r requirements.txt\n2) Verify no dependency conflicts\n3) Test import: python -c \"from appium import webdriver; print('OK')\"\n4) Verify existing imports still work",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T04:22:51.801Z"
          },
          {
            "id": 8,
            "title": "Test full Instagram posting flow with Appium on Android 15",
            "description": "Perform end-to-end testing of the complete Instagram Reel posting workflow using Appium for text input on an Android 15 device to validate the pivot from ADBKeyboard.",
            "dependencies": [
              4,
              5,
              6,
              7
            ],
            "details": "Test procedure:\n\n1) Pre-requisites:\n- Appium server running\n- Android 15 device available (reelwisdompod_ or talktrackhub)\n- Test video file and caption with Unicode/emojis prepared\n\n2) Test execution:\nStart Appium server in terminal 1: appium\nRun posting script in terminal 2: python post_reel_smart.py reelwisdompod_ test_video.mp4 \"Test caption with emojis 🎉✨🔥\"\n\n3) Verification steps:\n- Phone connects successfully\n- Instagram app opens\n- Video upload works (existing ADB-based file transfer)\n- Caption field is focused\n- Appium types caption including emojis correctly\n- Post is shared successfully\n- Verify post appears on Instagram with correct caption\n\n4) Performance comparison:\n- Time to type caption: Appium vs ADBKeyboard\n- Overall posting time\n- Success rate over multiple posts",
            "status": "pending",
            "testStrategy": "1) Execute full posting flow on Android 15 device with emoji-rich caption\n2) Verify caption appears correctly on published post\n3) Repeat test 3-5 times to verify consistency\n4) Test on Android 13 device for backwards compatibility\n5) Run batch_post.py with mix of Android versions to verify multi-device support",
            "parentId": "undefined"
          },
          {
            "id": 1,
            "title": "Research Android package manager ghost package and signature mismatch behaviors (cloud phones)",
            "description": "Investigate how Android handles ghost/orphaned package entries and INSTALL_FAILED_UPDATE_INCOMPATIBLE errors, especially on non-rootable or cloud-hosted devices like Geelark, and document feasible ADB-only remedies.",
            "dependencies": [],
            "details": "Use Perplexity to search Android developer docs, StackOverflow, and XDA for: (1) causes and fixes of INSTALL_FAILED_UPDATE_INCOMPATIBLE when pm uninstall fails; (2) techniques to clear or bypass ghost/orphaned packages without root (e.g., user 0 uninstall, package clear, disabling users, testharness, or resetting app state); (3) behavior differences for system apps vs. user apps in /system/app and /system/priv-app. Summarize which approaches are viable when you only have adb shell and no root, and call out any device-OEM-specific caveats relevant to cloud/virtual devices.\n<info added on 2025-12-11T02:49:23.733Z>\nBased on the codebase analysis and research findings, here is the new text to append:\n\nResearch findings for ADB-only ghost package remediation on Geelark cloud phones:\n\n1) Ghost package removal without root: Use `pm uninstall --user 0 com.android.adbkeyboard` (do NOT use -k flag as it keeps data and leaves ghost state). This removes the package for the current user even when standard pm uninstall fails with DELETE_FAILED_INTERNAL_ERROR.\n\n2) Restoring orphaned system apps: If ADBKeyboard was previously a system app (like on podmindstudio at /system/app/AdbKeyboard/AdbKeyboard.apk), use `cmd package install-existing com.android.adbkeyboard` to restore it from the system image.\n\n3) Alternative for DELETE_FAILED_INTERNAL_ERROR: Try `pm disable-user --user 0 com.android.adbkeyboard` first to disable the ghost entry before attempting uninstall.\n\n4) Detecting ghost packages: Compare output of `pm list packages` (installed) vs `pm list packages -u` (includes uninstalled-but-retained). Packages appearing only in -u output are ghosts.\n\n5) Fallback typing without ADBKeyboard: The codebase already has ClipboardHelper (setup_clipboard_helper.py) which sets clipboard via `am start -n com.geelark.clipboard/.CopyActivity -a com.geelark.clipboard.COPY --es base64 <b64text>`. After setting clipboard, use `input keyevent 279` (KEYCODE_PASTE) to paste content. This approach supports Unicode and emojis without requiring ADBKeyboard.\n\n6) Current setup_adbkeyboard.py (line 102) uses basic `pm uninstall` which fails on ghost packages. Fix requires updating to use `pm uninstall --user 0` approach.\n\nSources: XDA Forums, bayton.org, droidwin.com\n</info added on 2025-12-11T02:49:23.733Z>",
            "status": "done",
            "testStrategy": null,
            "updatedAt": "2025-12-11T02:49:43.751Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Probe Geelark cloud phones for ADBKeyboard package state and system app presence",
            "description": "Systematically inspect all relevant Geelark devices to understand current ADBKeyboard installation state, including ghost entries and potential system app copies.",
            "dependencies": [
              1
            ],
            "details": "On each Geelark phone (podmindstudio, miccliparchive, reelwisdompod_, talktrackhub), run a scripted adb diagnostic sequence: (1) `pm list packages | grep adbkeyboard`; (2) `pm list packages -s` and `-3` to see if it’s system or user; (3) `pm path com.android.adbkeyboard`; (4) `cmd package resolve-activity` and `dumpsys package com.android.adbkeyboard` to detect ghost entries or disabled states; (5) search filesystem for the APK (e.g., `/system/app`, `/system/priv-app`, `/product/app`) using `ls` patterns where allowed; (6) check `settings get secure default_input_method` and `ime list -a` to see if the IME is registered but disabled. Capture outputs in logs per device and infer whether each device has a system app copy, a broken/ghost entry, or no trace at all.\n<info added on 2025-12-11T02:52:30.810Z>\nDiagnosis Results:\n\n1) podmindstudio: INSTALLED and working - System app located at /system/app/AdbKeyboard/AdbKeyboard.apk. IME properly set to com.android.adbkeyboard/.AdbIME. No remediation needed.\n\n2) miccliparchive: GHOST PACKAGE - APK exists in /system/app but package uninstalled for user 0. Current IME set to Google keyboard (com.google.android.inputmethod.latin). Package appears in `pm list packages -u` but not in `pm list packages`. Remediation: Use `cmd package install-existing com.android.adbkeyboard` to restore system app for current user, then set IME.\n\n3) reelwisdompod_: GHOST PACKAGE - APK exists in /system/app but package uninstalled for user 0. IME setting still points to ADBKeyboard but keyboard non-functional since package not installed for user. Remediation: Same as miccliparchive - use `cmd package install-existing com.android.adbkeyboard` to restore.\n\n4) talktrackhub: NOT INSTALLED - Clean slate, no ADBKeyboard APK anywhere on the filesystem. No ghost package entries. Remediation options: (a) Copy APK from podmindstudio via `adb pull/push` and install, or (b) Use clipboard-based text input as fallback.\n\nFix Strategy for setup_adbkeyboard.py: Add detection logic to differentiate ghost package vs clean slate states. For ghost packages (miccliparchive, reelwisdompod_), use `cmd package install-existing com.android.adbkeyboard` instead of standard pm install. For clean installs (talktrackhub), either pull APK from working phone or use local ADBKeyboard.apk with pm install.\n</info added on 2025-12-11T02:52:30.810Z>",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-11T02:52:49.620Z"
          }
        ],
        "updatedAt": "2025-12-12T02:01:58.948Z"
      },
      {
        "id": "12",
        "title": "Investigate and fix empty Appium page_source on Android 15 Instagram sessions",
        "description": "Debug and instrument the Appium-based Android 15 setup so that page_source / dump_ui() returns a non-empty, correctly structured UI hierarchy for the Instagram app, and determine whether issues stem from app launch, hierarchy generation, or XML parsing.",
        "details": "Implementation plan:\n\n1. **Set up a focused Android 15/Appium debug harness**\n- Create a standalone Python script or test module (e.g. `debug/appium_source_debug.py`) that:\n  - Connects to the same Geelark Android 15 device configuration used in Task 11.\n  - Uses the same Appium capabilities (platformName, platformVersion, deviceName/UDID, automationName=UiAutomator2, appPackage/appActivity for Instagram, noReset, etc.).\n  - Logs all capabilities and the Appium server version at startup for reproducibility.\n- Ensure the harness is runnable independently from the main posting flow to speed up iteration.\n\n2. **Compare Appium page_source vs. uiautomator dump formats**\n- Use Appium’s `driver.page_source` and log the raw return value to a file (e.g. `artifacts/appium_source_raw.xml`) for multiple states: before Instagram launch, after launch, and after navigating to a known screen.[2]\n- On the same device and screen, use `adb shell uiautomator dump /sdcard/view.xml && adb pull /sdcard/view.xml artifacts/uiautomator_view.xml` and compare:\n  - Root element tag names and attributes (`hierarchy`, `node`, bounds, text, resource-id, content-desc).\n  - Character encoding and XML declaration.\n  - Presence/absence of expected views (e.g., Instagram home feed, buttons, bottom nav).\n- Document differences in a short markdown note (`docs/appium_vs_uiautomator.md`), highlighting any fields Appium normalizes or omits and confirming that Appium is returning **application hierarchy XML**, not a raw uiautomator dump.[2]\n\n3. **Verify that Instagram is truly launching and in foreground**\n- From the debug harness, add explicit steps:\n  - Call `driver.start_activity(appPackage, appActivity)` (or equivalent) and wait for a few seconds.\n  - Use `adb shell dumpsys window windows | grep -E 'mCurrentFocus|mFocusedApp'` to verify that the Instagram activity is in the foreground; log this output.\n  - Capture a screenshot via Appium (`driver.get_screenshot_as_png()`) and save to `artifacts/instagram_launch.png`; visually confirm the app is open.\n- If Appium connects but Instagram is not foregrounded, log this and add retries/explicit waits (e.g. wait for known accessibility id or resource-id) before calling `page_source`.\n\n4. **Instrument the page_source / dump_ui() call itself**\n- Wrap `driver.page_source` and any `dump_ui()` helper used in Task 11 in a small utility (e.g. `debug/get_hierarchy.py`) that:\n  - Measures call latency.\n  - Catches and logs exceptions.\n  - Logs the length of the returned XML string and the first 500–1000 characters.\n- Add verbose Appium server logging (log level `debug`) for these calls, capturing:\n  - The `Get Page Source` requests and responses.\n  - Any UiAutomator2/Android errors when traversing the hierarchy.\n- If `page_source` returns an empty hierarchy but no exception, investigate whether this is a known limitation with background apps, webviews, or Android 15 specifics.[1][6]\n\n5. **Check for webview / context or invisible-element issues**\n- Enumerate contexts using `driver.contexts` and log them; if a `WEBVIEW_` context exists for Instagram, switch contexts and compare `page_source` results to the native context.\n- Confirm whether the expected elements are off-screen or lazily created (e.g., lists or RecyclerViews)[3]; scroll a small amount and re-fetch `page_source` to see if the hierarchy populates.\n- Ensure that the harness requests **native context** when expecting native XML, and document how Instagram’s UI composition (native vs webview) affects what Appium can see.[4]\n\n6. **Rule out XML parsing issues in our code**\n- If Appium returns non-empty XML but our `dump_ui()` / parser reports no nodes, add unit-level diagnostics:\n  - Create a minimal parser module (e.g. `ui_parsing/xml_utils.py`) that loads the raw Appium XML using both `xml.etree.ElementTree` and `lxml` (if available) to handle any namespace/encoding quirks.\n  - Log any parsing errors, invalid characters, or namespace prefixes.\n  - Add defensive parsing: strip BOMs, normalize encoding to UTF‑8, and handle default namespaces.\n- Implement a small CLI (`python -m ui_parsing.debug_parse artifacts/appium_source_raw.xml`) that prints root tag, number of nodes, and a few sample attributes to quickly validate parsing.\n\n7. **Constrain work to Android 15 devices**\n- Ensure the harness inspects the device’s SDK level from `adb shell getprop ro.build.version.sdk` and asserts it is 35 (Android 15); otherwise, exit with a clear message.\n- If needed, parameterize the target device but keep the scope of this task to documenting and resolving the Android 15 behavior (other OS versions can be future work).\n\n8. **Output and documentation**\n- Produce a short troubleshooting doc `docs/android15_appium_empty_source.md` summarizing:\n  - Root cause(s): app not foregrounded, context mismatch, Android 15 UiAutomator behavior, or XML parsing bug.\n  - The final, recommended way to:\n    - Confirm Instagram is open.\n    - Fetch reliable page source.\n    - Parse and inspect the hierarchy.\n  - Any Appium capabilities or flags that improved results (e.g., waitForIdleTimeout, disableWindowAnimation, etc., if changed).\n- Expose any reusable utilities (e.g., `get_page_source_debug()`, `assert_instagram_foreground()`) in a `debug_utils` module so other tasks (like Task 11 and orchestrator work) can reuse them.\n",
        "testStrategy": "1. **Environment and connectivity sanity checks**\n- Run the debug harness against an Android 15 Geelark device and verify:\n  - Appium session is created without errors.\n  - Device SDK level is detected as 35; the script exits with an error on non‑15 devices.\n\n2. **Instagram launch verification**\n- Execute the harness with Instagram launch enabled and confirm:\n  - `dumpsys window` logs show an Instagram activity in `mCurrentFocus`/`mFocusedApp`.\n  - The saved screenshot clearly shows Instagram in the foreground.\n\n3. **Page source vs uiautomator comparison**\n- On the same screen, generate both `artifacts/appium_source_raw.xml` and `artifacts/uiautomator_view.xml`.\n- Manually inspect or script-compare them to confirm:\n  - Non-empty XML in both files.\n  - Similar numbers of nodes and presence of expected Instagram UI elements.\n\n4. **XML parsing validation**\n- Run the XML parser CLI against `appium_source_raw.xml` and verify it prints:\n  - Correct root element name.\n  - A positive node count (> 0).\n  - At least a few nodes with sensible attributes (e.g., text/resource-id not all empty).\n- Intentionally corrupt the XML file (e.g., truncate it) and confirm the parser reports clear parsing errors instead of silently returning zero nodes.\n\n5. **Context and visibility behavior tests**\n- From the harness, log `driver.contexts` and switch between native and any webview context, calling `page_source` in each and confirming non-empty output where expected.\n- Scroll within Instagram and re-run `page_source`, verifying the hierarchy updates and that elements entering/leaving the visible region appear/disappear from the XML.\n\n6. **Regression guard for empty source condition**\n- Add an automated check in the harness that fails if `page_source` length is below a small threshold (e.g., < 1 KB) while Instagram is reported as foreground.\n- Run the harness multiple times (at least 5) and confirm the check consistently passes on Android 15.\n\n7. **Documentation review**\n- Have a team member follow `docs/android15_appium_empty_source.md` on a fresh environment and verify they can reproduce the debug steps and obtain non-empty page source and parsed node counts without additional help.",
        "status": "done",
        "dependencies": [
          "4",
          "11"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:02:09.371Z"
      },
      {
        "id": "13",
        "title": "Apply Appium stability fixes with extended timeouts and crash recovery",
        "description": "Improve Appium connection reliability in post_reel_smart.py by adding missing timeout capabilities, increasing existing timeouts, implementing phone restart logic for UiAutomator2 crashes, and creating a typed exception for startup failures.",
        "details": "## Implementation Details\n\n### 1. Create typed UiAutomatorStartupError exception (top of file, after imports ~line 35)\n\n```python\nclass UiAutomatorStartupError(Exception):\n    \"\"\"Raised when UiAutomator2 fails to start on the device\"\"\"\n    pass\n```\n\n### 2. Update connect_appium() function (lines 730-763) with new capabilities\n\nAdd these capabilities to the `options` object:\n\n```python\ndef connect_appium(self, retries=3):\n    \"\"\"Connect Appium driver - REQUIRED for automation to work\"\"\"\n    print(\"Connecting Appium driver...\")\n\n    options = UiAutomator2Options()\n    options.platform_name = \"Android\"\n    options.automation_name = \"UiAutomator2\"\n    options.device_name = self.device\n    options.udid = self.device\n    options.no_reset = True\n    options.new_command_timeout = 60\n    \n    # Extended timeouts for stability (Android 15 devices need longer)\n    options.set_capability(\"appium:uiautomator2ServerLaunchTimeout\", 90000)  # NEW: 90s (was missing, defaulted to 30s)\n    options.set_capability(\"appium:uiautomator2ServerInstallTimeout\", 120000)  # INCREASED: 120s (was 60s)\n    options.set_capability(\"appium:adbExecTimeout\", 120000)  # INCREASED: 120s (was 30s)\n    options.set_capability(\"appium:androidDeviceReadyTimeout\", 60000)  # NEW: 60s device ready wait\n\n    last_error = None\n    for attempt in range(retries):\n        try:\n            self.appium_driver = webdriver.Remote(\n                command_executor=APPIUM_SERVER,\n                options=options\n            )\n            platform_ver = self.appium_driver.capabilities.get('platformVersion', 'unknown')\n            print(f\"  Appium connected! (Android {platform_ver})\")\n            return True\n        except Exception as e:\n            last_error = e\n            print(f\"  Appium connection failed (attempt {attempt + 1}/{retries}): {e}\")\n            self.appium_driver = None\n            \n            # Check if UiAutomator2 crashed - may need phone restart\n            if self.is_uiautomator2_crash(e):\n                print(f\"  [RECOVERY] UiAutomator2 crash detected, attempting phone restart...\")\n                self._restart_phone_for_recovery()\n            \n            if attempt < retries - 1:\n                print(f\"  Retrying in 15 seconds...\")  # INCREASED: 15s (was 5s)\n                time.sleep(15)\n\n    # All retries failed - raise typed exception\n    raise UiAutomatorStartupError(f\"Appium connection failed after {retries} attempts: {last_error}\")\n```\n\n### 3. Add phone restart recovery method (new method in SmartInstagramPoster class)\n\nAdd this method after `reconnect_appium()` (around line 84):\n\n```python\ndef _restart_phone_for_recovery(self):\n    \"\"\"Restart the Geelark phone to recover from UiAutomator2 crash\"\"\"\n    if not self.phone_id:\n        print(\"    Cannot restart phone - phone_id not set\")\n        return False\n    \n    try:\n        print(\"    Stopping phone...\")\n        self.client.stop_phone(self.phone_id)\n        time.sleep(5)\n        \n        print(\"    Starting phone...\")\n        self.client.start_phone(self.phone_id)\n        \n        # Wait for phone to boot (similar to connect() logic)\n        print(\"    Waiting for phone to boot...\")\n        for i in range(60):\n            time.sleep(2)\n            status_result = self.client.get_phone_status([self.phone_id])\n            items = status_result.get(\"successDetails\", [])\n            if items and items[0].get(\"status\") == 0:\n                print(f\"    Phone ready after restart! (took ~{(i+1)*2}s)\")\n                break\n            if i % 5 == 0:\n                print(f\"    Booting... ({(i+1)*2}s)\")\n        else:\n            print(\"    Warning: Phone boot timeout after restart\")\n            return False\n        \n        # Re-enable ADB after restart\n        time.sleep(3)\n        print(\"    Re-enabling ADB...\")\n        self.client.enable_adb(self.phone_id)\n        time.sleep(5)\n        \n        # Reconnect ADB\n        adb_info = self.client.get_adb_info(self.phone_id)\n        self.device = f\"{adb_info['ip']}:{adb_info['port']}\"\n        password = adb_info['pwd']\n        \n        import subprocess\n        subprocess.run([ADB_PATH, \"connect\", self.device], capture_output=True)\n        self.adb(f\"glogin {password}\")\n        time.sleep(3)\n        \n        print(\"    Phone restart recovery complete\")\n        return True\n        \n    except Exception as e:\n        print(f\"    Phone restart failed: {e}\")\n        return False\n```\n\n### 4. Update reconnect_appium() to use new exception (line 74-84)\n\n```python\ndef reconnect_appium(self):\n    \"\"\"Reconnect Appium driver after UiAutomator2 crash\"\"\"\n    print(\"  [RECOVERY] Reconnecting Appium driver...\")\n    try:\n        if self.appium_driver:\n            self.appium_driver.quit()\n    except:\n        pass\n    self.appium_driver = None\n    time.sleep(2)\n    try:\n        return self.connect_appium()\n    except UiAutomatorStartupError:\n        # If reconnect also fails, try phone restart\n        if self._restart_phone_for_recovery():\n            return self.connect_appium()\n        raise\n```\n\n### Summary of Changes\n\n| Item | Before | After |\n|------|--------|-------|\n| `uiautomator2ServerLaunchTimeout` | Missing (30s default) | 90000ms |\n| `uiautomator2ServerInstallTimeout` | 60000ms | 120000ms |\n| `adbExecTimeout` | 30000ms | 120000ms |\n| `androidDeviceReadyTimeout` | Missing | 60000ms |\n| Retry sleep | 5s | 15s |\n| Phone restart on crash | Not implemented | Implemented |\n| Typed exception | Generic Exception | UiAutomatorStartupError |\n\n### Files Modified\n- `post_reel_smart.py`: Add exception class, update `connect_appium()`, add `_restart_phone_for_recovery()`, update `reconnect_appium()`",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Tests for Exception Class\n- Verify `UiAutomatorStartupError` can be raised and caught\n- Verify it inherits from `Exception`\n- Verify error message is preserved correctly\n\n### 2. Timeout Configuration Tests\n- Start Appium with a mock device and verify the capabilities are set correctly:\n  - `uiautomator2ServerLaunchTimeout == 90000`\n  - `uiautomator2ServerInstallTimeout == 120000`\n  - `adbExecTimeout == 120000`\n  - `androidDeviceReadyTimeout == 60000`\n- Log the capabilities object before connection to verify values\n\n### 3. Retry Logic Tests\n- Mock Appium connection failures and verify:\n  - Retry happens 3 times\n  - Sleep between retries is 15 seconds (measure with time.time())\n  - `UiAutomatorStartupError` is raised after all retries fail\n\n### 4. Phone Restart Recovery Tests\n- Mock `is_uiautomator2_crash()` to return `True`\n- Verify `_restart_phone_for_recovery()` is called\n- Mock GeelarkClient methods (`stop_phone`, `start_phone`, `get_phone_status`, `enable_adb`, `get_adb_info`)\n- Verify the correct sequence of recovery calls\n\n### 5. Integration Test with Real Device\n```bash\n# Test on a Geelark Android 15 device\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\n\nposter = SmartInstagramPoster('test_phone_name')\nposter.connect()\n\n# Verify capabilities by checking the driver\ncaps = poster.appium_driver.capabilities\nprint(f'Platform: {caps.get(\\\"platformVersion\\\")}')\nprint(f'Appium connected successfully with extended timeouts')\n\nposter.cleanup()\n\"\n```\n\n### 6. Crash Recovery Simulation\n- Force a UiAutomator2 crash by killing the server process\n- Verify the recovery logic kicks in:\n  ```bash\n  # In a separate terminal while test is running:\n  adb shell \"pkill -f uiautomator\"\n  ```\n- Observe that phone restart and Appium reconnection occur\n\n### 7. End-to-End Test\n- Run full posting flow: `python post_reel_smart.py <phone> <video> <caption>`\n- Monitor logs for timeout-related errors\n- Verify no more \"30s timeout\" errors appear\n- Verify successful connection even under slow network conditions\n\n### 8. Regression Testing\n- Run the existing test suite to ensure no regressions\n- Verify `posting_scheduler.py` still works with the updated `connect_appium()`\n- Test with multiple concurrent phones to verify stability",
        "status": "done",
        "dependencies": [
          "11",
          "12"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:02:20.479Z"
      },
      {
        "id": "14",
        "title": "Analyze overnight scheduler run results (Dec 11-12)",
        "description": "Review batch_results_20251211*.csv files and scheduler logs to compute comprehensive metrics including success rates, error patterns, time correlations, and priority fixes needed.",
        "details": "## Implementation Details\n\n### 1. Create analysis script `analyze_scheduler_results.py`\n\n```python\nimport os\nimport csv\nimport json\nfrom datetime import datetime\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple, Optional\nimport statistics\n\nclass SchedulerAnalyzer:\n    def __init__(self, csv_pattern: str = \"batch_results_20251211*.csv\"):\n        self.csv_pattern = csv_pattern\n        self.records = []\n        \n    def load_data(self):\n        \"\"\"Load all matching CSV files\"\"\"\n        import glob\n        for filepath in glob.glob(self.csv_pattern):\n            with open(filepath, 'r', encoding='utf-8') as f:\n                reader = csv.DictReader(f)\n                for row in reader:\n                    row['source_file'] = filepath\n                    row['timestamp_parsed'] = datetime.fromisoformat(row['timestamp']) if row.get('timestamp') else None\n                    self.records.append(row)\n```\n\n### 2. Metric Calculations\n\n#### Success Rate by Account\n```python\ndef success_rate_by_account(self) -> Dict[str, dict]:\n    \"\"\"Calculate success/fail/error counts per phone/account\"\"\"\n    by_account = defaultdict(lambda: {'success': 0, 'failed': 0, 'error': 0, 'total': 0})\n    for r in self.records:\n        account = r.get('phone', 'unknown')\n        status = r.get('status', 'unknown')\n        by_account[account][status] = by_account[account].get(status, 0) + 1\n        by_account[account]['total'] += 1\n    # Calculate rates\n    for acc, data in by_account.items():\n        data['success_rate'] = data['success'] / data['total'] * 100 if data['total'] > 0 else 0\n    return dict(sorted(by_account.items(), key=lambda x: x[1]['success_rate']))\n```\n\n#### Success Rate by Hour\n```python\ndef success_rate_by_hour(self) -> Dict[int, dict]:\n    \"\"\"Calculate success rates grouped by hour of day\"\"\"\n    by_hour = defaultdict(lambda: {'success': 0, 'total': 0})\n    for r in self.records:\n        if r.get('timestamp_parsed'):\n            hour = r['timestamp_parsed'].hour\n            by_hour[hour]['total'] += 1\n            if r.get('status') == 'success':\n                by_hour[hour]['success'] += 1\n    for hour, data in by_hour.items():\n        data['success_rate'] = data['success'] / data['total'] * 100 if data['total'] > 0 else 0\n    return dict(sorted(by_hour.items()))\n```\n\n#### Error Type Classification\n```python\ndef classify_errors(self) -> Dict[str, List[dict]]:\n    \"\"\"Categorize errors by type based on error message patterns\"\"\"\n    error_patterns = {\n        'upload_timeout': ['Upload timeout', 'status: 1'],\n        'uiautomator_crash': ['UiAutomator2', 'instrumentation process is not running', 'crashed'],\n        'adb_timeout': ['timed out after', 'adb.exe'],\n        'connection_failed': ['connection', 'offline', 'refused'],\n        'instagram_blocked': ['action blocked', 'suspended', 'captcha'],\n    }\n    \n    classified = defaultdict(list)\n    for r in self.records:\n        if r.get('status') in ['error', 'failed']:\n            error_msg = r.get('error', '')\n            error_type = 'unknown'\n            for etype, patterns in error_patterns.items():\n                if any(p.lower() in error_msg.lower() for p in patterns):\n                    error_type = etype\n                    break\n            classified[error_type].append(r)\n    return dict(classified)\n```\n\n#### Average Attempts Before Success\n```python\ndef avg_attempts_before_success(self) -> dict:\n    \"\"\"Calculate average attempts needed for successful posts.\n    Requires correlation with scheduler_state.json for attempt tracking.\"\"\"\n    # Load from scheduler_state.json if available\n    state_file = \"scheduler_state.json\"\n    attempts_data = []\n    try:\n        with open(state_file, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        for job in data.get('jobs', []):\n            if job.get('status') == 'success':\n                attempts_data.append(job.get('attempts', 1))\n    except Exception:\n        pass\n    \n    if attempts_data:\n        return {\n            'mean': statistics.mean(attempts_data),\n            'median': statistics.median(attempts_data),\n            'max': max(attempts_data),\n            'samples': len(attempts_data)\n        }\n    return {'error': 'No attempt data available'}\n```\n\n#### Phones with Highest Failure Rates\n```python\ndef phones_by_failure_rate(self, min_attempts: int = 2) -> List[Tuple[str, float, int]]:\n    \"\"\"Return phones sorted by failure rate (highest first)\"\"\"\n    rates = self.success_rate_by_account()\n    failures = []\n    for phone, data in rates.items():\n        if data['total'] >= min_attempts:\n            failure_rate = 100 - data['success_rate']\n            failures.append((phone, failure_rate, data['total']))\n    return sorted(failures, key=lambda x: -x[1])\n```\n\n#### Time Patterns in Failures\n```python\ndef failure_time_patterns(self) -> dict:\n    \"\"\"Analyze when failures occur - time of day, day of week, gaps between attempts\"\"\"\n    failures_by_hour = defaultdict(int)\n    failures_by_minute_bucket = defaultdict(int)  # 10-min buckets\n    \n    for r in self.records:\n        if r.get('status') in ['error', 'failed'] and r.get('timestamp_parsed'):\n            ts = r['timestamp_parsed']\n            failures_by_hour[ts.hour] += 1\n            bucket = ts.hour * 6 + ts.minute // 10\n            failures_by_minute_bucket[bucket] += 1\n    \n    return {\n        'by_hour': dict(failures_by_hour),\n        'peak_failure_hour': max(failures_by_hour.items(), key=lambda x: x[1]) if failures_by_hour else None,\n        'failure_distribution': failures_by_minute_bucket\n    }\n```\n\n#### Video Size Correlation (placeholder - needs video file access)\n```python\ndef video_size_correlation(self, video_folder: str = \"chunk_01c\") -> dict:\n    \"\"\"Correlate video file sizes with success/failure rates.\n    Requires access to video files to get sizes.\"\"\"\n    # Map shortcodes to file sizes\n    shortcode_sizes = {}\n    success_sizes = []\n    fail_sizes = []\n    \n    # Walk video folder to build size map\n    for root, dirs, files in os.walk(video_folder):\n        for f in files:\n            if f.endswith('.mp4'):\n                shortcode = f.replace('.mp4', '')\n                path = os.path.join(root, f)\n                shortcode_sizes[shortcode] = os.path.getsize(path)\n    \n    for r in self.records:\n        shortcode = r.get('shortcode', '')\n        if shortcode in shortcode_sizes:\n            size_mb = shortcode_sizes[shortcode] / (1024 * 1024)\n            if r.get('status') == 'success':\n                success_sizes.append(size_mb)\n            else:\n                fail_sizes.append(size_mb)\n    \n    return {\n        'avg_success_size_mb': statistics.mean(success_sizes) if success_sizes else 0,\n        'avg_fail_size_mb': statistics.mean(fail_sizes) if fail_sizes else 0,\n        'success_samples': len(success_sizes),\n        'fail_samples': len(fail_sizes),\n    }\n```\n\n### 3. Report Generator\n\n```python\ndef generate_report(self) -> str:\n    \"\"\"Generate a comprehensive markdown report\"\"\"\n    report = []\n    report.append(\"# Scheduler Run Analysis Report - Dec 11, 2025\\n\")\n    \n    # Overall stats\n    total = len(self.records)\n    success = sum(1 for r in self.records if r.get('status') == 'success')\n    report.append(f\"## Overall Statistics\")\n    report.append(f\"- Total attempts: {total}\")\n    report.append(f\"- Successful: {success} ({success/total*100:.1f}%)\")\n    report.append(f\"- Failed/Error: {total - success}\")\n    \n    # Add each metric section...\n    # (success by account, by hour, error types, etc.)\n    \n    # Priority Fixes section\n    report.append(\"\\n## Priority Fixes Needed\")\n    errors = self.classify_errors()\n    if errors.get('upload_timeout'):\n        report.append(\"1. **Upload Timeout** - Increase upload timeout beyond 180s or implement chunked upload\")\n    if errors.get('uiautomator_crash'):\n        report.append(\"2. **UiAutomator2 Crashes** - Implement phone restart recovery per Task 13\")\n    \n    return \"\\n\".join(report)\n```\n\n### 4. CLI Interface\n\n```python\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser(description='Analyze scheduler results')\n    parser.add_argument('--date', default='20251211', help='Date pattern YYYYMMDD')\n    parser.add_argument('--output', default='scheduler_analysis_report.md', help='Output report file')\n    parser.add_argument('--json', action='store_true', help='Output raw data as JSON')\n    \n    args = parser.parse_args()\n    \n    analyzer = SchedulerAnalyzer(f\"batch_results_{args.date}*.csv\")\n    analyzer.load_data()\n    \n    if args.json:\n        data = {\n            'by_account': analyzer.success_rate_by_account(),\n            'by_hour': analyzer.success_rate_by_hour(),\n            'errors': analyzer.classify_errors(),\n            'failure_patterns': analyzer.failure_time_patterns(),\n        }\n        print(json.dumps(data, indent=2, default=str))\n    else:\n        report = analyzer.generate_report()\n        with open(args.output, 'w', encoding='utf-8') as f:\n            f.write(report)\n        print(f\"Report saved to {args.output}\")\n```\n\n### 5. Files to Read\n\n- `batch_results_20251211*.csv` - All CSV files from Dec 11 runs\n- `scheduler_state.json` - For attempt counts and job metadata\n- `geelark_batch.log` - For detailed error stack traces and phase timing\n- `chunk_01c/` - Video folder for file size analysis",
        "testStrategy": "## Test Strategy\n\n### 1. Data Loading Tests\n- Verify all Dec 11 CSV files are found and loaded (expect ~14 files based on glob results)\n- Confirm all expected columns are present: shortcode, phone, status, error, timestamp\n- Test handling of empty error fields and malformed timestamps\n\n### 2. Metric Calculation Validation\n- **Success rate by account**: Cross-reference with manual count from sample CSV files\n- **Success rate by hour**: Verify hour extraction from ISO timestamps (e.g., \"2025-12-11T18:22:34\" → hour 18)\n- **Error classification**: Test pattern matching against known error strings:\n  - \"Upload timeout after 180s (last status: 1)\" → upload_timeout\n  - \"UiAutomator2 server...instrumentation process is not running\" → uiautomator_crash\n  - \"timed out after 30 seconds\" → adb_timeout\n\n### 3. Report Verification\n- Run analysis and verify report includes all 7 requested metrics\n- Compare overall success count with sum across all CSVs\n- Verify phones with highest failure rates list shows accounts that appear in error records\n\n### 4. Edge Cases\n- Test with empty CSV files\n- Test with single-record files\n- Test when scheduler_state.json is unavailable or malformed\n- Test when video folder doesn't exist (video size correlation should gracefully report 0 samples)\n\n### 5. Manual Spot-Check\n```bash\n# Quick validation commands\npython analyze_scheduler_results.py --json | jq '.by_account | length'\n# Should return number of unique accounts\n\npython analyze_scheduler_results.py --json | jq '.errors | keys'\n# Should show error type categories found\n```\n\n### 6. Cross-Reference with Raw Data\n- Compare report findings with direct CSV inspection\n- Verify error messages in report match actual error strings from CSVs\n- Confirm time patterns align with file timestamps on batch_results_*.csv files",
        "status": "pending",
        "dependencies": [
          "2",
          "9"
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": "15",
        "title": "Integrate reliability features into posting_scheduler worker loop",
        "description": "Wire up existing but unused reliability mechanisms (Appium health checks, account cooldown backoff) into the scheduler's worker loop, add a heartbeat thread to keep the lock file fresh, and classify infrastructure errors to trigger account-level backoff.",
        "details": "## Current State Analysis\n\nThe codebase already has several reliability features that are **implemented but NOT wired up**:\n\n1. **Single-instance lock** (lines 30-111): Fully working but uses static lock file without heartbeat\n2. **Appium health checks** (lines 114-170): `check_appium_health()` and `restart_appium()` exist but never called\n3. **Account cooldown** (lines 435-471): `is_on_cooldown()` and `record_post(is_infra_error)` exist but:\n   - `is_on_cooldown()` is NOT checked in `get_next_job()` (line 661-703)\n   - `record_post()` is always called with just `False`, never passing `is_infra_error=True` (line 815)\n\n## Implementation Details\n\n### 1. Add Heartbeat Thread for Lock Freshness\n\nUpdate the lock file with a timestamp periodically so other instances can detect truly stale locks:\n\n```python\n# Add to PostingScheduler.__init__()\nself.heartbeat_thread: Optional[threading.Thread] = None\nself.heartbeat_interval = 30  # seconds\n\n# Add heartbeat method\ndef _heartbeat_loop(self):\n    \"\"\"Periodically update lock file to prove we're still alive\"\"\"\n    while self.running:\n        try:\n            if os.path.exists(LOCK_FILE):\n                with open(LOCK_FILE, 'r') as f:\n                    lock_data = json.load(f)\n                if lock_data.get('pid') == os.getpid():\n                    lock_data['last_heartbeat'] = datetime.now().isoformat()\n                    with open(LOCK_FILE, 'w') as f:\n                        json.dump(lock_data, f)\n        except Exception as e:\n            logger.warning(f\"Heartbeat error: {e}\")\n        time.sleep(self.heartbeat_interval)\n```\n\nUpdate `acquire_lock()` to check heartbeat staleness:\n```python\n# In acquire_lock(), after is_process_running check:\nlast_heartbeat = lock_data.get('last_heartbeat')\nif last_heartbeat:\n    hb_time = datetime.fromisoformat(last_heartbeat)\n    stale_threshold = timedelta(minutes=2)  # 2 minutes without heartbeat = stale\n    if datetime.now() - hb_time > stale_threshold:\n        print(f\"[LOCK] Lock heartbeat stale ({hb_time}). Taking over.\")\n        # Proceed to take over\n```\n\n### 2. Integrate Appium Health Check into Worker Loop\n\nIn `_worker_loop()`, before processing a job:\n\n```python\ndef _worker_loop(self):\n    \"\"\"Main worker loop\"\"\"\n    self._log(\"Worker started\")\n    \n    # Track consecutive Appium failures for restart logic\n    appium_consecutive_failures = 0\n    max_appium_failures_before_restart = 3\n    \n    while self.running:\n        if self.paused:\n            time.sleep(1)\n            continue\n        \n        # Check Appium health before each job\n        if not check_appium_health():\n            self._log(\"[APPIUM] Health check failed\")\n            appium_consecutive_failures += 1\n            \n            if appium_consecutive_failures >= max_appium_failures_before_restart:\n                self._log(\"[APPIUM] Attempting auto-restart...\")\n                if restart_appium():\n                    appium_consecutive_failures = 0\n                else:\n                    self._log(\"[APPIUM] Restart failed, waiting 60s...\")\n                    time.sleep(60)\n                    continue\n            else:\n                time.sleep(10)\n                continue\n        else:\n            appium_consecutive_failures = 0  # Reset on success\n        \n        job = self.get_next_job()\n        # ... rest of loop\n```\n\n### 3. Integrate Account Cooldown into get_next_job()\n\nUpdate `get_next_job()` to filter out accounts on cooldown:\n\n```python\ndef get_next_job(self) -> Optional[PostJob]:\n    \"\"\"Get next job that's ready to post\"\"\"\n    accounts_posted_today = get_accounts_posted_today()\n    \n    # Filter: can post today AND not on cooldown\n    available_accounts = [\n        acc for acc in self.accounts.values()\n        if acc.can_post_today(self.posts_per_account_per_day)\n        and acc.name not in accounts_posted_today\n        and not acc.is_on_cooldown()  # ADD THIS LINE\n    ]\n    # ... rest of method unchanged\n```\n\n### 4. Classify Infrastructure Errors in execute_job()\n\nUpdate the error handling in `execute_job()` to detect infrastructure errors:\n\n```python\n# In execute_job(), in the except block (around line 783):\nexcept Exception as e:\n    error_msg = str(e)\n    error_type_name = type(e).__name__\n    \n    # Classify infrastructure errors\n    infra_error_patterns = [\n        'ADB', 'adb', 'device offline', 'glogin', 'phone not running',\n        'Appium', 'appium', 'UiAutomator', 'WebDriver', \n        'connection refused', 'timeout', 'Timeout'\n    ]\n    is_infra_error = any(pattern in error_msg for pattern in infra_error_patterns) or \\\n                     any(pattern in error_type_name for pattern in infra_error_patterns)\n    \n    job.last_error = f\"[{phase}] {error_type_name}: {error_msg}\"\n    \n    # ... existing error handling ...\n    \n    # Pass is_infra_error to trigger backoff\n    self.accounts[job.account].record_post(False, is_infra_error=is_infra_error)\n```\n\n### 5. Start Heartbeat Thread in start()\n\n```python\ndef start(self):\n    \"\"\"Start the scheduler\"\"\"\n    if self.running:\n        return\n    \n    # ... existing phone cleanup ...\n    \n    self.running = True\n    self.paused = False\n    \n    # Start heartbeat thread\n    self.heartbeat_thread = threading.Thread(target=self._heartbeat_loop, daemon=True)\n    self.heartbeat_thread.start()\n    self._log(\"[HEARTBEAT] Started heartbeat thread\")\n    \n    # Start worker thread\n    self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)\n    self.worker_thread.start()\n    self._log(\"Scheduler started\")\n```\n\n### 6. Add Account Cooldown Status to get_stats()\n\n```python\ndef get_stats(self) -> dict:\n    \"\"\"Get current statistics\"\"\"\n    accounts_on_cooldown = [acc.name for acc in self.accounts.values() if acc.is_on_cooldown()]\n    \n    return {\n        # ... existing stats ...\n        'accounts_on_cooldown': accounts_on_cooldown,\n    }\n```\n\n## Files to Modify\n\n- `posting_scheduler.py`: All changes concentrated in this single file",
        "testStrategy": "## Test Strategy\n\n### 1. Single-Instance Lock with Heartbeat Tests\n\n**Test stale lock detection:**\n```bash\n# Create a stale lock file manually\necho '{\"pid\": 99999, \"started\": \"2024-01-01T00:00:00\", \"last_heartbeat\": \"2024-01-01T00:00:00\"}' > scheduler.lock\n\n# Run scheduler - should take over the stale lock\npython posting_scheduler.py --status\n# Expected: \"Lock heartbeat stale\" message, then acquires lock\n```\n\n**Test heartbeat updates:**\n```bash\n# Start scheduler in background\npython posting_scheduler.py --add-folder chunk_test --add-accounts test1 --run &\n\n# Check lock file updates every 30s\nwatch -n 10 'cat scheduler.lock | python -m json.tool | grep last_heartbeat'\n# Expected: last_heartbeat timestamp updates every ~30 seconds\n```\n\n**Test duplicate instance prevention:**\n```bash\n# Terminal 1: Start scheduler\npython posting_scheduler.py --run\n\n# Terminal 2: Try to start another\npython posting_scheduler.py --run\n# Expected: \"[LOCK ERROR] Another scheduler instance is already running!\"\n```\n\n### 2. Appium Health Check Integration Tests\n\n**Test health check detection:**\n```bash\n# Stop Appium server\ntaskkill /F /IM node.exe\n\n# Run scheduler - should detect Appium down\npython posting_scheduler.py --run\n# Expected: \"[APPIUM] Health check failed\" messages\n```\n\n**Test auto-restart:**\n```bash\n# With Appium stopped, scheduler should attempt restart after 3 failures\n# Expected log sequence:\n# [APPIUM] Health check failed (1)\n# [APPIUM] Health check failed (2) \n# [APPIUM] Health check failed (3)\n# [APPIUM] Attempting auto-restart...\n# [APPIUM] Server ready on port 4723\n```\n\n### 3. Account Cooldown Integration Tests\n\n**Test cooldown filtering in get_next_job:**\n```python\n# Unit test\nscheduler = PostingScheduler()\nscheduler.add_account(\"test1\")\nscheduler.accounts[\"test1\"].cooldown_until = (datetime.now() + timedelta(minutes=10)).isoformat()\n\n# get_next_job should not return jobs for test1\njob = scheduler.get_next_job()\nassert job is None or job.account != \"test1\"\n```\n\n**Test infrastructure error classification:**\n```python\n# Simulate infra error in execute_job\n# After 3 consecutive failures, account should be on cooldown\nassert scheduler.accounts[\"test1\"].is_on_cooldown() == True\nassert scheduler.accounts[\"test1\"].consecutive_failures >= 3\n```\n\n### 4. Status Command Verification\n\n```bash\npython posting_scheduler.py --status\n# Expected output includes:\n# - Lock status with last_heartbeat timestamp\n# - Accounts on cooldown list (if any)\n# - Appium health status\n```\n\n### 5. Error Log Verification\n\nAfter a test run with simulated failures:\n```bash\ngrep \"is_infra_error\" geelark_batch.log\n# Should show infrastructure errors being correctly classified\n\ngrep \"on cooldown\" geelark_batch.log  \n# Should show accounts being put on cooldown after consecutive failures\n```\n\n### 6. Integration Test with Real Posting\n\n```bash\n# Run with a small test batch\npython posting_scheduler.py --add-folder chunk_test --add-accounts phone1 --run\n\n# Monitor logs for:\n# 1. Heartbeat updates\n# 2. Appium health checks before each job\n# 3. Proper cooldown behavior if failures occur\n# 4. Clean shutdown releasing lock\n```",
        "status": "done",
        "dependencies": [
          "9",
          "11",
          "13"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-12T01:42:53.832Z"
      },
      {
        "id": "16",
        "title": "Ensure ANDROID_HOME / ANDROID_SDK_ROOT Are Recognized by Appium Server",
        "description": "Make Appium reliably detect the Android SDK by standardizing how ANDROID_HOME and ANDROID_SDK_ROOT are set, exported, and propagated into the Appium server process across all deployment environments.",
        "details": "## Goal\nGuarantee that when the Appium server is started (locally, via scripts, or inside workers/containers), it always has valid access to the Android SDK through **ANDROID_HOME** and/or **ANDROID_SDK_ROOT**, so errors like “Neither ANDROID_HOME nor ANDROID_SDK_ROOT environment variable was exported” do not occur.[7][8]\n\n## High-Level Approach\n1. **Standardize environment variable configuration** for Android SDK on all supported OSes (Linux/macOS; Windows only if relevant).\n2. **Ensure variables are set in a *non-interactive* context** (systemd services, cron, Docker, background workers), not just in interactive shells.[7]\n3. **Unify Appium startup** through a single entry point (Python helper or shell script) that validates and, if needed, sets or maps ANDROID_SDK_ROOT/ANDROID_HOME before launching the server.\n4. **Add diagnostics** so misconfiguration is obvious in logs.\n\n## Implementation Steps\n\n### 1. Discover Current SDK Paths and Usage\n- Inspect how Appium is currently started:\n  - Python wrapper (e.g., `post_reel_smart.py` / scheduler), direct `appium` CLI, Docker, or a service unit.\n  - Note whether `appium` is started via `subprocess` in Python.\n- On at least one working dev machine and one production-like host:\n  - Run `echo $ANDROID_HOME` and `echo $ANDROID_SDK_ROOT` (or `set` on Windows) to see what is set.[4][6]\n  - Run `sdkmanager --list` from the same shell that starts Appium to confirm SDK accessibility.\n  - If using Android Studio, open SDK Manager and capture the **Android SDK Location** to use as canonical ANDROID_HOME.[5][6]\n\n### 2. Standard OS-Level Environment Setup (Best Practices)\nFollow current best-practice patterns for SDK env configuration so that Appium’s CLI sees them by default.[2][4][5][6]\n\n**Linux/macOS:**\n- In the system or service user profile, set (example):\n  ```bash\n  export ANDROID_HOME=\"$HOME/Android/Sdk\"\n  export ANDROID_SDK_ROOT=\"$ANDROID_HOME\"\n  export PATH=\"$PATH:$ANDROID_HOME/emulator:$ANDROID_HOME/platform-tools:$ANDROID_HOME/tools:$ANDROID_HOME/tools/bin\"\n  ```[4][5][6]\n- Add to the appropriate file for non-interactive shells (e.g., `/etc/profile.d/android-sdk.sh` or the service user’s `.profile`), not just `.bashrc`.\n\n**Windows (if used for Appium host):**\n- In *System Properties → Environment Variables*:\n  - Add **ANDROID_HOME** and/or **ANDROID_SDK_ROOT** pointing to the SDK directory (e.g., `C:\\Users\\<User>\\AppData\\Local\\Android\\Sdk`).[2][3][5][6]\n  - Add to **PATH**:\n    - `%ANDROID_HOME%\\emulator`\n    - `%ANDROID_HOME%\\platform-tools`\n    - `%ANDROID_HOME%\\tools`\n    - `%ANDROID_HOME%\\tools\\bin`[2][5]\n- Reboot or restart relevant services after setting system variables.[5]\n\nDocument the canonical SDK path and env configuration in `docs/appium_env.md` so all environments can be made consistent.\n\n### 3. Central Appium Launcher With Env Validation\nCreate a central launcher responsible for starting Appium with a guaranteed-good environment.\n\n**Option A – Shell wrapper (for CLI/containers):**\n- Add a script `scripts/start_appium.sh`:\n  ```bash\n  #!/usr/bin/env bash\n  set -euo pipefail\n\n  # 1. Infer or normalize SDK env\n  if [[ -z \"${ANDROID_HOME:-}\" && -n \"${ANDROID_SDK_ROOT:-}\" ]]; then\n    export ANDROID_HOME=\"$ANDROID_SDK_ROOT\"\n  elif [[ -z \"${ANDROID_SDK_ROOT:-}\" && -n \"${ANDROID_HOME:-}\" ]]; then\n    export ANDROID_SDK_ROOT=\"$ANDROID_HOME\"\n  fi\n\n  # 2. Fallback: attempt to detect SDK in common locations (optional)\n  if [[ -z \"${ANDROID_HOME:-}\" ]]; then\n    for candidate in \"$HOME/Android/Sdk\" \\\n                    \"$HOME/Library/Android/sdk\" \\\n                    \"/usr/local/android-sdk\"; do\n      if [[ -d \"$candidate/platform-tools\" ]]; then\n        export ANDROID_HOME=\"$candidate\"\n        export ANDROID_SDK_ROOT=\"$candidate\"\n        break\n      fi\n    done\n  fi\n\n  # 3. Validate\n  if [[ -z \"${ANDROID_HOME:-}\" || ! -d \"$ANDROID_HOME/platform-tools\" ]]; then\n    echo \"[FATAL] ANDROID_HOME/ANDROID_SDK_ROOT not set or invalid. Please install Android SDK and configure env vars.\" >&2\n    exit 1\n  fi\n\n  export PATH=\"$PATH:$ANDROID_HOME/emulator:$ANDROID_HOME/platform-tools:$ANDROID_HOME/tools:$ANDROID_HOME/tools/bin\"\n\n  echo \"[INFO] Using ANDROID_HOME=$ANDROID_HOME\" >&2\n  echo \"[INFO] Using ANDROID_SDK_ROOT=${ANDROID_SDK_ROOT:-$ANDROID_HOME}\" >&2\n\n  # 4. Finally run Appium\n  exec appium \"$@\"\n  ```\n- Ensure all automation (scheduler, local dev docs, CI, systemd unit) uses this script instead of invoking `appium` directly.\n\n**Option B – Python-side launcher (if Appium is started from Python):**\n- Implement a helper (e.g., in a shared module `appium_env.py`):\n  ```python\n  import os\n  import shutil\n  import subprocess\n\n  class AndroidEnvError(RuntimeError):\n      pass\n\n  def ensure_android_env() -> dict:\n      env = os.environ.copy()\n      home = env.get(\"ANDROID_HOME\")\n      root = env.get(\"ANDROID_SDK_ROOT\")\n\n      if not home and root:\n          home = root\n          env[\"ANDROID_HOME\"] = root\n      elif not root and home:\n          root = home\n          env[\"ANDROID_SDK_ROOT\"] = home\n\n      if not home:\n          # Optional: probe common locations\n          for candidate in [\n              os.path.expanduser(\"~/Android/Sdk\"),\n              os.path.expanduser(\"~/Library/Android/sdk\"),\n              \"/usr/local/android-sdk\",\n          ]:\n              if os.path.isdir(os.path.join(candidate, \"platform-tools\")):\n                  home = root = candidate\n                  env[\"ANDROID_HOME\"] = candidate\n                  env[\"ANDROID_SDK_ROOT\"] = candidate\n                  break\n\n      if not home or not os.path.isdir(os.path.join(home, \"platform-tools\")):\n          raise AndroidEnvError(\n              \"ANDROID_HOME/ANDROID_SDK_ROOT not set or invalid; install Android SDK and configure env vars.\"\n          )\n\n      pt = os.path.join(home, \"platform-tools\")\n      emulator = os.path.join(home, \"emulator\")\n      tools = os.path.join(home, \"tools\")\n      tools_bin = os.path.join(tools, \"bin\")\n      extra = os.pathsep.join(p for p in [pt, emulator, tools, tools_bin] if os.path.isdir(p))\n      if extra:\n          env[\"PATH\"] = env.get(\"PATH\", \"\") + os.pathsep + extra\n\n      return env\n\n  def start_appium_server(args: list[str]) -> subprocess.Popen:\n      env = ensure_android_env()\n      appium_cmd = shutil.which(\"appium\") or \"appium\"\n      return subprocess.Popen([appium_cmd, *args], env=env)\n  ```\n- Refactor all places that start Appium (e.g., utilities used by Task 11 and 13 flows) to use `start_appium_server` instead of raw `subprocess.Popen`.\n\n### 4. Integrate with Existing Reliability / Health Logic\n- In the same place where Appium health checks and restarts are wired (Task 15) and connection stability is being improved (Task 13), ensure the restart path *also* uses the standardized launcher so restarted servers see the correct env.\n- When an Appium startup or health check fails due to env problems (e.g., server logs mention missing `adb` or ANDROID_HOME), log a distinct error code / message so future analysis (Task 14) can differentiate env configuration problems from device/Appium bugs.\n\n### 5. Diagnostics and Logging\n- At Appium startup, log the detected **ANDROID_HOME**, **ANDROID_SDK_ROOT**, and whether `adb` is found on PATH (e.g., `which adb` / `where adb`).\n- Optionally, run a lightweight `adb version` and `adb devices` check immediately after starting the server and log the output to quickly spot SDK vs. device issues.[4][6]\n- Update developer/ops documentation with:\n  - Required env vars and their purpose.\n  - Example configuration snippets for each OS.\n  - How to run `appium-doctor --android` to validate setup before running tests.[1][2][6]\n\n### 6. CI / Container Integration (If Applicable)\n- For Docker images, bake the SDK and env variables into the image:\n  ```dockerfile\n  ENV ANDROID_HOME=/opt/android-sdk \\\n      ANDROID_SDK_ROOT=/opt/android-sdk\n  ENV PATH=\"$PATH:$ANDROID_HOME/emulator:$ANDROID_HOME/platform-tools:$ANDROID_HOME/tools:$ANDROID_HOME/tools/bin\"\n  ```\n- Ensure the CI job that runs mobile tests uses either the shell or Python launcher above.\n\n## Notes / Best Practices\n- Prefer **ANDROID_SDK_ROOT** (more modern) but keep **ANDROID_HOME** for compatibility; set both to the same directory.[7]\n- Always ensure that at least `platform-tools` and `emulator` are on PATH for Appium Android testing.[2][4][5][6]\n- When changing system environment variables on Windows, restart services or the whole machine so Appium inherits them.[5]\n",
        "testStrategy": "1. **Env Sanity Checks**\n- On each supported OS:\n  - Open a shell configured the same way the Appium server is started (service user, CI container, or scheduler process).\n  - Run `echo $ANDROID_HOME` / `echo $ANDROID_SDK_ROOT` (or `set ANDROID_` on Windows) and confirm they point to the actual SDK directory.\n  - Run `adb version` and confirm it succeeds.\n  - Run `appium-doctor --android` and verify there are no Android SDK-related errors.[1][2][6]\n\n2. **Launcher-Level Tests (Shell Wrapper)**\n- Temporarily unset ANDROID_HOME/ANDROID_SDK_ROOT, then:\n  - Create a mock SDK directory at a common default path with a dummy `platform-tools` folder.\n  - Run `scripts/start_appium.sh --log-level debug` and confirm:\n    - The script discovers the SDK and sets ANDROID_HOME/ANDROID_SDK_ROOT (check printed logs).\n    - `adb` from the mock SDK is picked up (check `which adb` output if added).\n- Set only ANDROID_HOME and confirm the wrapper mirrors it to ANDROID_SDK_ROOT and logs both.\n- Set only ANDROID_SDK_ROOT and confirm the wrapper mirrors it to ANDROID_HOME.\n- Intentionally point ANDROID_HOME to a non-existent directory and verify the script exits non‑zero with a clear fatal error message.\n\n3. **Launcher-Level Tests (Python Helper, if implemented)**\n- Unit-test `ensure_android_env()` using `monkeypatch`/`os.environ` manipulation:\n  - Case: both vars absent, no SDK dirs → expect `AndroidEnvError`.\n  - Case: only ANDROID_HOME set → expect ANDROID_SDK_ROOT to be added and PATH extended.\n  - Case: only ANDROID_SDK_ROOT set → expect ANDROID_HOME to be added and PATH extended.\n  - Case: neither set but a test SDK directory exists in a probed path → expect both vars to be set to that directory.\n- Unit-test `start_appium_server()` by stubbing `subprocess.Popen` and asserting it receives an `env` with properly set ANDROID_HOME/ANDROID_SDK_ROOT and PATH.\n\n4. **Integration Test with Appium and Device**\n- From the worker/scheduler context that will run real jobs:\n  - Start Appium using the new launcher (shell or Python).\n  - Check the Appium server logs to confirm:\n    - ANDROID_HOME/ANDROID_SDK_ROOT values are logged as expected.\n    - No warnings like \"Neither ANDROID_HOME nor ANDROID_SDK_ROOT environment variable was exported\" appear.[8]\n  - Run a minimal Android session (e.g., from Task 11’s test harness):\n    - Create an Appium session to a real or cloud Android device.\n    - Verify the session initializes, `adb devices` lists the device, and a simple `driver.get_page_source()` succeeds.\n\n5. **Failure-Mode Regression Test**\n- Temporarily misconfigure env (e.g., unset ANDROID_HOME in the service config) and start Appium through the new launcher:\n  - Confirm the launcher fails fast with a clear error instead of starting a broken server.\n  - Ensure higher-level reliability/health logic (from Task 13 and Task 15) logs an explicit env-configuration error category and does not enter an infinite restart loop.\n\n6. **Documentation Validation**\n- Follow the updated `docs/appium_env.md` from a clean machine:\n  - Configure the SDK and env exactly as documented.\n  - Start Appium using the documented command.\n  - Confirm that an Android session can be created without additional manual tweaks, demonstrating the docs are accurate and sufficient.",
        "status": "done",
        "dependencies": [
          "11",
          "13"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-12T01:54:53.017Z"
      },
      {
        "id": "17",
        "title": "Reduce UiAutomator2 launch timeout from 90s to 10s",
        "description": "Optimize the Appium UiAutomator2 server launch timeout based on the observation that instrumentation either starts immediately (~1s) or times out completely - there is no middle ground, so waiting 90 seconds on failure wastes time unnecessarily.",
        "details": "## Background\n\nAnalysis documented in `geelark_uiautomator2_timeout_report.txt` reveals a **binary behavior pattern** for UiAutomator2 initialization on Geelark cloud phones:\n\n- **Success case**: Instrumentation starts in ~1 second (observed: 1104ms)\n- **Failure case**: Times out after the full timeout period (previously 90s)\n- **No middle ground**: There are no cases where initialization takes 30s, 50s, or any intermediate time\n\nThis means the previous 90-second timeout was wasteful - if UiAutomator2 doesn't start within a few seconds, it won't start at all until retry.\n\n## Implementation\n\nIn `post_reel_smart.py`, update the `connect_appium()` method (around line 743):\n\n### Before (Task 13 implementation):\n```python\noptions.set_capability(\"appium:uiautomator2ServerLaunchTimeout\", 90000)  # 90s\n```\n\n### After:\n```python\noptions.set_capability(\"appium:uiautomator2ServerLaunchTimeout\", 10000)  # 10s for launch - binary: works in ~1s or not at all\n```\n\n## Rationale\n\n1. **Time savings**: Failed attempts now waste 10s instead of 90s (80s saved per failure)\n2. **Faster retry cycle**: With 15s delay between retries, a full 3-attempt cycle takes:\n   - Before: 90s + 15s + 90s + 15s + 90s = 300s (5 minutes)\n   - After: 10s + 15s + 10s + 15s + 10s = 60s (1 minute)\n3. **No false negatives**: 10s is still generous given the observed ~1s success time\n4. **Buffer for edge cases**: 10s provides 10x buffer over the ~1s typical success time\n\n## Other timeouts remain unchanged\n\nThe following timeouts in `connect_appium()` should NOT be reduced as they serve different purposes:\n\n- `newCommandTimeout: 120` - For slow cloud phone operations during the session\n- `adbExecTimeout: 120000` - For slow ADB commands over network tunnels\n- `uiautomator2ServerInstallTimeout: 120000` - First-time APK installation can be slow\n- `androidDeviceReadyTimeout: 60` - Device boot/ready detection\n\nOnly `uiautomator2ServerLaunchTimeout` exhibits the binary behavior pattern.",
        "testStrategy": "## Test Strategy\n\n### 1. Verify timeout value is correctly set\n\nRun a quick Appium session and check the capabilities:\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nposter = SmartInstagramPoster('test_phone')\n# Connect to a phone and check capabilities\nposter.connect()\ncaps = poster.appium_driver.capabilities\nprint(f'Launch timeout: {caps.get(\\\"uiautomator2ServerLaunchTimeout\\\", \\\"not set\\\")}')\nposter.cleanup()\n\"\n```\n\n### 2. Timing verification on success\n\nStart a cloud phone and time the Appium connection:\n```bash\ntime python -c \"\nfrom post_reel_smart import SmartInstagramPoster\nposter = SmartInstagramPoster('podclipcrafters')\nposter.connect()\nprint('Connected successfully')\nposter.cleanup()\n\"\n```\n\nExpected: Total connection time should be well under 60 seconds on success.\n\n### 3. Timing verification on failure\n\nSimulate a failure scenario by connecting to an invalid device:\n```bash\ntimeout 20 python -c \"\nfrom appium import webdriver\nfrom appium.options.android import UiAutomator2Options\noptions = UiAutomator2Options()\noptions.device_name = 'invalid:12345'\noptions.udid = 'invalid:12345'\noptions.set_capability('appium:uiautomator2ServerLaunchTimeout', 10000)\ndriver = webdriver.Remote('http://127.0.0.1:4723', options=options)\n\" 2>&1 | grep -i timeout\n```\n\nExpected: Should timeout within ~15 seconds (10s timeout + overhead), not 90+ seconds.\n\n### 4. Full retry cycle timing\n\nRun a posting operation to a phone that may have intermittent connectivity:\n```bash\ntime python posting_scheduler.py --add-accounts podclipcrafters --add-folder test_videos --run --max-accounts 1\n```\n\nMonitor `geelark_batch.log` for retry timing. Expected:\n- If first attempt fails, retry should start within 25-30 seconds (10s timeout + 15s delay)\n- Full 3-attempt cycle should complete in under 2 minutes even with all failures\n\n### 5. Regression test - no false negatives\n\nRun the scheduler on 5-10 phones overnight and compare:\n- Success rate should remain the same or improve (not decrease)\n- Average time per attempt should decrease significantly",
        "status": "done",
        "dependencies": [
          "13"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:04:02.231Z"
      },
      {
        "id": "18",
        "title": "Fix Appium Connection Failures with Device-Ready Checks and Thread Cleanup",
        "description": "Resolve Appium connection instability by ensuring device readiness before glogin execution, fixing ThreadPoolExecutor cleanup to prevent orphaned sessions, and managing stale ADB connections through robust connect() flow improvements.",
        "details": "Implement comprehensive fixes for the three identified Appium connection failure root causes following Appium best practices for ADB stability and resource management[1][2][6].\n\n## 1. Device Readiness Check Before glogin (Primary Fix)\n\n**Current Problem**: glogin executes before ADB reports device as 'device' status, causing connection failures[1].\n\n**Implementation**:\n```python\n# In connect_appium() or connect() flow (~lines 730+ from Task 13)\nimport subprocess\nimport time\n\nfrom typing import Optional\n\ndef wait_for_device_ready(udid: str, timeout: int = 60) -> bool:\n    \"\"\"Wait for device to report 'device' status in ADB\"\"\"\n    start_time = time.time()\n    while time.time() - start_time < timeout:\n        result = subprocess.run(\n            ['adb', '-s', udid, 'get-state'],\n            capture_output=True, text=True, timeout=5\n        )\n        if result.returncode == 0 and 'device' in result.stdout.strip().lower():\n            return True\n        time.sleep(2)\n    return False\n\ndef safe_glogin(udid: str) -> None:\n    \"\"\"Only run glogin after device is confirmed ready\"\"\"\n    if not wait_for_device_ready(udid):\n        raise UiAutomatorStartupError(f\"Device {udid} never reached 'device' state\")\n    # Run glogin subprocess here (existing logic)\n    subprocess.run(['glogin', udid], check=True)\n```\n\n**Integration**: Call `safe_glogin(self.device_udid)` **before** Appium driver initialization in `connect_appium()`.\n\n## 2. Fix/Remove ThreadPoolExecutor Wrapper\n\n**Current Problem**: ThreadPoolExecutor timeouts leave orphaned Appium sessions/threads[6].\n\n**Best Practice**: Use context managers for guaranteed cleanup. Remove ThreadPoolExecutor wrapper entirely[6].\n\n**Implementation**:\n```python\n# REPLACE ThreadPoolExecutor wrapper pattern with direct context-managed sessions\n\nclass AppiumSessionManager:\n    def __enter__(self):\n        self.driver = self.connect_appium(retries=3)\n        return self.driver\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Guaranteed cleanup - even on exceptions/timeouts\"\"\"\n        if hasattr(self, 'driver') and self.driver:\n            try:\n                self.driver.quit()\n                # Force ADB session cleanup\n                subprocess.run(['adb', 'kill-server'])\n                subprocess.run(['adb', 'start-server'])\n            except Exception as e:\n                logger.warning(f\"Cleanup failed: {e}\")\n\n# Usage in posting logic:\nwith AppiumSessionManager() as driver:\n    # All automation here\n    pass  # Auto-cleanup guaranteed\n```\n\n## 3. Stale ADB Connection Management\n\n**Implementation**:\n```python\ndef refresh_adb_connection(udid: Optional[str] = None) -> None:\n    \"\"\"Kill/restart ADB server to clear stale connections[1][2]\"\"\"\n    subprocess.run(['adb', 'kill-server'])\n    time.sleep(2)\n    subprocess.run(['adb', 'start-server'])\n    if udid:\n        # Wait for specific device\n        wait_for_device_ready(udid)\n\n# Call refresh_adb_connection() at start of connect_appium() and on UiAutomatorStartupError\n```\n\n## 4. Updated connect_appium() Flow\n```python\ndef connect_appium(self, retries=3):\n    for attempt in range(retries):\n        try:\n            refresh_adb_connection(self.device_udid)\n            safe_glogin(self.device_udid)\n            \n            # Existing Appium connection logic with 10s UiAutomator2 timeout (Task 17)\n            options = UiAutomator2Options()\n            options.set_capability('uiautomator2ServerLaunchTimeout', 10000)  # 10s\n            self.driver = u2.connect(options)\n            return self.driver\n        except (UiAutomatorStartupError, Exception) as e:\n            logger.warning(f\"Attempt {attempt+1} failed: {e}\")\n            if attempt == retries - 1:\n                raise\n            time.sleep(5)\n```\n\n**Dependencies**: Builds directly on Task 13 (connect_appium() structure, UiAutomatorStartupError), Task 17 (10s timeout), Task 16 (ADB env vars).[1][2]",
        "testStrategy": "**Comprehensive Test Strategy** (Critical for production stability)\n\n### 1. Device Readiness Tests\n```bash\n# Test 1: Simulate offline→online transition\nadb disconnect <udid>\nsleep 5\n# Start device connection\npython -m pytest test_appium_connect.py::test_wait_device_ready\n```\n- Verify `wait_for_device_ready()` polls correctly\n- Confirm `safe_glogin()` blocks until 'device' state\n- Test 60s timeout raises `UiAutomatorStartupError`\n\n### 2. ThreadPoolExecutor Replacement Tests\n- Create unit test simulating timeout during session\n```python\nfrom contextlib import contextmanager\n\n@contextmanager\ndef failing_appium():\n    yield\n    raise TimeoutError(\"Simulated timeout\")\n\n# Verify __exit__ still executes cleanup\n```\n- Confirm `driver.quit()` and `adb kill-server` called even on exceptions\n\n### 3. End-to-End Connection Tests\n```bash\n# Test full connect() flow 50x\nfor i in {1..50}; do\n    python test_appium_stability.py --device <udid> || echo \"FAIL $i\"\ndone\n```\n**Success Criteria**:\n- 100% success rate (0 connection failures)\n- No orphaned Appium processes (`ps aux | grep appium`)\n- No stale ADB connections (`adb devices -l` shows clean list)\n\n### 4. ADB Stale Connection Tests\n```bash\n# Force stale connections\nadb kill-server\nadb start-server\n# Run multiple parallel sessions\npytest test_adb_cleanup.py -n auto\n```\n- Verify `refresh_adb_connection()` restores clean state\n- Confirm no 'offline' devices after cleanup\n\n### 5. Integration with Existing Codebase\n- Run full scheduler loop (Task 15) for 2+ hours\n- Monitor `batch_results_*.csv` for zero Appium connection errors\n- Validate no lockfile/heartbeat issues (Task 15)\n\n**Tools**:\n- `lsof -i :5037` (ADB port conflicts)\n- `ps aux | grep -E 'appium|glogin'` (orphaned processes)\n- Appium logs with `--log-level debug`",
        "status": "done",
        "dependencies": [
          "13",
          "16",
          "17"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T03:15:41.147Z"
      },
      {
        "id": "19",
        "title": "Investigate and Fix Appium Multi-Instance Crashes in Parallel Posting Scheduler",
        "description": "Research, design, and implement a robust parallel Appium execution strategy so that running 2+ concurrent sessions for the posting scheduler does not trigger UiAutomator2 instrumentation startup timeouts or cross-instance interference.",
        "details": "## Goals\n- Understand why **2+ simultaneous Appium sessions** cause `instrumentation process cannot be initialized within 10000ms` / UiAutomator2 startup failures when the posting scheduler runs in parallel.\n- Implement a **multi-instance–safe Appium orchestration layer** (ports, ADB, capabilities, process isolation, and retry policy) for stable parallel phone automation.\n- Ensure the design is compatible with existing connection improvements from Tasks 13, 16, 17, and 18.\n\n## Research & Design\n1. **Gather context from current codebase**\n   - Review `post_reel_smart.py`, the scheduler worker code, and any Appium orchestration helpers to understand:\n     - How drivers are created (where `connect_appium` / `connect()` is called).\n     - How many workers can run concurrently and how phones are assigned.\n     - Whether **multiple sessions can target the same device** or if there is a 1:1 mapping.\n     - Current handling of **ports, systemPort, wdaLocalPort (if iOS in future), and Appium server endpoints**.\n   - Document the current assumptions about **single-instance vs multi-instance** operation.\n\n2. **Research Appium parallel best practices (Android focus)**\n   - Summarize current recommendations for **parallel Android execution**:\n     - Use **one Appium server instance per device** or, if sharing a server, ensure unique `systemPort` / bootstrap ports to avoid conflicts.[2][5][7]\n     - Avoid having **two sessions on the same device at the same time**; enforce a per-device session lock.[3][7]\n     - Ensure **separate tmp/log directories** per server instance to avoid file-lock races.[1][5]\n     - Use delays or staggered startup when opening multiple sessions to avoid concurrent heavy ADB load and connection resets.[2][6][8]\n   - Check guidance around UiAutomator2 startup issues and instrumentation conflicts when multiple devices are attached and multiple sessions start simultaneously.[2][6]\n   - Record concrete recommendations (ports, capabilities, server process layout) to feed into implementation.\n\n3. **Identify likely root causes for the 10s instrumentation timeout in parallel**\n   - Hypothesize (and later validate) major categories:\n     - **Port and resource conflicts**:\n       - Multiple sessions using the same `systemPort` or appium internal ports.[2][5][7]\n       - Shared Appium server instance not isolating device traffic properly under parallel load.[3][6]\n     - **ADB contention**:\n       - Multiple heavy `adb shell` / `uiautomator` launches racing at startup causing delays beyond 10s limit.[2][6]\n     - **Process lifecycle and cleanup**:\n       - Orphaned UiAutomator2 / instrumentation processes from prior sessions after crashes (even after Task 18) causing subsequent startups to hang.\n     - **Scheduler-level concurrency**:\n       - All workers attempting `connect()` at exactly the same time on cold devices, amplifying the timing window for instrumentation startup.\n   - Plan to validate each via logs and targeted experiments.\n\n## Implementation Plan\n\n### 1. Introduce a Parallel-Oriented Session Orchestrator\n- Create a module/class, e.g. `parallel_appium_manager.py`, responsible for **safe multi-instance coordination**:\n  - Maintain a **registry of phones** with:\n    - Device identifier/UDID.\n    - Appium server endpoint (host, port).\n    - Unique `systemPort` (and any other per-device ports) if sharing a server.\n    - Current session status (idle, connecting, busy, error).\n  - Provide methods:\n    - `acquire_device(for_job_id)` – atomically reserves a device for a worker.\n    - `release_device(device_id, status)` – releases and optionally marks health.\n    - `connect_driver(device_id, retries=...)` – wrapper around existing `connect()` that adds **parallel-aware policies**:\n      - Enforce **one active session per device**.\n      - Apply a **startup jitter** (small randomized sleep) when multiple workers start concurrently.\n      - Limit the number of **simultaneous fresh instrumentation startups** (e.g., at most N devices doing initial UiAutomator2 startup at once).\n\n### 2. Port & Server Layout Hardening\n- Decide and implement one of these strategies (config-driven so it can be changed later):\n  1. **One Appium server per device** (recommended for simplicity and isolation):[2][5][7]\n     - Ensure each server is started with:\n       - Unique `--port`.\n       - Unique `--tmp` directory per device.[1]\n     - Keep or add a tiny **Appium server manager** that:\n       - Starts/stops per-device servers.\n       - Ensures environment (`ANDROID_HOME` from Task 16) is correctly set.\n  2. **Single Appium server, multiple devices**:\n     - Guarantee **unique `systemPort` per device** in desired capabilities to avoid port collision.[2][5][7]\n     - Persist this mapping somewhere (config file or code constant) and validate on startup.\n- In both cases:\n  - Make the **per-device port configuration explicit** (no implicit defaults).\n  - Fail fast on startup if two devices would share the same `systemPort` or Appium port.\n\n### 3. Strengthen UiAutomator2 Startup & Timeout Handling for Parallelism\n- Build on Task 13 and Task 17:\n  - Keep the **10s UiAutomator2 timeout** but add **parallel-aware retry logic**:\n    - On `instrumentation process cannot be initialized within 10000ms`:\n      - Immediately perform a targeted **cleanup routine** on that device:\n        - Kill stray `uiautomator` / `io.appium.uiautomator2.server` processes (via `adb shell ps` + `kill` or `am force-stop`).\n        - Clear any stuck instrumentation states.\n      - Backoff with jitter (e.g., 2–5 seconds) before retrying to reduce synchronized retries.\n    - Limit number of retries and, after failure, mark device as **temporarily unhealthy** for the scheduler.\n  - Ensure any instrumentation failure raises the existing `UiAutomatorStartupError`, but now tagged with **device id and parallel context** (e.g., number of concurrent connections at that time) for observability.\n\n### 4. Device-Ready & ADB Stability in Parallel (Builds on Task 18)\n- Reuse and extend the **device-ready checks** and **ADB connect() flow improvements** from Task 18 for multi-instance:\n  - Before starting UiAutomator2 on a device, assert:\n    - Device is in `device` state.\n    - `adb -s <udid> shell echo ping` returns quickly (sanity check).\n  - If multiple devices are going from `offline` to `device` simultaneously:\n    - Randomize or sequence **Appium session creation** so not all devices attempt heavy instrumentation launch at the same instant.\n- Add logging/metrics per device:\n  - Time-to-ready: from scheduler worker start → device-ready → instrumentation ready.\n  - Count and rate of instrumentation timeouts under parallel load.\n\n### 5. Scheduler Integration & Concurrency Controls\n- Integrate the new orchestrator with the **posting_scheduler worker loop** (Task 15):\n  - Ensure that **each worker acquires a device lock** from `parallel_appium_manager` before calling `SmartInstagramPoster.connect()`.\n  - Guarantee no worker is allowed to open a second session on a device that is already in use.\n  - If the device pool is exhausted, define behavior:\n    - Either block until device frees up, or\n    - Immediately requeue / reschedule the job.\n- Add **configurable limits**:\n  - `MAX_PARALLEL_DEVICES` – how many devices the scheduler tries to use at the same time.\n  - `MAX_SIMULTANEOUS_STARTUPS` – cap number of devices that can be in `connecting` state concurrently.\n\n### 6. Observability & Diagnostics\n- Enhance logs to make parallel issues diagnosable:\n  - Include **device id, appium port, systemPort, worker id, and session id** in all connection-related logs.\n  - Log explicit messages on:\n    - Session acquisition/release.\n    - Instrumentation startup, timeout, and retries.\n    - ADB cleanup operations.\n- Optionally add a lightweight **debug script**:\n  - `debug/run_parallel_connect_test.py` that spins up N parallel connection attempts (without running full posting logic) to reproduce and stress-test parallel startup behavior.\n\n## Non-Goals / Constraints\n- Do not attempt to support **multiple sessions on the same physical device** concurrently; enforce one active session per device as a hard invariant, in line with Appium/WDA expectations.[3][7]\n- Avoid introducing global sleeps; prefer **bounded jitter + limits** to maintain throughput.\n",
        "testStrategy": "### 1. Unit & Integration Tests for Orchestrator\n1. Write unit tests for `parallel_appium_manager`:\n   - **Device lock behavior**:\n     - Simulate multiple threads calling `acquire_device()`; assert only one obtains the same device at a time.\n   - **Port configuration validation**:\n     - Provide a configuration with duplicate `systemPort`; assert initialization fails fast with clear error.\n   - **Health marking**:\n     - Simulate a device failing to connect repeatedly; verify it gets marked unhealthy and is not handed out to new workers.\n\n2. Add integration tests (can be slow / optional in CI):\n   - Use a small number of mock or stubbed devices to simulate the scheduler requesting devices in parallel and verify that no concurrent session is created for the same device.\n\n### 2. Parallel Connection Load Tests (On Real or Cloud Devices)\n1. **Two-device baseline test**:\n   - Attach 2 Android devices (or cloud device slots).\n   - Start 2 scheduler workers, each configured to use a different device via the orchestrator.\n   - Run at least 20–30 short posting jobs.\n   - Verify:\n     - No `instrumentation process cannot be initialized within 10000ms` errors.\n     - No cross-device interference (e.g., logs for device A show only device A’s UDID and ports).\n\n2. **Scaled parallel test (3–4 devices, if available)**:\n   - Configure 3–4 devices with unique Appium ports and `systemPort` values.\n   - Run the debug script `run_parallel_connect_test.py` to:\n     - Start N parallel connection attempts.\n     - Repeat several times.\n   - Collect metrics:\n     - Connection success rate.\n     - Average and max time from connect start to session ready.\n     - Count of retries and any instrumentation timeouts.\n\n3. **Cold-start storm test**:\n   - Power-cycle or adb-reboot all devices.\n   - Immediately start the scheduler with multiple workers so all try to run at once.\n   - Confirm that:\n     - Startup jitter and `MAX_SIMULTANEOUS_STARTUPS` prevent mass failures.\n     - Timeouts, if any, are contained and recovered via retry logic.\n\n### 3. Instrumentation Timeout Handling Tests\n1. **Forced timeout simulation** (where possible):\n   - Temporarily configure an unrealistically low UiAutomator2 timeout to force failures, or inject a stub around instrumentation start to simulate a timeout.\n   - Verify that:\n     - Cleanup routine runs (killing stray processes),\n     - Retry is attempted with backoff,\n     - After configured retries, device is marked unhealthy and surfaced to logs.\n\n2. **Regression tests vs single-instance behavior**:\n   - Run the scheduler in **single-device mode** (only one device configured) and ensure:\n     - No regression in connection reliability vs results from Tasks 13, 17, and 18.\n\n### 4. Scheduler-Level End-to-End Tests\n1. **End-to-end posting test, 2+ workers**:\n   - Configure the scheduler with 2–3 workers and 2–3 devices.\n   - Run a realistic batch of posting jobs.\n   - Verify:\n     - All jobs complete or fail only for non-infrastructure reasons (e.g., account problems),\n     - No long hangs at the first post,\n     - Logs show orderly acquire→connect→post→release flow per device.\n\n2. **Failure-injection tests**:\n   - Manually disconnect a device mid-run (e.g., `adb disconnect` or physically unplug, if safe) and observe:\n     - The affected session fails fast with clear logs,\n     - Other devices continue working without instrumentation crashes,\n     - The device is not re-used until it is healthy again.\n\n3. **Metrics/log review**:\n   - After each test run, inspect logs to ensure:\n     - Per-device port mapping is consistent.\n     - No overlapping sessions per device.\n     - Instrumentation timeout count is zero or within expected bounds and always followed by proper retry/cleanup.\n",
        "status": "done",
        "dependencies": [
          "13",
          "16",
          "17",
          "18"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T06:05:31.230Z"
      },
      {
        "id": "20",
        "title": "Fix state persistence bug in parallel scheduler SUCCESS handling",
        "description": "Ensure that when parallel jobs complete with SUCCESS in the posting scheduler, their final status and metadata are reliably persisted to scheduler_state_parallel.json in all execution paths, not just logged.",
        "details": "## Objectives\n- Identify and fix the mismatch between **logged SUCCESS events** and the **missing/incorrect job status** in `scheduler_state_parallel.json` for the parallel scheduler.\n- Ensure that the **single source of truth** for job state is updated atomically and consistently from all worker paths, including parallel/async execution.\n- Harden the persistence layer against race conditions, partial writes, and crashes, following current best practices for concurrent state management.\n\n## Analysis & Design\n1. **Locate current persistence flow**\n   - Find where `execute_job` is implemented in the parallel scheduler and how it currently:\n     - Logs job start and SUCCESS/FAILURE.\n     - Updates in-memory job state.\n     - Persists the state to `scheduler_state_parallel.json` (or delegates to a separate component).\n   - Map the exact call chain for a typical job:\n     - Job fetch → `execute_job(job)` → status log → state update → file write.\n\n2. **Identify failure mode**\n   - Using logs and current code, determine which of these is happening:\n     - **SUCCESS is logged before state update**, and a later error prevents the state from being written.\n     - **State updated in memory but not flushed** to disk due to missing/incorrect persistence calls in parallel workers.\n     - **Race condition** between multiple workers writing to `scheduler_state_parallel.json` (last writer wins, clobbered writes, or JSON corruption).\n     - **Per-process vs shared state issue**: workers run in separate processes/threads with their own in-memory state, and only some process owns the state file.\n\n3. **Refine persistence model for parallel execution**\n   - Adopt a **clear ownership model** for the state file:\n     - Prefer a **single writer/owner** (e.g., main scheduler process) that applies state updates received from workers, rather than every worker writing the JSON file directly.\n   - Define **job state transitions** explicitly (e.g., PENDING → RUNNING → SUCCESS/FAILED/SKIPPED) and ensure only valid transitions are persisted.\n   - Decide on **state update granularity**:\n     - Full snapshot writes vs. incremental updates.\n     - For simplicity and robustness, full-snapshot JSON writes with atomic replace are acceptable if file size is moderate.\n\n4. **Implement a robust state update API**\n   - Introduce (or standardize) a small internal API, e.g.:\n     ```python\n     from enum import Enum\n     from dataclasses import dataclass, asdict\n     from typing import Optional, Dict, Any\n\n     class JobStatus(str, Enum):\n         PENDING = \"PENDING\"\n         RUNNING = \"RUNNING\"\n         SUCCESS = \"SUCCESS\"\n         FAILED = \"FAILED\"\n         SKIPPED = \"SKIPPED\"\n\n     @dataclass\n     class JobState:\n         id: str\n         status: JobStatus\n         started_at: Optional[str] = None\n         finished_at: Optional[str] = None\n         error: Optional[str] = None\n         attempt: int = 0\n         metadata: Dict[str, Any] = None\n\n     class ParallelSchedulerStateStore:\n         def __init__(self, path: str, lock=None):\n             self._path = path\n             self._lock = lock  # e.g., threading.Lock() in single process, or file lock\n             self._state: Dict[str, JobState] = {}\n\n         def load(self) -> None:\n             # read JSON atomically, populate self._state\n             ...\n\n         def get(self, job_id: str) -> Optional[JobState]:\n             return self._state.get(job_id)\n\n         def update_job(self, job: JobState) -> None:\n             self._state[job.id] = job\n             self._persist()\n\n         def mark_success(self, job_id: str, finished_at: str, metadata: Dict[str, Any] | None = None):\n             job = self._state.get(job_id) or JobState(id=job_id, status=JobStatus.PENDING)\n             job.status = JobStatus.SUCCESS\n             job.finished_at = finished_at\n             if metadata:\n                 job.metadata = {**(job.metadata or {}), **metadata}\n             self.update_job(job)\n\n         def _persist(self) -> None:\n             # write JSON atomically (temp file + rename) under lock\n             ...\n     ```\n   - Ensure **execute_job never touches the JSON file directly**; it instead calls methods on the shared `ParallelSchedulerStateStore` or sends state-update events to a central owner.\n\n5. **Concurrency & atomicity best practices**\n   - If the parallel scheduler uses **threads in a single process**:\n     - Use a **`threading.Lock`** or `RLock` around any in-memory state mutation and file write.\n     - Implement atomic writes:\n       - Serialize the full state to a temp file in the same directory.\n       - `os.replace(temp_path, final_path)` to ensure readers always see a complete JSON file.\n   - If it uses **multiprocessing or multi-process workers**:\n     - Use one of these patterns:\n       - A **single writer process** that holds the `ParallelSchedulerStateStore`; worker processes send state-change messages via a `multiprocessing.Queue` or pipe.\n       - Or use a **file lock** (e.g., `fcntl` on Unix or `portalocker`/`filelock` library) around load–modify–write cycles to `scheduler_state_parallel.json`.\n     - Avoid any pattern where multiple processes write JSON concurrently without a lock.\n\n6. **Align logging and persistence**\n   - Enforce a strict sequence in `execute_job` (simplified):\n     ```python\n     def execute_job(job, state_store: ParallelSchedulerStateStore, logger):\n         job_id = job[\"id\"]\n         start = datetime.utcnow().isoformat()\n\n         # Optional: persist RUNNING state\n         state_store.update_job(JobState(id=job_id, status=JobStatus.RUNNING, started_at=start))\n\n         try:\n             result = run_job(job)\n             finished = datetime.utcnow().isoformat()\n             logger.info(\"Job %s SUCCESS: %s\", job_id, result)\n\n             state_store.mark_success(job_id, finished_at=finished, metadata={\"result\": result})\n\n         except Exception as exc:\n             finished = datetime.utcnow().isoformat()\n             logger.exception(\"Job %s FAILED\", job_id)\n\n             state_store.update_job(JobState(\n                 id=job_id,\n                 status=JobStatus.FAILED,\n                 started_at=start,\n                 finished_at=finished,\n                 error=str(exc),\n             ))\n     ```\n   - Ensure that **any code path that logs SUCCESS also calls a persistence method**; consider adding an assertion or helper like `log_and_persist_success(job, result)` to reduce drift.\n\n7. **Backward compatibility and migration**\n   - If the new `JobState` schema differs from existing JSON, add a **migration/upgrade step** when loading old state:\n     - Tolerate missing fields and default them.\n     - Consider writing a one-off script to normalize existing `scheduler_state_parallel.json` if needed.\n\n8. **Observability & diagnostics**\n   - Add targeted log lines around persistence operations:\n     - Log when a job state is written: job_id, old_status → new_status.\n     - Log any JSON or IO errors with clear messages.\n   - Optionally add a **debug flag** to dump a small summary of state in memory after each batch of updates.\n\n9. **Documentation**\n   - Document in the scheduler README or inline module docstring:\n     - The **state ownership model** (who writes `scheduler_state_parallel.json`).\n     - Thread/process safety guarantees and any expectations for worker processes.\n     - The contract for `execute_job` (i.e., which layer is responsible for persistence).\n",
        "testStrategy": "1. **Unit tests for state store**\n   - Create tests for `ParallelSchedulerStateStore` (or equivalent):\n     - Load from an empty/missing `scheduler_state_parallel.json` and verify it initializes cleanly.\n     - Load from an existing JSON with legacy structure and verify migration/defaulting works.\n     - Call `mark_success` and verify in-memory state and on-disk JSON both show `status: SUCCESS`, correct timestamps, and any metadata.\n     - Verify `_persist` performs atomic writes: simulate a crash between temp-file write and replace using mocks and assert that the final JSON is always valid or falls back to the previous file.\n\n2. **Concurrency tests**\n   - If using threads:\n     - Spawn multiple threads that concurrently call `state_store.mark_success` with different job IDs; assert no exceptions, and final JSON contains all jobs with correct statuses.\n     - If you support updating the same job from concurrent threads (e.g., retries), create a test that races updates and ensure resulting state is logically consistent according to your rules (e.g., last update wins, or terminal states are not overwritten).\n   - If using multiprocessing:\n     - Write an integration test that starts multiple worker processes, each simulating `execute_job` and reporting completion to the central state owner; assert that after all processes exit, `scheduler_state_parallel.json` contains all expected SUCCESS/FAILED jobs.\n\n3. **execute_job integration tests**\n   - Write tests that run `execute_job` end-to-end with a fake `run_job`:\n     - **Success path**: ensure that when `run_job` returns normally and a SUCCESS log entry is emitted, the corresponding job entry in `scheduler_state_parallel.json` is updated to `SUCCESS` with `finished_at` set.\n     - **Failure path**: ensure that when `run_job` raises, the job is marked `FAILED` with `error` populated.\n     - Verify the ordering: log entries and state changes both exist; there should be no SUCCESS log without a corresponding SUCCESS state.\n\n4. **Regression test for original bug**\n   - Build a test that mirrors the exact sequence that previously exhibited the bug:\n     - Run the parallel scheduler against a small job set in a test environment.\n     - Wait until logs show multiple `SUCCESS` lines from `execute_job`.\n     - Read `scheduler_state_parallel.json` and assert that all jobs with logged SUCCESS are present with `status == SUCCESS`.\n   - Add a check that repeatedly queries the state file during execution to ensure there are no transient periods where a job is permanently logged as SUCCESS but never appears in the final state.\n\n5. **Durability and crash-safety tests**\n   - Simulate a crash between job completion and scheduler shutdown:\n     - Run a batch of jobs, forcefully terminate the scheduler process mid-run (e.g., SIGKILL in a controlled test) after some jobs log SUCCESS.\n     - Restart the scheduler and load `scheduler_state_parallel.json`; verify that all SUCCESS jobs that had their state persisted before the crash are still marked SUCCESS, and there is no JSON corruption.\n\n6. **Manual validation in staging**\n   - Run the parallel scheduler with a realistic workload in a staging or test environment.\n   - After completion:\n     - Export job results to CSV or query logs to list all jobs with `SUCCESS` log entries.\n     - Compare this list to the contents of `scheduler_state_parallel.json` and confirm 1:1 alignment for job IDs and statuses.\n   - Repeat with multiple concurrent runs to ensure behavior is stable under load.",
        "status": "done",
        "dependencies": [
          "15",
          "19"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T07:48:58.966Z"
      },
      {
        "id": "21",
        "title": "Implement Automatic Stale Job Cleanup in Parallel Scheduler",
        "description": "Add heartbeat tracking, periodic stale job detection, and automatic status reset for jobs stuck in 'in_progress' due to worker crashes or timeouts, including cleanup on scheduler startup.",
        "details": "Implement stale job cleanup following distributed systems best practices (e.g., Kubernetes TTL-after-finished controller patterns adapted for custom scheduler): 1) Add `last_heartbeat` timestamp and `worker_id` fields to the Job model/dataclass. 2) In worker job acquisition/execution: set `status='in_progress'`, `worker_id=process_id`, update `last_heartbeat` on job start and every 30s during execution using a background heartbeat thread per worker. 3) Create configurable staleness threshold (default 10min) in Config: `stale_job_timeout_minutes=10`. 4) Implement `cleanup_stale_jobs()` function: query jobs where `status='in_progress' AND last_heartbeat < NOW() - staleness_threshold`, then atomically reset to `status='retrying'`, clear `worker_id`, log the event with `logging.warning(f\"Reset stale job {job.id} (last heartbeat: {job.last_heartbeat})\")`. Use database transactions or optimistic locking to prevent races. 5) Startup cleanup: in scheduler init (e.g., main.py or scheduler module), call `cleanup_stale_jobs()` synchronously before accepting new jobs. 6) Runtime cleanup: use `APScheduler` or `schedule` library for periodic execution every 2 minutes via `@scheduler.scheduled_job('interval', minutes=2)` decorator on `cleanup_stale_jobs`. 7) For parallel safety: if using multiprocessing/threading workers, ensure heartbeat updates use thread-safe DB access (e.g., SQLAlchemy session per thread). 8) Configurable actions: add `stale_job_action` enum ('retrying', 'pending', 'failed') with 'retrying' default. 9) Metrics: log number of cleaned jobs per run and expose via structured logging for monitoring. 10) Graceful handling: exclude jobs with recent heartbeats; consider worker self-registration with liveness checks.",
        "testStrategy": "1) Unit tests: mock DB/job model, verify `cleanup_stale_jobs()` correctly identifies and resets jobs older than threshold but ignores recent ones; test edge cases like concurrent updates using threading. 2) Integration tests: spin up test scheduler with mock workers, simulate worker crash by killing process mid-job, confirm periodic cleanup resets job after timeout and job becomes eligible for retry. 3) Startup test: restart scheduler, verify immediate cleanup of artificially stale 'in_progress' jobs from previous run. 4) Load test: enqueue 100 jobs, crash 20 workers randomly, run cleanup loop 5x, assert all stale jobs reset within expected time, no false positives on active jobs. 5) Manual verification: monitor logs during real runs to confirm heartbeat updates every 30s and cleanup events logged correctly. 6) Config test: vary `stale_job_timeout_minutes` and verify threshold respected.",
        "status": "done",
        "dependencies": [
          "9",
          "10"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T06:34:22.430Z"
      },
      {
        "id": "22",
        "title": "Fix ADB Wait Loop in connect() to Prevent Appium Device Not Found Errors",
        "description": "After enabling ADB in post_reel_smart.py connect(), implement an extended wait loop (60s max, 3s polling) that monitors 'adb devices' until the device appears with 'device' status. If the device never appears, raise an exception immediately instead of proceeding to Appium connection, preventing 'Device was not in the list of' errors.",
        "details": "## Current Problem (post_reel_smart.py:723-744)\n\nThe current ADB wait loop has two critical issues:\n1. **Insufficient wait time**: Only 20 seconds max (10 attempts × 2 seconds each)\n2. **Soft failure mode**: If device doesn't appear, only prints a warning and continues to Appium connection, which then fails with \"Device was not in the list of\" errors\n\n## Implementation Changes\n\n### 1. Update Wait Loop Parameters (lines 727-744)\n\nReplace the current wait loop with extended configuration:\n\n```python\n# Wait for device to appear in ADB devices list BEFORE running glogin\nprint(\"Waiting for ADB connection to stabilize...\")\ndevice_ready = False\nMAX_ADB_WAIT_SECONDS = 60  # Extended from 20s\nPOLL_INTERVAL_SECONDS = 3   # Increased from 2s for stability\nmax_attempts = MAX_ADB_WAIT_SECONDS // POLL_INTERVAL_SECONDS  # 20 attempts\n\nfor attempt in range(max_attempts):\n    time.sleep(POLL_INTERVAL_SECONDS)\n    result = subprocess.run([ADB_PATH, \"devices\"], capture_output=True, encoding='utf-8')\n    \n    if self.device in result.stdout:\n        # Check if device status is \"device\" (not \"offline\" or \"unauthorized\")\n        lines = result.stdout.strip().split('\\n')\n        for line in lines:\n            if self.device in line:\n                if '\\tdevice' in line:\n                    device_ready = True\n                    print(f\"  Device {self.device} is ready (took {(attempt + 1) * POLL_INTERVAL_SECONDS}s)\")\n                    break\n                elif '\\toffline' in line:\n                    print(f\"  Device {self.device} is offline (attempt {attempt + 1}/{max_attempts})\")\n                elif '\\tunauthorized' in line:\n                    print(f\"  Device {self.device} unauthorized (attempt {attempt + 1}/{max_attempts})\")\n                else:\n                    print(f\"  Device {self.device} status: {line.split('\\t')[-1]} (attempt {attempt + 1}/{max_attempts})\")\n        if device_ready:\n            break\n    else:\n        print(f\"  Device not in ADB list yet (attempt {attempt + 1}/{max_attempts})\")\n\n# CRITICAL: Fail fast if device never appeared - don't attempt Appium connection\nif not device_ready:\n    raise Exception(\n        f\"ADB device {self.device} did not become ready within {MAX_ADB_WAIT_SECONDS}s. \"\n        f\"Last 'adb devices' output: {result.stdout.strip()}\"\n    )\n```\n\n### 2. Key Changes from Current Implementation\n\n| Aspect | Current | New |\n|--------|---------|-----|\n| Max wait time | 20s | 60s |\n| Poll interval | 2s | 3s |\n| Failure mode | Warning + continue | Raise exception |\n| Status logging | Basic | Detailed (offline/unauthorized) |\n| Error message | N/A | Includes ADB output for debugging |\n\n### 3. Benefits\n\n1. **Prevents Appium errors**: By raising an exception before Appium connection, we avoid the cryptic \"Device was not in the list of\" Appium errors\n2. **Better debugging**: The exception includes the actual `adb devices` output, making it easier to diagnose why the device didn't appear\n3. **Faster failure detection**: Instead of waiting for Appium to timeout (which can take 60-120s), we fail immediately when we know ADB won't work\n4. **Status visibility**: Shows whether device is offline/unauthorized/unknown, not just missing\n\n### 4. Integration with Task 18\n\nThis fix complements the device-ready checks implemented in Task 18 by:\n- Moving the wait-for-device logic earlier in the flow (before glogin)\n- Making it a hard requirement rather than a soft check\n- Providing better error context for debugging",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Tests (Manual Verification)\n\n**Test 1: Device appears quickly (happy path)**\n```bash\n# Start a Geelark phone, then run:\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nposter = SmartInstagramPoster('test_phone')\nposter.connect()  # Should succeed within first few attempts\nprint('SUCCESS: Device connected')\n\"\n```\n- Expected: Device ready message shows within 10-15s\n- Verify: `Device X is ready (took Ns)` message appears\n\n**Test 2: Device takes time to appear (extended wait)**\n```bash\n# Start a phone and immediately try connecting (before ADB is ready)\n# This tests that the 60s timeout is sufficient\npython post_reel_smart.py <slow_booting_phone> video.mp4 \"test\"\n```\n- Expected: Should wait patiently up to 60s\n- Verify: Multiple \"Device not in ADB list yet\" messages, then success\n\n**Test 3: Device never appears (failure path - CRITICAL)**\n```bash\n# Test with a phone that's not started or invalid phone name\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nposter = SmartInstagramPoster('nonexistent_phone_12345')\ntry:\n    poster.connect()\n    print('FAIL: Should have raised exception')\nexcept Exception as e:\n    if 'ADB device' in str(e) and 'did not become ready' in str(e):\n        print('SUCCESS: Got expected exception')\n        print(f'Error message: {e}')\n    else:\n        print(f'FAIL: Wrong exception: {e}')\n\"\n```\n- Expected: Exception raised with descriptive message\n- Verify: Message includes device address and ADB output\n\n**Test 4: Device offline status**\n```bash\n# Simulate offline device by disconnecting ADB during wait\n# In terminal 1: python post_reel_smart.py <phone> video.mp4 \"test\"\n# In terminal 2 (during wait): adb disconnect <device_ip:port>\n```\n- Expected: Shows \"Device X is offline\" messages\n- Verify: Eventually times out or recovers if device comes back online\n\n### 2. Integration Tests (With Parallel Scheduler)\n\n**Test 5: Parallel execution stability**\n```bash\npython posting_scheduler_parallel.py --accounts-file accounts_list.txt --add-folder chunk_01c --workers 3 --run --dry-run\n```\n- Run with 3 workers to verify no ADB connection race conditions\n- Verify: Each worker waits for its device independently\n\n**Test 6: Batch failure handling**\n```bash\n# Include a mix of valid and invalid phones in accounts file\n# Run scheduler and verify it:\n# 1. Properly fails fast on invalid phones\n# 2. Continues processing other phones\n# 3. Error logs show \"ADB device did not become ready\" for failures\n```\n\n### 3. Error Message Verification\n\nAfter implementation, verify error messages contain:\n- Device IP:port address\n- Wait timeout duration (60s)\n- Actual `adb devices` output at time of failure\n- Clear indication that this is an ADB-level failure (not Appium)",
        "status": "done",
        "dependencies": [
          "18",
          "4"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-12T07:48:52.948Z"
      },
      {
        "id": "23",
        "title": "Add Scheduler Startup Guardrails for Clean Parallel Execution",
        "description": "Implement guardrails in posting_scheduler_parallel.py to ensure clean scheduler starts: log 'Using num_workers=X from CLI' at startup, strip num_workers from save_state settings to prevent stale values in scheduler_state_parallel.json, and enhance PID lock checking to properly prevent multiple instances with clearer error messages.",
        "details": "## Implementation Details\n\n### 1. Log 'Using num_workers=X from CLI' at Startup (posting_scheduler_parallel.py)\n\n**Current behavior (line 787):**\n```python\nlogger.log(f\"Starting parallel scheduler with {self.num_workers} workers\")\n```\n\n**Required change in `main()` function after line 912:**\nAdd explicit logging immediately after scheduler instantiation to clarify the source:\n```python\nscheduler = ParallelScheduler(num_workers=args.workers)\nlogger.log(f\"Using num_workers={args.workers} from CLI\")  # Add this line\n```\n\nThis makes it clear that:\n- The value comes from CLI `--workers` argument (not from state file)\n- Debugging is easier when num_workers doesn't match expectations\n\n### 2. Strip num_workers from save_state Settings (posting_scheduler_parallel.py)\n\n**Current issue (lines 409-413):**\n```python\n'settings': {\n    'posts_per_account_per_day': self.posts_per_account_per_day,\n    'delay_between_posts': self.delay_between_posts,\n    'humanize': self.humanize,\n    'num_workers': self.num_workers,  # REMOVE THIS LINE\n},\n```\n\nThe state file currently contains `\"num_workers\": 1` or `\"num_workers\": 3` which can be confusing and misleading since:\n- It's explicitly NOT loaded (per comments on lines 391-397)\n- It represents a stale value from a previous run\n- It may lead to confusion when debugging (seeing `num_workers: 1` in state but running with `--workers 3`)\n\n**Required change:**\nRemove `'num_workers': self.num_workers,` from the `save_state()` method's settings dict entirely.\n\nAfter change:\n```python\n'settings': {\n    'posts_per_account_per_day': self.posts_per_account_per_day,\n    'delay_between_posts': self.delay_between_posts,\n    'humanize': self.humanize,\n    # num_workers intentionally NOT saved - must come from CLI\n},\n```\n\n### 3. Enhance PID Lock Check to Properly Prevent Multiple Instances (posting_scheduler_parallel.py)\n\n**Current implementation (lines 113-152):**\nThe `acquire_lock()` function has good logic but needs enhancement:\n\na) **Add same-PID check** - If the current process already holds the lock, it shouldn't fail:\n```python\ndef acquire_lock() -> bool:\n    current_pid = os.getpid()\n\n    # Check if MAIN scheduler is running (scheduler.lock)\n    if os.path.exists(\"scheduler.lock\"):\n        try:\n            with open(\"scheduler.lock\", 'r') as f:\n                main_lock = json.load(f)\n            main_pid = main_lock.get('pid')\n            if main_pid and is_process_running(main_pid):\n                print(f\"[LOCK ERROR] Main scheduler is running (PID {main_pid})\")\n                print(f\"[LOCK ERROR] Stop it first: taskkill //F //PID {main_pid}\")\n                return False\n        except Exception as e:\n            logger.log(f\"[LOCK] Warning: Could not read scheduler.lock: {e}\")\n\n    # Check if another parallel scheduler is running\n    if os.path.exists(LOCK_FILE):\n        try:\n            with open(LOCK_FILE, 'r') as f:\n                lock_data = json.load(f)\n            old_pid = lock_data.get('pid')\n            \n            # NEW: Same PID means we already have the lock (re-entrant case)\n            if old_pid == current_pid:\n                print(f\"[LOCK] Already held by this process (PID {current_pid})\")\n                return True\n\n            if old_pid and is_process_running(old_pid):\n                print(f\"[LOCK ERROR] Another parallel scheduler is running (PID {old_pid})\")\n                print(f\"[LOCK ERROR] Stop it first: taskkill //F //PID {old_pid}\")  # Add helpful message\n                return False\n            else:\n                print(f\"[LOCK] Stale lock found (PID {old_pid} not running), taking over\")\n        except json.JSONDecodeError as e:\n            print(f\"[LOCK] Corrupted lock file, removing: {e}\")\n            os.remove(LOCK_FILE)\n        except Exception as e:\n            print(f\"[LOCK] Warning: Error reading lock file: {e}\")\n\n    # Write new lock\n    lock_data = {\n        'pid': current_pid,\n        'started': datetime.now().isoformat(),\n        'workers': None,  # Will be updated after scheduler starts\n    }\n    try:\n        with open(LOCK_FILE, 'w') as f:\n            json.dump(lock_data, f, indent=2)\n        print(f\"[LOCK] Acquired (PID {current_pid})\")\n        return True\n    except Exception as e:\n        print(f\"[LOCK ERROR] Could not write lock file: {e}\")\n        return False\n```\n\nb) **Update lock file with worker count after scheduler starts** - Add helper to update lock file:\n```python\ndef update_lock_workers(num_workers: int):\n    \"\"\"Update lock file with worker count for debugging\"\"\"\n    current_pid = os.getpid()\n    if os.path.exists(LOCK_FILE):\n        try:\n            with open(LOCK_FILE, 'r') as f:\n                lock_data = json.load(f)\n            if lock_data.get('pid') == current_pid:\n                lock_data['workers'] = num_workers\n                with open(LOCK_FILE, 'w') as f:\n                    json.dump(lock_data, f, indent=2)\n        except:\n            pass\n```\n\nCall this in `main()` after `scheduler.start()`:\n```python\nscheduler.start()\nupdate_lock_workers(args.workers)\n```\n\n### 4. Files to Modify\n\n| File | Changes |\n|------|---------|\n| `posting_scheduler_parallel.py` | Lines 113-152 (acquire_lock), lines 409-413 (save_state), after line 912 (main) |\n\n### 5. Summary of Changes\n\n1. **Startup logging**: Add `logger.log(f\"Using num_workers={args.workers} from CLI\")` in main()\n2. **save_state()**: Remove `'num_workers': self.num_workers,` from settings dict\n3. **acquire_lock()**: Add same-PID re-entrant check, improve error messages with taskkill hints, add exception handling for corrupted lock files\n4. **Optional**: Add `update_lock_workers()` helper to record workers in lock file for debugging",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Tests for CLI Logging\n\n**Test: num_workers CLI logging appears at startup**\n```bash\n# Run scheduler and capture output\npython posting_scheduler_parallel.py --workers 2 --status 2>&1 | grep \"num_workers\"\n# Expected: Should see \"Using num_workers=2 from CLI\" in output\n```\n\n**Test: Verify num_workers matches CLI**\n```bash\n# Run with different worker counts\npython posting_scheduler_parallel.py --workers 1 --status 2>&1 | grep \"num_workers=1\"\npython posting_scheduler_parallel.py --workers 5 --status 2>&1 | grep \"num_workers=5\"\n# Both should show the CLI-specified value\n```\n\n### 2. Tests for num_workers Removal from State File\n\n**Test: Verify num_workers is NOT in saved state**\n```bash\n# Clear state, run scheduler briefly, check state file\nrm scheduler_state_parallel.json 2>/dev/null\npython posting_scheduler_parallel.py --fresh-state --add-accounts test_acc --workers 3 --status\ncat scheduler_state_parallel.json | grep -c \"num_workers\"\n# Expected: 0 (no matches - num_workers should NOT be in state file)\n```\n\n**Test: Verify settings section structure**\n```bash\npython -c \"\nimport json\nwith open('scheduler_state_parallel.json') as f:\n    state = json.load(f)\nsettings = state.get('settings', {})\nassert 'num_workers' not in settings, 'num_workers should NOT be in settings!'\nassert 'posts_per_account_per_day' in settings, 'posts_per_account_per_day should exist'\nassert 'delay_between_posts' in settings, 'delay_between_posts should exist'\nassert 'humanize' in settings, 'humanize should exist'\nprint('PASS: State file settings structure is correct')\n\"\n```\n\n### 3. Tests for PID Lock Prevention\n\n**Test: Same process re-entrant lock acquisition**\n```bash\npython -c \"\nimport sys\nsys.path.insert(0, '.')\nfrom posting_scheduler_parallel import acquire_lock, release_lock\nimport os\n\n# First acquisition\nresult1 = acquire_lock()\nassert result1 == True, 'First lock acquisition should succeed'\n\n# Same process re-acquisition (should succeed - re-entrant)\nresult2 = acquire_lock()\nassert result2 == True, 'Same-PID re-acquisition should succeed'\n\nrelease_lock()\nprint('PASS: Re-entrant lock works correctly')\n\"\n```\n\n**Test: Different process lock rejection**\n```bash\n# Terminal 1: Acquire lock and hold it\npython -c \"\nimport sys\nsys.path.insert(0, '.')\nfrom posting_scheduler_parallel import acquire_lock\nimport time\nacquire_lock()\nprint('Lock acquired, waiting 30s...')\ntime.sleep(30)\n\" &\n\nsleep 2\n\n# Terminal 2: Try to acquire (should fail)\npython -c \"\nimport sys\nsys.path.insert(0, '.')\nfrom posting_scheduler_parallel import acquire_lock\nresult = acquire_lock()\nassert result == False, 'Second process should be rejected!'\nprint('PASS: Second process correctly rejected')\n\"\n```\n\n**Test: Stale lock cleanup**\n```bash\n# Create a lock file with non-existent PID\npython -c \"\nimport json\nwith open('scheduler_parallel.lock', 'w') as f:\n    json.dump({'pid': 99999, 'started': '2025-01-01T00:00:00'}, f)\n\"\n\n# Try to acquire (should succeed with stale lock message)\npython -c \"\nimport sys\nsys.path.insert(0, '.')\nfrom posting_scheduler_parallel import acquire_lock, release_lock\nresult = acquire_lock()\nassert result == True, 'Stale lock should be taken over'\nrelease_lock()\nprint('PASS: Stale lock correctly cleaned up')\n\"\n```\n\n### 4. Integration Test: Full Startup Sequence\n\n```bash\n# Clean environment\nrm scheduler_parallel.lock 2>/dev/null\nrm scheduler_state_parallel.json 2>/dev/null\n\n# Run with status only (doesn't need Appium)\npython posting_scheduler_parallel.py --workers 2 --add-accounts test1 test2 --status 2>&1\n\n# Verify output contains:\n# 1. \"Using num_workers=2 from CLI\"\n# 2. \"Workers: 2\" in status output\n# 3. No lock error messages\n\n# Verify state file:\npython -c \"\nimport json\nwith open('scheduler_state_parallel.json') as f:\n    state = json.load(f)\nassert 'num_workers' not in state.get('settings', {}), 'num_workers leaked to state!'\nprint('PASS: Full startup sequence works correctly')\n\"\n```\n\n### 5. Manual Verification Checklist\n\n- [ ] Start scheduler with `--workers 3`, verify log shows \"Using num_workers=3 from CLI\"\n- [ ] Check `scheduler_state_parallel.json` does NOT contain `num_workers` in settings\n- [ ] Try starting second scheduler instance, verify it's blocked with clear error\n- [ ] Kill scheduler with Ctrl+C, restart, verify stale lock is cleaned up\n- [ ] Verify `scheduler_parallel.lock` contains PID and optionally workers count",
        "status": "pending",
        "dependencies": [
          "20",
          "21"
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": "24",
        "title": "Harden Parallel Scheduler State Management and Appium Session Handling",
        "description": "Improve robustness of posting_scheduler_parallel.py by explicitly stripping num_workers from loaded state, adding stale Appium driver cleanup, expanding error detection patterns in check_appium_health(), and adding startup logging for worker systemPorts.",
        "details": "## Implementation Details\n\n### 1. Explicitly Strip num_workers from Loaded State with settings.pop() (posting_scheduler_parallel.py:374-399)\n\n**Current Issue**: The `load_state()` function has comments about NOT loading num_workers, but if a stale `scheduler_state_parallel.json` contains `\"num_workers\": 3` (line 15350 shows this exists), there's no explicit removal. The comment-based prevention relies on not accessing the key, but it would be more robust to actively strip it.\n\n**Implementation** in `load_state()` method around line 391-398:\n```python\n# Load settings (but NOT num_workers - that comes from CLI)\nsettings = data.get('settings', {})\n# CRITICAL: Explicitly strip num_workers to prevent any accidental override\n# This ensures CLI argument ALWAYS takes precedence\nsettings.pop('num_workers', None)  # Remove if present, ignore if absent\n\nself.posts_per_account_per_day = settings.get('posts_per_account_per_day', DEFAULT_POSTS_PER_ACCOUNT)\nself.delay_between_posts = settings.get('delay_between_posts', DEFAULT_DELAY_BETWEEN_POSTS)\nself.humanize = settings.get('humanize', True)\n```\n\n**Also update `save_state()` around line 403-422** to ensure num_workers is never written:\n```python\n'settings': {\n    'posts_per_account_per_day': self.posts_per_account_per_day,\n    'delay_between_posts': self.delay_between_posts,\n    'humanize': self.humanize,\n    # CRITICAL: num_workers is NEVER saved - must come from CLI only\n},\n```\nNote: The current code already has comments, but adding the `.pop()` in load_state is the defensive addition.\n\n### 2. Add Stale Appium Driver Cleanup Before New Session (post_reel_smart.py:783-829)\n\n**Current Issue**: If a previous Appium session crashed or wasn't properly closed, attempting to create a new session with the same systemPort can fail with \"port already in use\" or leave orphaned sessions. The current code only removes the port forward but doesn't kill stale drivers.\n\n**Implementation** in `connect_appium()` method, before the retry loop:\n```python\ndef connect_appium(self, retries=3):\n    \"\"\"Connect Appium driver - REQUIRED for automation to work\"\"\"\n    print(\"Connecting Appium driver...\")\n    \n    # CRITICAL: Clean up any stale Appium driver from previous session\n    # This prevents \"session already exists\" or port conflict errors\n    if self.appium_driver is not None:\n        print(\"  Cleaning up stale Appium driver from previous session...\")\n        try:\n            self.appium_driver.quit()\n        except Exception as e:\n            print(f\"  Stale driver cleanup warning (safe to ignore): {e}\")\n        self.appium_driver = None\n        time.sleep(1)  # Brief pause to ensure cleanup completes\n    \n    options = UiAutomator2Options()\n    # ... rest of options setup ...\n```\n\nAdditionally, clean up any leftover UiAutomator2 server on the device before connecting:\n```python\n# Clean up any stale UiAutomator2 server on device\nif self.device:\n    print(\"  Cleaning stale UiAutomator2 server...\")\n    try:\n        subprocess.run([ADB_PATH, \"-s\", self.device, \"shell\", \n                       \"am force-stop io.appium.uiautomator2.server\"], \n                       capture_output=True, timeout=10)\n        subprocess.run([ADB_PATH, \"-s\", self.device, \"shell\", \n                       \"am force-stop io.appium.uiautomator2.server.test\"], \n                       capture_output=True, timeout=10)\n    except:\n        pass  # Best effort cleanup\n```\n\n### 3. Add More Error Patterns to Appium Health Check (posting_scheduler_parallel.py:169-180)\n\n**Current Issue**: The `check_appium_health()` function only checks if Appium returns `ready: true`, but doesn't detect specific error patterns that indicate Appium is running but in a bad state.\n\n**Enhanced Implementation**:\n```python\ndef check_appium_health(port: int = 4723) -> bool:\n    \"\"\"Check if Appium server is healthy and ready for new sessions.\n    \n    Returns False for:\n    - Server not running\n    - Server running but not ready\n    - Known error patterns: 'could not proxy', 'invalid session id'\n    \"\"\"\n    import urllib.request\n    import urllib.error\n    try:\n        url = f\"http://127.0.0.1:{port}/status\"\n        req = urllib.request.Request(url, method='GET')\n        with urllib.request.urlopen(req, timeout=5) as response:\n            data = json.loads(response.read().decode())\n            \n            # Check basic ready status\n            if not data.get('value', {}).get('ready', False):\n                return False\n            \n            # Check for known error patterns in response\n            response_str = json.dumps(data).lower()\n            error_patterns = [\n                'could not proxy',\n                'invalid session id', \n                'session not found',\n                'no active session',\n                'cannot proxy command',\n            ]\n            for pattern in error_patterns:\n                if pattern in response_str:\n                    print(f\"[APPIUM] Unhealthy: found '{pattern}' in status\")\n                    return False\n            \n            return True\n    except urllib.error.URLError as e:\n        # Connection refused or timeout - server not running\n        return False\n    except Exception as e:\n        # Any other error - assume unhealthy\n        return False\n```\n\n**Also update the error pattern detection in `execute_job()` around line 711**:\n```python\n# Check if this was an Appium error - verify Appium is still running\nappium_error_patterns = [\n    \"appium\", \n    \"HTTPConnectionPool\", \n    \"instrumentation\",\n    \"could not proxy\",\n    \"invalid session id\",\n    \"session not found\",\n    \"cannot proxy command\",\n]\nif any(pattern in error_msg.lower() for pattern in appium_error_patterns):\n    if not check_appium_health():\n        logger.log(f\"[CRITICAL] Appium is DOWN! Worker {worker_id} waiting...\")\n```\n\n### 4. Add Startup Logging for Worker systemPorts (posting_scheduler_parallel.py:783-817)\n\n**Current Issue**: While individual workers log their systemPort during execution (line 675), there's no consolidated startup log showing all worker port assignments. This makes debugging port conflicts difficult.\n\n**Implementation** in `start()` method after line 789:\n```python\ndef start(self):\n    \"\"\"Start the parallel scheduler\"\"\"\n    if self.running:\n        return\n\n    # IMPORTANT: num_workers is ALWAYS from CLI, never loaded from state file\n    logger.log(f\"Starting parallel scheduler with {self.num_workers} workers (from CLI argument)\")\n    \n    # Log systemPort assignments for all workers at startup\n    # This helps debug port conflicts and verify unique assignments\n    logger.log(f\"[STARTUP] Worker systemPort assignments:\")\n    for i in range(1, self.num_workers + 1):\n        system_port = 8200 + i\n        logger.log(f\"  Worker {i}: systemPort {system_port}\")\n    \n    # CRITICAL: Clean up stale jobs from previous crashed runs BEFORE starting\n    # ... rest of method ...\n```\n\n**Also add verification that ports aren't already in use**:\n```python\n# Verify systemPorts are available before starting workers\nfor i in range(1, self.num_workers + 1):\n    system_port = 8200 + i\n    try:\n        import socket\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.settimeout(1)\n        result = s.connect_ex(('127.0.0.1', system_port))\n        s.close()\n        if result == 0:\n            logger.log(f\"[WARNING] systemPort {system_port} already in use - may cause conflicts\")\n    except:\n        pass  # Best effort check\n```\n\n### File Changes Summary\n\n| File | Changes |\n|------|---------|\n| `posting_scheduler_parallel.py` | 1) Add `settings.pop('num_workers', None)` in `load_state()`, 2) Enhance `check_appium_health()` with more error patterns, 3) Add startup logging for worker systemPorts in `start()`, 4) Update error detection patterns in `execute_job()` |\n| `post_reel_smart.py` | Add stale Appium driver cleanup at start of `connect_appium()`, including killing UiAutomator2 server processes on device |",
        "testStrategy": "## Test Strategy\n\n### 1. Test num_workers State Stripping\n\n**Test A: Verify num_workers is stripped from loaded state**\n```bash\n# 1. Create state file with stale num_workers\necho '{\"settings\": {\"num_workers\": 5, \"humanize\": true}}' > scheduler_state_parallel_test.json\n\n# 2. Run scheduler with different worker count\npython posting_scheduler_parallel.py --workers 2 --status 2>&1\n\n# Expected: Should see \"Starting parallel scheduler with 2 workers (from CLI argument)\"\n# NOT \"Starting parallel scheduler with 5 workers\"\n```\n\n**Test B: Verify num_workers is never saved**\n```bash\n# 1. Run scheduler briefly\npython posting_scheduler_parallel.py --workers 7 --status\n\n# 2. Check state file\ngrep -i \"num_workers\" scheduler_state_parallel.json\n\n# Expected: \"num_workers\" should NOT appear in the file\n```\n\n### 2. Test Stale Appium Driver Cleanup\n\n**Test A: Verify cleanup runs on reconnect**\n```bash\n# 1. Manually crash an Appium session (Ctrl+C during a job)\n# 2. Start a new job on the same worker\n\n# Expected in logs:\n# \"Cleaning up stale Appium driver from previous session...\"\n# \"Cleaning stale UiAutomator2 server...\"\n```\n\n**Test B: Verify reconnect_appium() calls cleanup**\n```python\n# Unit test\nposter = SmartInstagramPoster(\"test_phone\", system_port=8201)\nposter.appium_driver = \"mock_stale_driver\"  # Simulate stale driver\nposter.reconnect_appium()\n# Should not raise, should clean up gracefully\n```\n\n### 3. Test Enhanced Appium Health Check\n\n**Test A: Verify error pattern detection**\n```python\n# Mock response with error patterns\nimport unittest\nfrom unittest.mock import patch\n\nclass TestAppiumHealth(unittest.TestCase):\n    @patch('urllib.request.urlopen')\n    def test_detects_proxy_error(self, mock_urlopen):\n        mock_response = MagicMock()\n        mock_response.read.return_value = b'{\"value\": {\"ready\": true, \"message\": \"could not proxy command\"}}'\n        mock_urlopen.return_value.__enter__.return_value = mock_response\n        \n        result = check_appium_health()\n        self.assertFalse(result)  # Should be False due to error pattern\n```\n\n**Test B: Manual verification**\n```bash\n# 1. Stop Appium server\n# 2. Run health check - should return False\n\n# 3. Start Appium but kill a session mid-use\n# 4. Run health check - should detect bad state\n```\n\n### 4. Test Worker systemPort Startup Logging\n\n**Test A: Verify startup log output**\n```bash\npython posting_scheduler_parallel.py --workers 3 --run 2>&1 | head -20\n\n# Expected output should include:\n# \"[STARTUP] Worker systemPort assignments:\"\n# \"  Worker 1: systemPort 8201\"\n# \"  Worker 2: systemPort 8202\"\n# \"  Worker 3: systemPort 8203\"\n```\n\n**Test B: Verify port conflict warning**\n```bash\n# 1. Occupy port 8202\nnc -l 8202 &\n\n# 2. Start scheduler\npython posting_scheduler_parallel.py --workers 3 --run\n\n# Expected: \"[WARNING] systemPort 8202 already in use - may cause conflicts\"\n```\n\n### 5. Integration Test: Full Run with All Fixes\n\n```bash\n# 1. Ensure Appium is running\ncurl -s http://127.0.0.1:4723/status | jq .value.ready\n\n# 2. Create stale state file with num_workers\ncp scheduler_state_parallel.json scheduler_state_backup.json\n# Edit to add \"num_workers\": 5 in settings\n\n# 3. Run scheduler with 2 workers\npython posting_scheduler_parallel.py --accounts-file accounts_list.txt --add-folder chunk_01c --workers 2 --run\n\n# 4. Verify:\n#    - Logs show \"2 workers (from CLI argument)\" not 5\n#    - Logs show systemPort assignments 8201, 8202\n#    - No port conflict warnings (if ports are free)\n#    - State file after run has no num_workers key\n```\n\n### 6. Regression Test: Ensure No Breaking Changes\n\n```bash\n# Run existing tests/behavior\npython posting_scheduler_parallel.py --status  # Should work\npython posting_scheduler_parallel.py --workers 1 --status  # Should show 1 worker\n\n# Single account test post\npython post_reel_smart.py test_account video.mp4 \"Test\" \n# Should connect successfully with cleanup messages\n```",
        "status": "pending",
        "dependencies": [
          "20",
          "21",
          "23"
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": "25",
        "title": "Investigate and Fix ADB Device Missing from Connected List After glogin for Appium Sessions",
        "description": "Research and implement robust diagnostics and readiness checks so that after `adb connect` and `glogin`, the Geelark device is reliably listed as a connected ADB device before Appium starts, and glogin failures are surfaced instead of failing later with 'Device was not in the list of connected devices'.",
        "details": "## Goals\n- Understand and eliminate the failure mode where `adb connect` succeeds but the device is missing from Appium’s connected device list after `glogin`, leading to \"Device was not in the list of connected devices\" errors.\n- Make `glogin` failures observable (no more silent empty `[]` results without clear logs or exceptions).\n- Introduce a **single, authoritative ADB readiness gate** that runs between `adb connect`/`glogin` and Appium session creation.\n\n## High-Level Approach\n1. **Instrument and wrap `glogin`** so that any non-\"device ready\" outcome is logged at ERROR level and raises a clear exception instead of silently returning `[]`.\n2. **Unify ADB readiness checks** into a reusable helper that:\n   - Confirms the device is present in `adb devices` with `device` status (not `offline`, `unauthorized`, etc.).[2][6]\n   - Optionally runs a lightweight command (`adb shell getprop ro.build.version.sdk` or `echo`) to ensure the shell is responsive.\n   - Verifies the exact serial/host:port used by Appium is what appears in `adb devices`.[1][3]\n3. **Ensure Appium is looking at the same ADB server and device ID** that `adb connect`/`glogin` used (alignment of serial, ADB host/port, environment variables, and capabilities).[1][2][5]\n4. **Document and guard against known conditions where `adb devices` lists a device but Appium still cannot use it** (wireless/TCP devices, Appium restarting ADB, permission issues, offline/unauthorized state).[1][3][4][6]\n\n## Detailed Implementation Steps\n\n### 1. Analyze Current glogin + ADB Flow\n- Locate all code paths that:\n  - Run `adb connect <ip:port>` against Geelark devices.\n  - Call `glogin` (likely a CLI or API to log into a Geelark cloud phone and/or bind it to an account).\n  - Instantiate Appium sessions (e.g., in `post_reel_smart.py connect()` already modified by Task 22).\n- Draw a simple sequence diagram in the repo docs (e.g., `docs/geelark_adb_flow.md`) showing:\n  1) Geelark device acquisition.\n  2) `adb connect`.\n  3) `glogin`.\n  4) ADB readiness checks.\n  5) Appium session creation.\n- Confirm how `GeelarkDeviceController` (Task 3) is or will be used for this flow, and where the new logic should live (preferably behind the abstraction: e.g., `GeelarkDeviceController.connect()` ensures ADB + glogin readiness before returning a handle).\n\n### 2. Wrap glogin with Strong Diagnostics and Failure Semantics\n- Implement a Python wrapper function (or class) around glogin, e.g. in `geelark/glogin_client.py`:\n  ```python\n  import json\n  import subprocess\n  import logging\n\n  logger = logging.getLogger(__name__)\n\n  class GLoginError(RuntimeError):\n      pass\n\n  def run_glogin(account_name: str, device_id: str | None = None, timeout: int = 60) -> dict:\n      cmd = [\"glogin\", \"--account\", account_name]\n      if device_id:\n          cmd += [\"--device\", device_id]\n\n      logger.info(\"Running glogin: %s\", \" \".join(cmd))\n      try:\n          proc = subprocess.run(\n              cmd,\n              capture_output=True,\n              text=True,\n              timeout=timeout,\n              check=False,\n          )\n      except subprocess.TimeoutExpired as e:\n          logger.error(\"glogin timed out after %s seconds\", timeout)\n          raise GLoginError(f\"glogin timed out after {timeout}s\") from e\n\n      stdout, stderr, code = proc.stdout.strip(), proc.stderr.strip(), proc.returncode\n      logger.debug(\"glogin exit=%s stdout=%r stderr=%r\", code, stdout, stderr)\n\n      if code != 0:\n          raise GLoginError(f\"glogin failed with code {code}: {stderr or stdout}\")\n\n      # Expect JSON or at least a non-empty structure\n      try:\n          data = json.loads(stdout) if stdout else None\n      except json.JSONDecodeError as e:\n          logger.error(\"glogin returned non-JSON output: %r\", stdout)\n          raise GLoginError(\"glogin returned non-JSON output\") from e\n\n      if not data:\n          # This is the previously silent failure mode\n          logger.error(\"glogin returned empty data for account=%s device=%s\", account_name, device_id)\n          raise GLoginError(\"glogin returned empty data (no devices)\")\n\n      return data\n  ```\n- Replace any direct calls to `glogin` in the codebase with `run_glogin()` and handle `GLoginError` explicitly (abort connect, do not proceed to Appium).\n- Add structured logging:\n  - Tag logs with account, Geelark device identifier, and ADB serial.\n  - Capture both stdout and stderr to aid remote debugging.\n\n### 3. Implement a Central ADB Readiness Helper (Beyond Task 22)\nTask 22 added a basic wait loop for `adb devices` in `post_reel_smart.py`.\nFor a more robust solution:\n- Extract a reusable helper, e.g. `adb_utils.wait_for_device_ready(serial: str, timeout_s: int = 60, poll_s: float = 3.0)`:\n  ```python\n  import subprocess\n  import time\n  import logging\n\n  logger = logging.getLogger(__name__)\n\n  class AdbDeviceNotReadyError(RuntimeError):\n      pass\n\n  def _list_adb_devices() -> dict[str, str]:\n      # Returns {serial: state}\n      proc = subprocess.run([\n          \"adb\", \"devices\"], capture_output=True, text=True, check=False\n      )\n      lines = proc.stdout.splitlines()\n      devices = {}\n      for line in lines[1:]:  # skip header\n          line = line.strip()\n          if not line:\n              continue\n          parts = line.split()\n          if len(parts) >= 2:\n              devices[parts[0]] = parts[1]\n      return devices\n\n  def wait_for_device_ready(serial: str, timeout_s: int = 60, poll_s: float = 3.0) -> None:\n      deadline = time.time() + timeout_s\n      last_state = None\n      while time.time() < deadline:\n          devices = _list_adb_devices()\n          state = devices.get(serial)\n          last_state = state\n          logger.debug(\"ADB device states: %s\", devices)\n\n          if state == \"device\":\n              # Optional: quick shell ping\n              shell = subprocess.run([\n                  \"adb\", \"-s\", serial, \"shell\", \"echo\", \"ping\"\n              ], capture_output=True, text=True, check=False)\n              if shell.returncode == 0 and \"ping\" in shell.stdout:\n                  logger.info(\"ADB device %s is ready (state=device, shell responsive)\", serial)\n                  return\n              logger.warning(\"ADB device %s state=device but shell not responsive yet\", serial)\n\n          time.sleep(poll_s)\n\n      raise AdbDeviceNotReadyError(\n          f\"ADB device {serial} not ready within {timeout_s}s (last_state={last_state!r})\"\n      )\n  ```\n- Update `SmartInstagramPoster.connect()` (and any other connect flows) to:\n  1) Run `adb connect <ip:port>` and validate non-zero exit codes.\n  2) Call `run_glogin()` for the relevant account/device.\n  3) Derive the exact ADB serial Appium will use:\n     - For TCP devices typically `<ip>:<port>`.\n     - For USB devices whatever appears in `adb devices`.[1][3]\n  4) Call `wait_for_device_ready(serial, timeout_s=60)`.\n  5) Only then create the Appium driver.\n- Log the **resolved serial** so that Appium logs can be correlated with ADB logs.\n\n### 4. Align Appium Capabilities and ADB Configuration with the Device\nBest practices and common pitfalls from Appium issues and docs:[1][2][3][5][6]\n- Ensure the capabilities use the **same identifier** that `adb devices` shows:\n  - For Android, Appium uses `deviceName` to pick the device when only one is connected; `udid` can be used to explicitly specify the serial.[1][6]\n  - For TCP devices, use the `<ip>:<port>` string as `udid` and/or `deviceName`, but confirm it matches `adb devices` output exactly (no extra spaces or different ports).[1][3]\n- Avoid Appium killing or restarting ADB in ways that lose the TCP-connected device:\n  - Consider setting Appium capability `suppressKillServer=true` when using wireless/TCP devices, as recommended in Appium issues for wireless setups.[6]\n  - If you run your own adb server on a non-default host/port, configure Appium’s `remoteAdbHost`/port consistently.[3]\n- Confirm environment consistency (relies on Task 16):\n  - The Python process starting Appium must use the same `adb` binary/SDK as your CLI (`ANDROID_HOME`/`ANDROID_SDK_ROOT`).[5][16]\n  - Log out the resolved `adb` path and SDK root once per run.\n\n### 5. Enumerate and Log Root Causes When Device Not Listed\nImplement a diagnostic function `explain_missing_device(serial)` that is called before raising final errors:\n- Check and log:\n  - Output of `adb devices -l`.\n  - Whether device appears as `offline` or `unauthorized`.[2][6]\n  - Whether `adb get-state` for the serial returns `device`, `offline`, or errors.\n  - Whether `glogin` succeeded and what it returned (device mappings, if any).\n  - Whether Appium is configured with `suppressKillServer` or may be restarting ADB (which can drop TCP devices).[4][6]\n- Convert these into precise log messages:\n  - \"Device appears as unauthorized in ADB; check USB debugging prompt on device.\"\n  - \"Device connected via TCP <ip:port> but Appium restarted adb and lost the connection; consider suppressKillServer or reconnect after session start.\"[3][4]\n\n### 6. Integrate with GeelarkDeviceController Abstraction (Task 3)\n- Extend `GeelarkDeviceController.connect(account_name: str) -> DeviceHandle` to:\n  - Internally: run `adb connect`, `run_glogin`, `wait_for_device_ready`, and any Geelark account/device registration logic.\n  - Return a `DeviceHandle` that includes the resolved `adb_serial` to be used in Appium capabilities.\n- This ensures **all higher-level flows** (posting schedulers, debug harnesses, etc.) benefit from the same hardened path without duplicating logic.\n\n### 7. Documentation\n- Add `docs/geelark_adb_troubleshooting.md` covering:\n  - The new glogin wrapper behavior and error messages.\n  - How the ADB readiness gate works and what errors mean.\n  - Common causes of \"Device was not in the list of connected devices\" and recommended operator actions.\n- Cross-link from any existing Android/Appium troubleshooting docs created for Task 12 and Task 16.\n\n## Non-Goals\n- Changing Geelark backend behavior itself; this task focuses on the client/orchestration side.\n- Replacing Task 22’s wait loop entirely; instead, this task generalizes and centralizes it.\n",
        "testStrategy": "1. **Unit Tests for glogin Wrapper**\n- Mock `subprocess.run` to simulate:\n  - Non-zero exit code with stderr: assert `GLoginError` is raised and message includes exit code & stderr.\n  - Zero exit, empty stdout: assert `GLoginError` is raised and an ERROR log is emitted about empty data.\n  - Zero exit, invalid JSON stdout: assert `GLoginError` is raised and error is logged.\n  - Zero exit, valid JSON array/object: assert the parsed data is returned and INFO/DEBUG logs contain the command and output.\n\n2. **Unit Tests for ADB Readiness Helper**\n- Mock `_list_adb_devices()` and `subprocess.run` for shell ping to cover:\n  - Device appears as `device` on first poll and shell returns `ping`: helper returns without error and logs readiness.\n  - Device cycles through `offline` → `device` within timeout: helper eventually returns successfully.\n  - Device never appears or remains `offline` until timeout: assert `AdbDeviceNotReadyError` with `last_state` info.\n  - `adb devices` returns transient empty list followed by valid list: helper tolerates and still succeeds.\n\n3. **Integration Tests with Real/Geelark Device (Happy Paths)**\n- Scenario A: USB-connected device\n  - Start a Geelark phone (or local emulator) with USB debugging enabled.\n  - Run a small script that uses `GeelarkDeviceController.connect(account_name)`.\n  - Verify logs show: `adb connect` (if applicable), successful `glogin`, ADB readiness messages, and that `adb devices` lists the serial as `device`.\n  - Start an Appium session using the returned `adb_serial` as `udid` or `deviceName`; confirm no \"Device was not in the list of connected devices\" error and the session launches Instagram (reusing harness ideas from Task 12).\n\n- Scenario B: TCP-connected device\n  - Connect the device via `adb connect <ip:port>` and ensure it appears in `adb devices` as `<ip:port> device`.\n  - Use the orchestrated connect flow and verify that:\n    - `run_glogin()` succeeds and logs non-empty data.\n    - `wait_for_device_ready('<ip:port>')` completes successfully.\n    - Appium capabilities use `<ip:port>` consistently and the session starts.\n\n4. **Integration Tests for Failure Modes**\n- Scenario C: glogin returns empty `[]`\n  - Temporarily mock `glogin` to print `[]` with exit code 0.\n  - Run the connect flow and verify:\n    - Connect aborts before Appium is started.\n    - `GLoginError` is raised.\n    - Logs contain clear ERROR message about glogin returning empty data.\n\n- Scenario D: Device never comes online\n  - Connect to a non-existent or powered-off device IP.\n  - Verify `wait_for_device_ready()` times out with `AdbDeviceNotReadyError` and connect aborts without launching Appium.\n  - Confirm that the diagnostic function logs ADB state and suggested root causes.\n\n- Scenario E: Device listed as `offline`/`unauthorized`\n  - Reproduce by revoking USB debugging authorization or purposely misconfiguring.\n  - Confirm diagnostics log `offline`/`unauthorized` state from `adb devices -l` and that no Appium session is attempted.\n\n5. **Configuration and Capability Alignment Tests**\n- Run a small test harness that:\n  - Starts an Appium server with `suppressKillServer=true` for a TCP device.\n  - Ensures that after connect + readiness gate, Appium logs show the same serial as `adb devices` and the session remains stable through simple operations (tap, screenshot) using `GeelarkDeviceController` methods.\n- Verify that environment logging shows the same `adb` binary and SDK root as the CLI (leveraging Task 16’s work).\n\n6. **Regression Tests Around Task 22 Behavior**\n- Confirm that existing `SmartInstagramPoster.connect()` behavior (60s wait loop, raising on missing device) is preserved or improved:\n  - Device appears within 10 seconds: connect completes quickly and still passes all new checks.\n  - Device appears just before 60-second boundary: connect succeeds rather than failing early.\n  - No device: clear exception is raised and no attempt is made to start an Appium session.\n\n7. **Logging Verification**\n- For each scenario above, capture logs and verify they include at minimum:\n  - glogin command and outcome (exit code, truncated stdout/stderr).\n  - ADB device list snapshots and final state.\n  - Resolved serial used for Appium.\n  - Clear error messages when aborting before Appium due to glogin or ADB readiness failures.",
        "status": "pending",
        "dependencies": [
          "3",
          "16",
          "22"
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": "26",
        "title": "Diagnose and Fix Persistent Appium systemPort Busy Errors After Failed Sessions",
        "description": "Investigate why Appium’s UiAutomator2 systemPort (e.g., 8201) remains busy after failed sessions and implement robust cleanup logic so ports are reliably freed without always requiring a full Appium server restart.",
        "details": "## Goals\n- Determine whether the **systemPort** is held by the Appium Node process, the UiAutomator2 server on-device, or an ADB port‑forwarding rule.\n- Implement **reliable cleanup** of UiAutomator2/systemPort resources when sessions crash or fail, so subsequent sessions do not hit \"local port #8201 is busy\" or \"Cannot find any free port in range 8200..8299\" errors.[1][3][8]\n- Define whether and when it is necessary to **restart the Appium server** vs. cleaning up at the ADB / device level only.\n\n## Background & Constraints\n- Appium’s UiAutomator2 driver uses **`systemPort`** (default 8200, configurable) as a local port that the Appium server proxies to the UiAutomator2 server running on the device.[4][8]\n- In parallel or repeated runs, stale sessions or orphaned UiAutomator2 processes can leave ports in a BUSY state and block new sessions, often due to **inaccurate sessions management** and not properly closing old sessions.[1][3]\n- Current cleanup attempts only run `adb forward --remove`, which may not address:\n  - The **Node/Appium process** still listening on systemPort.\n  - UiAutomator2 still running on device and holding the target port via a still‑valid ADB forward.\n  - OS‑level TCP TIME_WAIT or zombie processes after abrupt crashes.\n\n## Implementation Plan\n\n### 1. Instrumentation & Root Cause Diagnostics\n1. **Add targeted logging around session lifecycle** in the Appium client wrapper (e.g., `connect_appium`, driver teardown, error handlers):\n   - Log assigned **systemPort** per worker/device.\n   - Log when **driver.quit()** is called and whether it succeeded or threw.\n   - Log whether any **cleanup function** (ADB forward removal, UiAutomator2 kill, etc.) ran, and its results.\n\n2. **Add low‑level port diagnostics** on the Appium host:\n   - Implement helper functions, e.g. `check_local_port_usage(port)` using `lsof -i :{port}` or `netstat` (and `ps` fallback on non‑UNIX if needed) to identify which process holds 8201.\n   - Call this:\n     - Immediately after a **session crash/failure**.\n     - Before starting a **new session** on the same device/worker.\n   - Log:\n     - PID, process name (should show `node`/`appium` if server is still bound; or nothing if freed).\n     - Whether an ADB process is involved.\n\n3. **ADB‑level inspection**:\n   - Implement a helper `list_adb_forwards(device_udid=None)` that runs `adb -s <udid> forward --list` (or `adb forward --list` if global) and logs entries for:\n     - `tcp:8201` → `tcp:` or `localabstract:...` typically used by UiAutomator2.[8]\n   - Call this in the same error paths as above and at scheduler startup when enumerating worker/systemPort allocations (ties into Task 24’s startup logging).\n\n4. **On‑device process inspection (optional, if feasible)**:\n   - Add a debug‑only helper that runs `adb -s <udid> shell ps | grep uiautomator` or `ps | grep io.appium.uiautomator2.server` to detect zombie UiAutomator2 processes after a crash.\n\n5. **Document behavior**:\n   - From collected logs, document for at least 3–5 failure cases:\n     - Whether the OS still shows a listening socket on 8201.\n     - Whether an ADB forward for 8201 persists.\n     - Whether UiAutomator2 is still running on the device.\n   - Use this to answer explicitly:\n     - **What actually holds the port** in your environment.\n\n### 2. Robust Cleanup Workflow for Failed/Aborted Sessions\n1. **Centralize teardown** in a single function (e.g., `cleanup_appium_session(driver, device, system_port)`):\n   - Responsibilities:\n     - Safe `driver.quit()` with timeout and exception handling.\n     - Host‑side port cleanup.\n     - ADB forward cleanup.\n     - Optional device‑side UiAutomator2 kill.\n\n2. **Host‑side port cleanup**:\n   - After `driver.quit()`:\n     - Run `check_local_port_usage(system_port)`.\n     - If port is still in use **and** the holder is the main Appium server PID:\n       - Mark the associated server instance as **unhealthy** and schedule a restart (see section 3).\n     - If held by a **stale child process** or unknown PID:\n       - Log error and, for non‑production or under a config flag, attempt a targeted kill (`os.kill(pid, SIGTERM)` with fallback `SIGKILL`), then recheck port.\n\n3. **ADB forward cleanup**:\n   - Replace naïve `adb forward --remove tcp:8201` with:\n     - First run `adb -s <udid> forward --list` and check if `tcp:systemPort` exists; if yes:\n       - Run `adb -s <udid> forward --remove tcp:systemPort`.\n       - Verify removal by listing forwards again.\n     - If running multiple forwards per device, guard against removing unrelated ports.\n   - Handle command failures with detailed logs and **non‑silent exceptions** if cleanup is critical.\n\n4. **Device‑side UiAutomator2 cleanup** (as a last resort):\n   - If port remains busy and ADB forward is gone, but logs show UiAutomator2 is still active:\n     - Run one of:\n       - `adb -s <udid> shell am force-stop io.appium.uiautomator2.server`.\n       - Or `adb -s <udid> shell pkill -f io.appium.uiautomator2.server` (Android version‑dependent).\n     - Optionally re‑run the ADB forward removal and port checks.\n\n5. **Retry policy and backoff**:\n   - Implement a bounded retry loop in the scheduler/worker when starting a new session:\n     - If \"local port #8201 is busy\" or \"Cannot find any free port\" occurs:[1][3]\n       - Immediately trigger the above cleanup.\n       - Wait with exponential backoff (e.g., 1s → 2s → 4s) and re‑attempt session creation up to N times (e.g., N=3).\n     - If after N retries port is still busy:\n       - Mark device as **temporarily unhealthy** and skip further jobs for a configurable cooldown period.\n\n### 3. Define When to Restart the Appium Server\n1. **Heuristics for mandatory restart**:\n   - Restart the Appium server instance if any of the following hold after cleanup attempts:\n     - Port 8201 remains bound to the main Appium process even after `driver.quit()` and ADB forward cleanup.\n     - Multiple busy ports in the 8200..8299 range with no corresponding active sessions.[1][3]\n     - Repeated (>N) \"systemPort busy\" failures on the same server within a short window.\n\n2. **Implementation options**:\n   - If you manage Appium as a **long‑lived service**:\n     - Expose a control interface from the scheduler/worker that can:\n       - Send a graceful shutdown signal to the Appium process.\n       - Wait for process exit and port release.\n       - Restart with the same configuration (ties into Task 16’s environment consistency for ANDROID_HOME/SDK).[16]\n   - If each worker starts its own Appium server process:\n     - Encapsulate Appium process management in a class that provides `start()`, `stop()`, and `restart_if_unhealthy()` using the above heuristics.\n\n3. **Document server‑restart policy**:\n   - In `README` or developer docs, document:\n     - Under what conditions a server restart is triggered.\n     - Expected service impact and recovery time.\n\n### 4. Integration with Scheduler & Existing Tasks\n1. **Leverage Task 18 and 22 connect()/cleanup logic**:\n   - Reuse existing device‑ready and connection‑stability improvements so the new cleanup hooks execute in the same **connect()/disconnect()** path used by Task 18 and Task 22.[18][22]\n   - Ensure the new session cleanup function is invoked on:\n     - Normal job completion.\n     - Exceptions in `connect_appium` or glogin (Task 25).[25]\n\n2. **Extend Task 24’s \"stale Appium driver cleanup\"**:\n   - Integrate systemPort tracking into the parallel scheduler state:\n     - Track `device_id → systemPort` mapping per worker.\n     - On scheduler startup, log assigned ports and run `check_local_port_usage` for each (aligns with Task 24’s startup worker systemPort logging).[24]\n   - When cleaning up stale drivers or jobs, also invoke the new **systemPort cleanup** routine.\n\n3. **Configuration & observability**:\n   - Add configuration keys (e.g., in settings JSON or env vars):\n     - `appium_system_port_cleanup_enabled` (bool).\n     - `appium_system_port_cleanup_retries` (int).\n     - `appium_server_restart_on_port_stuck` (bool).\n   - Ensure logs clearly label entries with `[SYSTEM_PORT_CLEANUP]` or similar for easier grep and alerting.\n\n### 5. Best Practices & Hardening\n- Avoid hard‑coding 8201 everywhere; always refer to the capability or per‑device configuration for **systemPort**.[4][8]\n- For **parallel devices**, assign **unique systemPort values** rather than relying on Appium’s auto‑range to reduce contention and make debugging easier.[4][5][8]\n- Ensure all cleanup operations are **idempotent**:\n  - Multiple calls should be safe and not error if the port/forward/process is already gone.\n- Wrap all shell/ADB invocations in a thin abstraction with:\n  - Timeouts.\n  - Exit‑code checking.\n  - Structured logging of command + stdout/stderr.\n",
        "testStrategy": "## Manual & Automated Tests\n\n### 1. Repro & Diagnostics Validation\n1. Intentionally trigger a failing Appium session (e.g., invalid capabilities or forced crash) using a device configured with `systemPort=8201`.\n2. After failure, verify logs contain:\n   - The failure reason.\n   - Output from `check_local_port_usage(8201)` showing which process (if any) holds the port.\n   - ADB forward listing for 8201.\n3. Confirm that in at least one scenario you can see the pre‑fix behavior (port remains busy) and that after implementing the new cleanup logic, the same scenario no longer leaves the port busy.\n\n### 2. Cleanup Function Tests\n1. **Unit tests** (can be Python tests with subprocess mocking):\n   - Mock `driver.quit()` to:\n     - Succeed.\n     - Raise an exception.\n   - Mock port‑check function to simulate:\n     - Port already free.\n     - Port held by a dummy PID.\n   - Mock ADB commands to verify:\n     - `adb -s <udid> forward --list` is called before and after removal.\n     - `adb -s <udid> forward --remove tcp:8201` is invoked only when a matching forward exists.\n   - Assert the cleanup function:\n     - Logs appropriate messages.\n     - Continues even if some commands fail (within reason).\n\n2. **Integration tests on a real device/emulator**:\n   - Scenario A: Normal run\n     - Start scheduler/worker and run a job that creates and closes an Appium session normally.\n     - Confirm that after job completion:\n       - No ADB forward exists for `tcp:8201`.\n       - `lsof`/`netstat` shows no listener on 8201.\n   - Scenario B: Abrupt failure before `driver.quit()`\n     - Introduce a controlled exception after session creation but before explicit teardown.\n     - Let the new centralized cleanup run.\n     - Verify that new sessions on the same device with `systemPort=8201` start successfully without \"local port #8201 is busy\".\n\n### 3. Appium Server Restart Policy Tests\n1. Configure `appium_server_restart_on_port_stuck=true` and artificially simulate a stuck port by mocking `check_local_port_usage` to always report 8201 as busy.\n2. Attempt to start a new session:\n   - Assert that the code path triggers an Appium server restart (e.g., by checking that the Appium process PID changes and a restart log message is emitted).\n   - Verify that after restart (with mocks reset so that port is free), a new session can be created successfully.\n\n### 4. Parallel & Regression Tests\n1. Run multiple workers/devices in parallel with **distinct systemPorts** and repeated job failures.\n2. Observe over a long run (e.g., 50–100 jobs per device):\n   - No accumulation of stale ADB forwards.\n   - No increase in \"systemPort busy\" or \"Cannot find any free port in range 8200..8299\" type errors.[3]\n3. Confirm that existing device‑ready checks and connection improvements from Tasks 18 and 22 still pass their tests and that introducing the new cleanup logic does not reintroduce \"Device was not in the list of connected devices\" or similar errors.\n",
        "status": "pending",
        "dependencies": [
          "16",
          "18",
          "22",
          "24",
          "25"
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": "27",
        "title": "Enforce Strict ADB Readiness Gate and Hard-Fail Behavior in post_reel_smart.py",
        "description": "Implement a strict ADB readiness gate that blocks progression to Appium until a specific ip:port device is reported as ready by `adb devices`, and treat repeated empty glogin results as fatal errors rather than warnings.",
        "details": "Implementation plan:\n\n1. **Identify integration points in `post_reel_smart.py`**\n- Locate the flow where **Geelark** reports the phone ready and `enableadb` is called, immediately before any Appium connection logic (likely near existing glogin/Appium startup code added around Task 18).\n- Document current behavior: how ADB is enabled, where `adb devices` is inspected (if at all), and where glogin is invoked.\n- Identify all locations that currently **log a warning and continue** on ADB/glogin/Appium readiness issues; these must be changed to **hard failures via exceptions**.\n\n2. **Define configuration knobs**\n- Add two new configuration values, preferably in the central config module or constants section used by `post_reel_smart.py`:\n  - `ADB_MAX_WAIT_SECONDS: int` (e.g., default 120).\n  - `ADB_POLL_INTERVAL_SECONDS: int` (e.g., default 3–5).\n- Ensure they are:\n  - Overrideable via environment variables or config file if your project already uses those patterns.\n  - Validated (positive integers, `poll_interval < max_wait`).\n\n3. **Implement a strict ADB wait helper**\n- Implement a utility function in `post_reel_smart.py` or a nearby `adb_utils.py`:\n\n```python\nimport subprocess\nimport time\nfrom typing import Optional\n\nclass AdbReadinessError(RuntimeError):\n    pass\n\ndef _run_adb_devices() -> str:\n    # Assume adb is already on PATH and server running per Task 16/18\n    # Use text mode and check return code.\n    result = subprocess.run(\n        [\"adb\", \"devices\"],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n        timeout=10,\n        check=False,\n    )\n    if result.returncode != 0:\n        # Non-zero return code is treated as transient failure; caller decides when to give up\n        return \"\"\n    return result.stdout or \"\"\n\ndef _parse_adb_devices(stdout: str) -> dict[str, str]:\n    \"\"\"Parse `adb devices` output into {serial: status}.\n\n    Expected format lines after header: \"<serial>\\t<status>\".\n    Ignores blank and header lines.\n    \"\"\"\n    devices: dict[str, str] = {}\n    for line in stdout.splitlines():\n        line = line.strip()\n        if not line or line.startswith(\"List of devices attached\"):\n            continue\n        # Only accept lines with a tab separator to avoid malformed data.\n        if \"\\t\" not in line:\n            continue\n        serial, status = line.split(\"\\t\", 1)\n        serial, status = serial.strip(), status.strip()\n        if not serial or not status:\n            continue\n        devices[serial] = status\n    return devices\n\ndef wait_for_adb_device_ready(\n    target_serial: str,\n    max_wait_seconds: int,\n    poll_interval_seconds: int,\n    logger,\n) -> None:\n    \"\"\"Block until `adb devices` shows `target_serial` with status 'device'.\n\n    Raises AdbReadinessError if timeout is hit or device is offline/missing.\n    \"\"\"\n    deadline = time.time() + max_wait_seconds\n    while time.time() < deadline:\n        output = _run_adb_devices()\n        devices = _parse_adb_devices(output)\n        status = devices.get(target_serial)\n\n        # Require exact ip:port key and non-offline status.\n        if status == \"device\":\n            logger.info(f\"ADB device {target_serial} is ready (status=device)\")\n            return\n        if status == \"offline\":\n            logger.warning(\n                f\"ADB device {target_serial} currently offline; will retry until deadline\"\n            )\n        elif status is None:\n            logger.warning(\n                f\"ADB device {target_serial} not present in `adb devices`; will retry until deadline\"\n            )\n\n        time.sleep(poll_interval_seconds)\n\n    raise AdbReadinessError(\n        f\"ADB readiness failed: device {target_serial} never reached status 'device' \"\n        f\"within {max_wait_seconds}s\"\n    )\n```\n\n- **Key rules enforced**:\n  - Only accept entries with a **tab** separator (`ip:port\\tstatus`).\n  - Require **status == 'device'**; any other status (e.g., `offline`, `unauthorized`) is considered not-ready.\n  - Poll `adb devices` every `ADB_POLL_INTERVAL_SECONDS` until `ADB_MAX_WAIT_SECONDS`.\n  - On timeout, raise **`AdbReadinessError`**; **do not attempt Appium**.\n\n4. **Wire the wait loop into the Geelark/Appium flow**\n- In `post_reel_smart.py` after Geelark indicates the phone is ready and **immediately after** `enableadb` completes successfully:\n  - Determine the **exact `ip:port` serial** that ADB should expose (likely available from Geelark config or connection details used in Task 18’s connect flow).\n  - Call `wait_for_adb_device_ready(...)` before any glogin/Appium logic.\n\nExample integration:\n\n```python\n# After geelark.enableadb(device) returns\nadb_serial = f\"{device.ip}:{device.adb_port}\"  # adapt to your actual model\n\nwait_for_adb_device_ready(\n    target_serial=adb_serial,\n    max_wait_seconds=config.ADB_MAX_WAIT_SECONDS,\n    poll_interval_seconds=config.ADB_POLL_INTERVAL_SECONDS,\n    logger=logger,\n)\n\n# Only reach here if ADB is truly ready; now proceed to glogin/Appium\n```\n\n- Ensure that if `wait_for_adb_device_ready` raises, the exception propagates up to the main orchestrator so that the job is marked **failed** and no Appium session is attempted.\n\n5. **Convert glogin empty-array handling into hard failure**\n- Identify where `glogin` (or equivalent login/bootstrap call) is invoked and its return type (currently returns an array/sequence of something like accounts/devices).\n- Introduce a robust wrapper to enforce failure on repeated empty results:\n\n```python\nclass GloginReadinessError(RuntimeError):\n    pass\n\nGLOGIN_MAX_EMPTY_RETRIES = 3\n\n\ndef run_glogin_with_retries(logger, *args, **kwargs):\n    empty_count = 0\n    while empty_count < GLOGIN_MAX_EMPTY_RETRIES:\n        result = glogin(*args, **kwargs)  # adapt to actual call\n        if result:\n            return result\n        empty_count += 1\n        logger.warning(\n            f\"glogin returned empty array (attempt {empty_count}/\" \n            f\"{GLOGIN_MAX_EMPTY_RETRIES}); treating as transient and retrying\"\n        )\n        time.sleep(1)  # small delay between tries; consider config if needed\n\n    # After 3 consecutive empty arrays, treat as fatal ADB/glogin readiness failure\n    raise GloginReadinessError(\n        \"glogin readiness failed: empty array returned 3 times; \"\n        \"treating as ADB-readiness failure and aborting before Appium\"\n    )\n```\n\n- Replace any direct `glogin(...)` usage in `post_reel_smart.py` with `run_glogin_with_retries(...)`.\n- Ensure the raised `GloginReadinessError` is **not swallowed**; it should cause the current job to fail and prevent Appium startup.\n\n6. **Change all warning-and-continue patterns to hard failures**\n- Search `post_reel_smart.py` (and closely related modules, if needed) for patterns like:\n  - `logger.warning(...);` followed by continuing the flow when:\n    - `adb devices` output does not contain the target device.\n    - Device is `offline`.\n    - glogin returns `[]` or similarly unusable results.\n    - Appium/ADB connection pre-checks fail but code currently falls back to “best effort” behavior.\n- For each relevant case **before Appium connection**:\n  - Replace the warning-and-continue branch with a raised exception (`AdbReadinessError` or `GloginReadinessError`, or a shared `DeviceReadinessError` base class).\n  - Keep logging at **ERROR** level before raising for observability.\n\nExample refactor:\n\n```python\nif not devices:\n    logger.error(\"No ADB devices detected for target; failing fast\")\n    raise AdbReadinessError(\"No ADB devices detected\")\n\nif status == \"offline\":\n    logger.error(f\"ADB device {adb_serial} is offline; failing fast\")\n    raise AdbReadinessError(f\"Device {adb_serial} offline\")\n```\n\n7. **Ensure orchestration honors hard failures**\n- Verify that higher-level orchestration (e.g., main loop from Task 9 / scheduler from Task 21) already treats raised exceptions as job failures and logs them appropriately, rather than swallowing them and continuing silently.\n- Confirm behavior aligns with existing error-handling patterns:\n  - Job should be marked `fail`/`error` with a clear message like `adb_readiness_failed` or `glogin_readiness_failed`.\n  - Scheduler should **not retry** indefinitely unless explicitly configured to do so elsewhere.\n\n8. **Best practices and robustness considerations**\n- **Avoid `adb wait-for-device` shell calls** in favor of parsing `adb devices` output in Python for full control and better logging, consistent with common Python ADB wrappers that poll or reconnect in loops.[1][3][5]\n- Use **bounded waits** with clear timeouts to prevent hangs in CI or production environments.\n- Prefer **dedicated exception types** (as above) for readiness failures so that they can be distinguished from generic runtime errors in logs and potentially used for intelligent retry policies.\n- Ensure logs clearly record:\n  - Time spent in the wait loop.\n  - Final failure reason (timeout, offline status, missing device, repeated empty glogin).\n\n9. **Documentation and configuration notes**\n- Update any developer/ops documentation to describe:\n  - The new `ADB_MAX_WAIT_SECONDS` and `ADB_POLL_INTERVAL_SECONDS` settings and recommended defaults.\n  - That ADB/glogin readiness failures are now **hard stops** for the job, not soft warnings.\n  - How these changes interact with existing Appium stability improvements from Task 18 and environment configuration from Task 16.\n",
        "testStrategy": "1. **Unit Tests for `wait_for_adb_device_ready`**\n- Mock `subprocess.run` to simulate varying `adb devices` outputs without requiring a real device.\n\n  - **Test: Immediate success**\n    - First call to `adb devices` returns:\n      - Header line\n      - `\"10.0.0.5:5555\\tdevice\"`\n    - Expect `wait_for_adb_device_ready` to return without sleeping and not raise.\n\n  - **Test: Offline then ready**\n    - First N calls return `\"10.0.0.5:5555\\toffline\"`, final call returns `\"10.0.0.5:5555\\tdevice\"`.\n    - Confirm that function logs warnings, loops, and eventually returns successfully before timeout.\n\n  - **Test: Never appears (timeout)**\n    - All calls return header-only output or contain only a different device serial.\n    - Configure `max_wait_seconds` small (e.g., 5) and `poll_interval_seconds` > 0.\n    - Assert that `AdbReadinessError` is raised and that the number of `subprocess.run` calls matches expected iterations.\n\n  - **Test: Always offline**\n    - All polls return `\"10.0.0.5:5555\\toffline\"`.\n    - Assert timeout and `AdbReadinessError` with a message mentioning `offline`.\n\n  - **Test: Malformed lines ignored**\n    - Output includes malformed lines (no tab, blank, etc.) plus a correct `ip:port\\tdevice` line.\n    - Verify parsing logic ignores malformed lines but still succeeds on the valid one.\n\n  - **Test: Non-zero adb exit codes**\n    - Simulate `subprocess.run` returning `returncode != 0` and stderr content.\n    - Ensure function treats this as transient (empty output) and eventually raises on timeout.\n\n2. **Unit Tests for glogin wrapper**\n- Use monkeypatch/mocking to replace `glogin` with a fake implementation.\n\n  - **Test: Success on first call**\n    - Fake `glogin` returns a non-empty list immediately.\n    - Expect `run_glogin_with_retries` to return that list and not sleep or log warnings.\n\n  - **Test: Empty then success**\n    - Fake `glogin` returns `[]` on first call, valid list on second.\n    - Expect a single warning log and successful return.\n\n  - **Test: Always empty**\n    - Fake `glogin` returns `[]` on all calls.\n    - Expect exactly 3 invocations, then `GloginReadinessError` raised.\n\n  - **Test: Propagation of unexpected errors**\n    - Fake `glogin` raises an unexpected exception on first call.\n    - Ensure `run_glogin_with_retries` does not swallow it; it should propagate unchanged.\n\n3. **Integration Tests in `post_reel_smart` flow**\n- Use a high-level orchestrator test (similar style to Task 9 and Task 18 tests), with mocks for:\n  - Geelark device controller (reports ready and provides `ip:port`).\n  - `subprocess.run` for `adb devices`.\n  - glogin.\n  - Appium connection methods.\n\n  - **Test: Appium not called on ADB readiness failure**\n    - Simulate `adb devices` never reporting the target `ip:port`.\n    - Run a single job through the `post_reel_smart` orchestration.\n    - Assert:\n      - Job is marked as failed (per Task 9 behavior).\n      - Appium connection function is **never** called.\n      - Logs contain the `AdbReadinessError` message.\n\n  - **Test: Appium not called on repeated glogin empty result**\n    - Simulate `wait_for_adb_device_ready` succeeding.\n    - Fake `glogin` returns `[]` three times.\n    - Assert:\n      - `GloginReadinessError` is raised and propagated.\n      - Appium connection is not invoked.\n\n  - **Test: Normal success path**\n    - `wait_for_adb_device_ready` sees `device` status after one or two polls.\n    - `glogin` returns non-empty result.\n    - Appium connection is invoked, and the job completes successfully.\n\n4. **Regression Tests for warning-to-exception refactors**\n- Add tests that previously expected warning-only behavior (if any existed) or create new ones to assert:\n  - When ADB device is missing or offline, the code now raises `AdbReadinessError`.\n  - When glogin is repeatedly empty, `GloginReadinessError` is raised.\n- Ensure these exceptions are surfaced in logs via the existing structured logging from Task 9.\n\n5. **Manual / environment tests**\n- On a real or staging environment with a Geelark device:\n  - **Case: Device ready**\n    - Trigger a posting job and confirm that:\n      - Logs show the ADB wait loop and eventual success.\n      - Appium session starts and completes as before.\n\n  - **Case: ADB disabled or wrong `ip:port`**\n    - Misconfigure ADB endpoint so the target serial never appears.\n    - Confirm that the job fails fast after `ADB_MAX_WAIT_SECONDS`, with clear log messages and no Appium attempts.\n\n  - **Case: glogin issues**\n    - Temporarily configure glogin backend or credentials to produce empty-array responses.\n    - Confirm that after three attempts, the job fails with the correct error message and no Appium session is started.\n",
        "status": "done",
        "dependencies": [
          "9",
          "16",
          "18"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-12T11:12:45.907Z"
      },
      {
        "id": "28",
        "title": "Make Appium URL Configurable in SmartInstagramPoster",
        "description": "Replace hardcoded APPIUM_SERVER global with a configurable appium_url parameter in SmartInstagramPoster's __init__ method, using self.appium_url in connect_appium() with default fallback to enable lane-based multi-server architecture.",
        "details": "## Implementation Details\n\n### 1. Locate and Document Current Hardcoded Usage (~post_reel_smart.py lines 730+ from Task 13/18)\n- Find `APPIUM_SERVER` global definition (likely `APPIUM_SERVER = 'http://127.0.0.1:4723'` or similar).\n- Identify all usages in `SmartInstagramPoster.connect_appium()` where it's passed as `command_executor` to `webdriver.Remote()`.\n\n### 2. Add appium_url Parameter to SmartInstagramPoster.__init__() (Best Practice: Appium Python Client v4)[1]\n```python\nclass SmartInstagramPoster:\n    def __init__(self, appium_url: str = None, *args, **kwargs):\n        \"\"\"\n        Initialize with optional Appium server URL for lane-based architecture.\n        Defaults to existing APPIUM_SERVER global if not provided.\n        \"\"\"\n        global APPIUM_SERVER\n        self.appium_url = appium_url or APPIUM_SERVER\n        # ... existing init code\n```\n\n### 3. Update connect_appium() to Use self.appium_url with AppiumClientConfig (Modern Pattern)[1]\nReplace hardcoded global usage:\n```python\nfrom appium.webdriver.client_config import AppiumClientConfig\n\n    def connect_appium(self, retries=3):\n        \"\"\"Connect Appium driver using configurable self.appium_url\"\"\"\n        client_config = AppiumClientConfig(\n            remote_server_addr=self.appium_url,  # e.g. 'http://lane1:4723'\n            direct_connection=True,\n            keep_alive=False,\n            ignore_certificates=True  # or strict_ssl=False for older style\n        )\n        \n        driver = webdriver.Remote(\n            command_executor=None,  # Use client_config instead\n            options=self.options,   # Existing UiAutomator2Options\n            client_config=client_config\n        )\n        # ... rest of existing connection logic with retries\n```\n\n### 4. Backward Compatibility & Default Behavior\n- Keep `APPIUM_SERVER` global as fallback for existing codepaths.\n- Log effective URL at connection: `logger.info(f\"Connecting to Appium at {self.appium_url}\")`.\n- Validate URL format: `if not self.appium_url.startswith('http'): raise ValueError(...)`.\n\n### 5. Update Caller Sites (posting_scheduler_parallel.py integration)\n- Pass lane-specific URLs when instantiating `SmartInstagramPoster`:\n```python\n# Example for lane-based usage\nposter = SmartInstagramPoster(appium_url=f'http://appium-lane-{lane_id}:4723')\n```\n\n### 6. Type Hints & Documentation\n- Add to class docstring: \"Supports multi-lane Appium servers via appium_url parameter.\"\n- Update `self.options` if needed for per-lane capabilities (systemPort, etc. from Task 26).\n\n## Best Practices Incorporated [1][5][7]\n- Use `AppiumClientConfig` (Appium Python Client v4+ preferred over legacy url param).\n- Default `http://127.0.0.1:4723` aligns with modern Appium server default.\n- `direct_connection=True` avoids Selenium proxy issues.\n- Graceful fallback preserves existing single-server behavior.",
        "testStrategy": "## Comprehensive Test Strategy\n\n### 1. Unit Tests for __init__ Parameter Handling\n```python\n# test_smartinstagram_poster.py\nclass TestSmartInstagramPosterConfig:\n    def test_default_appium_url(self):\n        global APPIUM_SERVER\n        APPIUM_SERVER = 'http://default:4723'\n        poster = SmartInstagramPoster()\n        assert poster.appium_url == 'http://default:4723'\n    \n    def test_explicit_appium_url(self):\n        poster = SmartInstagramPoster(appium_url='http://lane1:4723')\n        assert poster.appium_url == 'http://lane1:4723'\n```\n\n### 2. Integration Tests for connect_appium() (Mock Appium Server)\n- Use `pytest-mock` or `unittest.mock` to mock `webdriver.Remote`:\n```python\n    def test_connect_appium_uses_self_appium_url(mocker):\n        mocker.patch('appium.webdriver.remote.webdriver.WebDriver')\n        poster = SmartInstagramPoster(appium_url='http://test-lane:4725')\n        poster.connect_appium()\n        WebDriver.assert_called_once_with(\n            command_executor=None,\n            client_config=mocker.ANY,  # Verify AppiumClientConfig has correct remote_server_addr\n            options=poster.options\n        )\n```\n\n### 3. End-to-End Tests with Real/Mocked Appium Servers\n```bash\n# Test 1: Default URL (single server)\npytest test_post_reel.py::TestSmartPoster -k \"test_default_connection\" \\\n  --appium-url ''  # Should use global fallback\n\n# Test 2: Lane-specific URL\npytest test_post_reel.py::TestSmartPoster -k \"test_lane_connection\" \\\n  --appium-url 'http://localhost:4724'  # Different port\n```\n- Verify logs show correct URL: `grep \"Connecting to Appium at http://\"`.\n- Confirm driver connects successfully to specified server.\n\n### 4. Negative Tests\n- Invalid URL format → `ValueError` raised.\n- Missing protocol → `ValueError`.\n\n### 5. Scheduler Integration Test (Task 24 context)\n- Run `posting_scheduler_parallel.py --workers 2` with mocked lane URLs.\n- Verify each worker uses distinct `appium_url` without conflicts.",
        "status": "done",
        "dependencies": [
          "13",
          "18"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-12T11:13:41.092Z"
      },
      {
        "id": "29",
        "title": "Implement Pre-Appium Cleanup in post_reel_smart.py",
        "description": "Add conditional cleanup logic before connect_appium() to force-stop UiAutomator2 server, remove stale ADB port forwards, and quit existing Appium drivers only when system_port is set and device is valid, preventing port busy errors from failed sessions.",
        "details": "**Implementation Details**\n\n### 1. Locate connect_appium() Entry Point (~post_reel_smart.py lines 730+ from Task 13/28)\n- Identify the start of `SmartInstagramPoster.connect_appium()` method.\n- Add cleanup **immediately before** any Appium session creation logic (before `webdriver.Remote()` or capabilities setup).\n\n### 2. Implement `cleanup_appium_resources()` Method (New Private Method)\n```python\nasync def _cleanup_appium_resources(self):\n    \"\"\"Force cleanup UiAutomator2, ADB forwards, and old driver only when conditions met\"\"\"\n    if not hasattr(self, 'system_port') or not self.system_port:\n        logger.debug('Skipping Appium cleanup: system_port not set')\n        return\n    \n    if not hasattr(self, 'device') or not self.device:\n        logger.debug('Skipping Appium cleanup: device not valid')\n        return\n    \n    device_serial = self.device  # e.g. '10.0.0.5:5555'\n    system_port = self.system_port  # e.g. 8201\n    \n    logger.info(f'Performing Appium cleanup for {device_serial}:{system_port}')\n    \n    try:\n        # 1. Force-stop UiAutomator2 server on device [1][2]\n        cmd = ['adb', '-s', device_serial, 'shell', 'am', 'force-stop', 'io.appium.uiautomator2.server']\n        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n        if result.returncode == 0:\n            logger.info(f'Successfully force-stopped UiAutomator2 on {device_serial}')\n        else:\n            logger.warning(f'UiAutomator2 force-stop failed (code {result.returncode}): {result.stderr}')\n            \n        # 2. Remove stale ADB forward [Appium best practice]\n        cmd = ['adb', 'forward', '--remove', f'tcp:{system_port}']\n        result = subprocess.run(cmd, capture_output=True, text=True, timeout=5)\n        if result.returncode == 0:\n            logger.info(f'Removed ADB forward tcp:{system_port}')\n        else:\n            logger.debug(f'ADB forward tcp:{system_port} already removed or not found')\n            \n        # 3. Quit existing Appium driver if present\n        if hasattr(self, 'appium_driver') and self.appium_driver:\n            try:\n                self.appium_driver.quit()\n                logger.info('Quit existing appium_driver')\n            except Exception as e:\n                logger.warning(f'Failed to quit existing driver: {e}')\n            finally:\n                self.appium_driver = None\n                \n    except subprocess.TimeoutExpired:\n        logger.error(f'Appium cleanup timed out for {device_serial}')\n    except Exception as e:\n        logger.error(f'Appium cleanup failed for {device_serial}: {e}')\n```\n\n### 3. Integrate Cleanup into connect_appium()\n```python\nasync def connect_appium(self, retries=3):\n    # ADD THIS LINE AT THE VERY TOP:\n    await self._cleanup_appium_resources()\n    \n    # ... rest of existing connect_appium() logic\n```\n\n### 4. Update Class Docstring\nAdd to `SmartInstagramPoster.__doc__`:\n```\n- Pre-connection cleanup: Automatically force-stops UiAutomator2, removes stale ADB forwards, and quits old drivers\n```\n\n### 5. Error Handling Best Practices [Appium Python Client v4]\n- Use `async` if `connect_appium()` is async (matches Task 13 pattern)\n- 10s timeout on `am force-stop` prevents hangs [2][4]\n- Graceful handling of 'already removed' ADB forward responses\n- Defensive checks prevent cleanup on uninitialized state\n\n**Dependencies Context**: Builds directly on Task 26 (systemPort diagnosis), Task 28 (configurable URL), Task 13 (connect_appium() stability).",
        "testStrategy": "**Comprehensive Test Strategy**\n\n### 1. Unit Tests for `_cleanup_appium_resources()` (test_post_reel_smart.py)\n```python\nclass TestAppiumCleanup:\n    @patch('subprocess.run')\n    def test_cleanup_skipped_when_no_system_port(self, mock_run):\n        poster = SmartInstagramPoster()\n        poster._cleanup_appium_resources()\n        mock_run.assert_not_called()\n        \n    @patch('subprocess.run')\n    def test_cleanup_skipped_when_no_device(self, mock_run):\n        poster = SmartInstagramPoster()\n        poster.system_port = 8201\n        poster._cleanup_appium_resources()\n        mock_run.assert_not_called()\n        \n    @patch('subprocess.run')\n    def test_full_cleanup_sequence(self, mock_run):\n        poster = SmartInstagramPoster()\n        poster.system_port = 8201\n        poster.device = '10.0.0.5:5555'\n        poster.appium_driver = Mock()\n        \n        poster._cleanup_appium_resources()\n        \n        # Verify exact ADB commands in order\n        mock_run.assert_has_calls([\n            call(['adb', '-s', '10.0.0.5:5555', 'shell', 'am', 'force-stop', 'io.appium.uiautomator2.server'], ...),\n            call(['adb', 'forward', '--remove', 'tcp:8201'], ...)\n        ], any_order=False)\n        assert poster.appium_driver is None\n```\n\n### 2. Integration Tests (Real ADB/Device)\n1. **Stale Session Cleanup**:\n   - Start failed Appium session with `systemPort=8201` → verify port busy error\n   - Run `connect_appium()` → verify cleanup succeeds, new session starts\n   - Check: `adb forward --list` empty, `adb shell ps | grep uiautomator2` returns nothing\n\n2. **Fresh State (No Cleanup Needed)**:\n   - Cold start with valid `device`/`system_port` but no stale resources\n   - Verify cleanup runs but ADB commands are no-ops\n\n### 3. Negative Tests\n- Invalid device serial → cleanup logs warning but doesn't crash\n- `am force-stop` timeout → logs error, continues to next step\n- Missing `self.appium_driver` → no AttributeError\n\n### 4. Manual Verification\n```bash\n# After failed session, before/after cleanup:\nadb forward --list  # Should show tcp:8201 before, empty after\nadb -s 10.0.0.5:5555 shell ps | grep uiautomator2  # Should be gone\n```",
        "status": "done",
        "dependencies": [
          "13",
          "26",
          "28"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-12T11:14:23.050Z"
      },
      {
        "id": "30",
        "title": "Create LANES configuration structure for multi-Appium architecture",
        "description": "Introduce a centralized LANES configuration for multi-Appium execution, ensuring each lane has a unique Appium server URL and non-overlapping system_port range, plus helpers to resolve lane configs by name.",
        "details": "Implementation plan:\n\n1. **Define the LANES data model**\n- Use a clear, typed structure to describe each lane, e.g. a `@dataclass` or `TypedDict` for static checking:\n  ```python\n  from dataclasses import dataclass\n\n  @dataclass(frozen=True)\n  class LaneConfig:\n      name: str\n      appium_url: str  # e.g. \"http://127.0.0.1:4723\"\n      system_port_base: int  # e.g. 8200\n      system_port_count: int = 50  # optional, size of per-lane port window\n  ```\n- Store all lane configs in a top-level `LANES` mapping or list, e.g. in `lanes_config.py` or at the top of `posting_lane.py`:\n  ```python\n  LANES: dict[str, LaneConfig] = {\n      \"lane1\": LaneConfig(\n          name=\"lane1\",\n          appium_url=\"http://127.0.0.1:4723\",\n          system_port_base=8200,\n      ),\n      \"lane2\": LaneConfig(\n          name=\"lane2\",\n          appium_url=\"http://127.0.0.1:4725\",\n          system_port_base=8300,\n      ),\n  }\n  ```\n- Follow multi-device Appium best practices by giving each Appium server a unique host:port and non-overlapping `systemPort` ranges so sessions do not conflict.[1][4][6][7]\n\n2. **Configuration source and environment overrides**\n- Support both code-level defaults and environment/config-file overrides (12-factor style):\n  - Allow loading lane definitions from a JSON/YAML config (e.g. `lanes.json`) or a section inside the existing project config.\n  - For quick setups, keep the static `LANES` default but add a `load_lanes_config(config_path: str | None)` function that will:\n    - If `config_path` is provided and exists, load and validate it.\n    - Otherwise, return the built-in `LANES` mapping.\n  - Consider simple JSON schema, e.g.:\n    ```json\n    [\n      {\"name\": \"lane1\", \"appium_url\": \"http://127.0.0.1:4723\", \"system_port_base\": 8200},\n      {\"name\": \"lane2\", \"appium_url\": \"http://127.0.0.1:4725\", \"system_port_base\": 8300}\n    ]\n    ```\n\n3. **Validation rules (no overlap, well-formed URLs)**\n- Implement a `validate_lanes(lanes: dict[str, LaneConfig]) -> None` function that enforces:\n  - All lane names are unique.\n  - All `appium_url` values are unique (no two lanes share the same server URL).\n  - All `system_port_base` values are unique and, if `system_port_count` is set, that port ranges do not overlap, e.g. using intervals `[base, base + count)`.\n  - `system_port_base` is within a sane range (e.g. > 1024, < 65535) and `system_port_count` > 0.\n  - URLs parse successfully with `urllib.parse.urlparse` and scheme is `http` or `https` as expected.\n- Run validation once during module import or app startup and raise a clear configuration error if any rule fails.\n\n4. **Helper functions for lane lookup and systemPort allocation**\n- Add helpers in `posting_lane.py` (or a dedicated module) for downstream code:\n  ```python\n  class LaneNotFoundError(KeyError):\n      pass\n\n  def get_lane_config(name: str) -> LaneConfig:\n      try:\n          return LANES[name]\n      except KeyError as e:\n          raise LaneNotFoundError(f\"Unknown lane '{name}'. Available lanes: {list(LANES)}\") from e\n  ```\n- If the architecture needs per-session `systemPort` allocation within a lane (for multiple parallel sessions on one lane), add:\n  ```python\n  import itertools\n  from threading import Lock\n\n  _lane_port_state: dict[str, itertools.cycle] = {}\n  _lane_port_lock = Lock()\n\n  def init_lane_port_allocators(lanes: dict[str, LaneConfig]) -> None:\n      global _lane_port_state\n      with _lane_port_lock:\n          _lane_port_state = {\n              name: itertools.cycle(\n                  range(cfg.system_port_base, cfg.system_port_base + cfg.system_port_count)\n              )\n              for name, cfg in lanes.items()\n          }\n\n  def allocate_system_port(lane_name: str) -> int:\n      with _lane_port_lock:\n          try:\n              cycle_obj = _lane_port_state[lane_name]\n          except KeyError as e:\n              raise LaneNotFoundError(f\"Unknown lane '{lane_name}' for port allocation\") from e\n          return next(cycle_obj)\n  ```\n- Ensure allocation is thread-safe using a lock, as tests may run in parallel (e.g. pytest xdist or custom threading), which is standard practice in parallel Appium setups.[3][5][8]\n\n5. **Integrate with SmartInstagramPoster / Appium client**\n- Modify the code that instantiates `SmartInstagramPoster` (Task 28) or equivalent orchestrator so it:\n  - Accepts a `lane_name` parameter.\n  - Uses `get_lane_config(lane_name)` to obtain `appium_url` and `system_port_base` (or `allocate_system_port(lane_name)` for per-session `systemPort`).\n  - Passes the lane-specific `appium_url` into `SmartInstagramPoster.__init__`.\n  - Uses the chosen system port when building Appium capabilities (e.g. `\"systemPort\": selected_port`).\n- Keep lane selection orthogonal to CSV job parsing and main loop: the orchestrator should decide which lane to use per job (e.g. based on account sharding) and then rely on these helpers.\n\n6. **Error handling, logging, and observability**\n- On lane lookup failures, raise explicit custom exceptions and log at ERROR level with context (lane name requested, job/account info if available).\n- Log lane assignment at INFO level when starting a job: lane name, `appium_url`, and `systemPort` used. This aids debugging multi-device and systemPort-busy issues addressed in related tasks.\n- Consider exposing a small debug helper like `list_lanes()` that returns a human-readable summary for diagnostics and logging at startup.\n\n7. **Documentation and examples**\n- Add docstrings to all public helpers explaining:\n  - What a \"lane\" represents (Appium server + port window).\n  - How to add a new lane safely (unique `appium_url`, non-overlapping port base).\n- Provide a short usage example in comments or project docs:\n  ```python\n  lane = get_lane_config(\"lane1\")\n  system_port = allocate_system_port(lane.name)\n  poster = SmartInstagramPoster(appium_url=lane.appium_url, system_port=system_port)\n  ```",
        "testStrategy": "1. **Unit tests for configuration parsing and validation**\n- Test that the default in-code `LANES` mapping passes `validate_lanes` without errors.\n- Test that two lanes sharing the same `appium_url` cause `validate_lanes` to raise a clear exception.\n- Test that two lanes whose `[system_port_base, base+count)` ranges overlap cause validation to fail.\n- Test that invalid URLs (missing scheme, malformed host) are rejected.\n\n2. **Unit tests for helper functions**\n- `get_lane_config`:\n  - Returns the correct `LaneConfig` when an existing lane name is provided.\n  - Raises `LaneNotFoundError` with a helpful message when the lane does not exist.\n- `init_lane_port_allocators` and `allocate_system_port`:\n  - After initialization, repeated calls for the same lane yield ports within the configured range.\n  - When `system_port_count` is small (e.g. 2), calling `allocate_system_port` more times cycles through allowed ports without leaving the range.\n  - Attempting to allocate a port for an unknown lane raises `LaneNotFoundError`.\n  - Under simple concurrent access (e.g. using `concurrent.futures.ThreadPoolExecutor`), ensure returned ports are all within the lane’s range and no exceptions are raised due to race conditions.\n\n3. **Integration tests with SmartInstagramPoster configuration (non-networked/mocked)**\n- Using a mock or fake `SmartInstagramPoster` and Appium client, verify that:\n  - Given a selected `lane_name`, the orchestrator passes the lane’s `appium_url` into `SmartInstagramPoster.__init__`.\n  - The `systemPort` capability used when building desired capabilities equals the value returned from `allocate_system_port`.\n- Ensure that changing the lane name results in a different `appium_url` and system port range in the constructed capabilities.\n\n4. **Configuration file loading tests**\n- If a file-based config loader is implemented, test:\n  - Loading a valid JSON/YAML lanes file yields the expected `LANES` mapping and passes validation.\n  - A file with duplicate lane names or overlapping port ranges fails with a clear error message.\n\n5. **Logging and error-path tests**\n- Use log capturing (e.g. `caplog` in pytest) to assert that:\n  - A lane lookup failure logs an ERROR entry with the missing lane name.\n  - Successful lane selection logs an INFO entry including lane name and `appium_url`.\n- Optionally, simulate a misconfigured lanes file in an integration test and assert that the application fails fast at startup with an appropriate error rather than continuing with a bad config.",
        "status": "done",
        "dependencies": [
          "25",
          "26",
          "27",
          "28",
          "29"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-12T11:15:09.680Z"
      },
      {
        "id": "31",
        "title": "Create single-lane posting CLI script posting_lane.py",
        "description": "Implement posting_lane.py as a single-lane CLI entrypoint that runs a dedicated worker loop for one lane, wiring lane-specific Appium configuration, strict ADB gating, and per-lane state/logging with no interaction with the parallel scheduler state.",
        "details": "Implementation plan:\n\n1. **Overall goals and architecture**\n- Create a new top-level script/module **`posting_lane.py`** that acts as a **single-lane worker**: it repeatedly pulls the next account/video job and runs the existing SmartInstagramPoster flow for that lane.\n- Ensure that each lane uses its **own Appium URL and systemPort range**, obtained from the LANES configuration by lane name (from Task 30).\n- Integrate the **strict ADB readiness gate and hard-fail behavior** from Task 27 so each job only proceeds when its device is truly ready.\n- Guarantee **no reads or writes** to `scheduler_state_parallel.json`; instead use a per-lane state file (e.g. `scheduler_state_<lane>.json`) or rely solely on CSV logs.\n- Design the script to be **CLI-friendly**, using Python’s `argparse` as recommended best practice for robust command-line interfaces.[1][2][6]\n\n2. **CLI design and argparse implementation**\n- Use `argparse.ArgumentParser` to define the required arguments following best practices (clear help messages, types, and validation).[2][4][5]\n- Required options:\n  - `--lane-name` (str): logical lane name, used to look up lane configuration.\n  - `--appium-url` (str, optional override): if provided, overrides the lane’s configured Appium URL.\n  - `--system-port-base` (int, optional override): if provided, overrides the lane’s configured base system_port for this lane/worker.\n  - `--accounts-file` (path-like str): CSV or JSON describing accounts/jobs, or a pointer to the input CSV used by existing CSV utilities (Task 2). Clarify expected format in help.\n  - `--add-folder` (path-like str, optional): additional folder path (e.g. for videos or assets) to be merged into existing configuration.\n- Recommended flags (if not already globally configured):\n  - `--log-path` (optional): explicit log CSV path; if omitted, derive from lane name (e.g. `logs/posting_<lane>.csv`).\n  - `--state-file` (optional): explicit state JSON path; default to `scheduler_state_<lane>.json` in a configurable directory.\n- Follow best practices:\n  - Provide a concise description for the parser and `--help` strings for each argument.[4]\n  - Use **types** (`type=int`, `type=pathlib.Path`) to validate inputs and fail early on bad arguments.[2][4]\n  - Optionally use environment variables or config defaults but ensure CLI always wins.\n\n3. **Lane configuration resolution and wiring**\n- Import the LANES config helpers from Task 30 (e.g. `get_lane_config(lane_name: str) -> LaneConfig`).\n- Steps in `main()` after parsing args:\n  1. Resolve `lane = get_lane_config(args.lane_name)`.\n  2. Compute **effective Appium URL** and **system_port_base**:\n     - `appium_url = args.appium_url or lane.appium_url`.\n     - `system_port_base = args.system_port_base or lane.system_port_base`.\n  3. Optionally calculate a **per-account systemPort** from `system_port_base` if multiple devices/jobs might reuse the lane sequentially; ensure no overlap within lane’s designated range.\n- Provide defensive checks:\n  - Validate `system_port_base` is within the lane’s allowed window from Task 30.\n  - Log effective configuration to structured logs.\n\n4. **Job source and worker loop design**\n- Decide on the job abstraction, ideally reusing the existing CSV parsing utilities from Task 2 (`read_jobs`, `PostJob` dataclass).\n- Implement a **job provider** for this script:\n  - For v1, simplest approach: read **all jobs** at startup from `--accounts-file` (likely a CSV of jobs) via `read_jobs()`, then iterate.\n  - Optionally, combine account information with job rows if accounts-file is separate from job CSV.\n- Worker loop pattern (best practice for long-running CLIs):\n  ```python\n  def run_lane_worker(config: LaneRuntimeConfig) -> None:\n      jobs = load_jobs(config.accounts_file, config.add_folder, lane=config.lane_name)\n      for job in jobs:\n          process_single_job(job, config)\n  ```\n- For future extensibility, design `run_lane_worker` so that it could:\n  - Re-scan for new jobs periodically.\n  - Respect a `--max-jobs` or `--once` flag, if needed later.\n\n5. **Strict ADB gating and SmartInstagramPoster integration**\n- Import and reuse the **strict ADB readiness** and **hard-fail** utilities introduced in Task 27 (e.g. `wait_for_adb_device_ready(udid, timeout)`), using them in this new worker loop **before any Appium session is created**.\n- Define a function `run_post_job_with_adb_gate(job, lane_runtime_cfg)`:\n  1. Derive the device identifier (e.g. ip:port) from account, lane, or configuration.\n  2. Call the strict ADB gate:\n     - `wait_for_adb_device_ready(device_udid, timeout=...);` raise an exception if not ready.\n  3. Build Appium capabilities including **systemPort** computed from `system_port_base`.\n  4. Instantiate `SmartInstagramPoster` (or equivalent orchestrator) with:\n     - `appium_url=appium_url` from the lane runtime config.\n     - `system_port=system_port` derived from the base.\n     - Any other required dependencies (device controller, navigator, etc.) coming from previously completed tasks (3, 6, 9, 18, 27).\n  5. Run the post flow (e.g. `poster.run(job)`), catching and logging exceptions.\n  6. On hard failures (repeated empty glogin, persistent ADB issues), propagate the exception per Task 27 design to avoid silently continuing on a bad device.\n- Ensure this code path **does not access** the parallel scheduler state API; use only its own state and logs.\n\n6. **Per-lane state and logging, with no cross-talk**\n- **State file strategy**:\n  - If state persistence is desired, create a simple JSON-based `LaneStateStore` responsible for:\n    - Loading from `scheduler_state_<lane>.json` at startup (if file exists).\n    - Tracking per-job status (PENDING, RUNNING, SUCCESS, FAIL) and any metadata.\n    - Persisting changes atomically (write to temp file + rename) to avoid partial writes, adopting patterns from Task 20’s robust persistence design.\n  - Alternatively support a `--no-state-file` mode where only CSV logs are used and no JSON state is written; in this case, `LaneStateStore` can be a no-op implementation.\n- **Critical rule**: never open, read, or write `scheduler_state_parallel.json`; guard against accidental usage by:\n  - Not importing parallel scheduler modules into `posting_lane.py`.\n  - Naming state store classes distinctly (`LaneStateStore` vs `ParallelSchedulerStateStore`).\n- **Logging**:\n  - Reuse `append_log_row` from Task 2 to log each job’s outcome to a **per-lane log CSV**.\n  - Use the standard `logging` module with a lane-specific logger name (e.g. `posting_lane.<lane>`), configured to log JSON-like lines including lane name, job id, account, and device id.\n  - Log:\n    - Start/finish of each job.\n    - ADB gate results.\n    - Appium connection details (URL, systemPort) without secrets.\n\n7. **Robustness and lifecycle management**\n- Error handling:\n  - For per-job failures, catch exceptions, log failure, update lane state, and continue to next job by default.\n  - For **fatal errors** (e.g. ADB never becomes ready, unrecoverable Appium port issues flagged by Tasks 26/18/27 semantics), log at ERROR level and exit with non-zero status.\n- Graceful shutdown:\n  - Handle SIGINT/SIGTERM by stopping the worker after the current job, flushing logs/state, and exiting cleanly.\n  - Encapsulate the main loop in a `try/finally` to ensure final state persistence.\n\n8. **Implementation best practices**\n- Structure `posting_lane.py` with a clear `if __name__ == \"__main__\": main()` entrypoint.\n- Use type hints for all functions and dataclasses for configuration objects, enabling static checking.\n- Keep lane-specific constants (state file naming, log path derivation) in small helper functions for reuse and testability.\n- Write small, testable functions: `parse_args()`, `build_lane_runtime_config()`, `run_lane_worker()`, `run_post_job_with_adb_gate()`.\n\n",
        "testStrategy": "1. **Unit tests for CLI argument parsing**\n- Test that missing required args (e.g. `--lane-name` or `--accounts-file`) cause `argparse` to exit with an error.\n- Test that valid invocations with and without overrides (`--appium-url`, `--system-port-base`, `--add-folder`) produce the expected runtime config values.\n- Test type validation: non-integer `--system-port-base` should fail parsing.\n\n2. **Unit tests for lane configuration resolution**\n- Mock the LANES config accessor from Task 30 to return a known `LaneConfig`.\n- Verify that when no overrides are given, the effective `appium_url` and `system_port_base` match the lane config.\n- Verify that when overrides are supplied, they replace the lane config values.\n- Test that invalid lane names raise a clear, user-facing error.\n\n3. **Unit tests for per-lane state and logging behavior**\n- With a temporary directory, run a small in-process worker against 2–3 fake jobs using a mock SmartInstagramPoster that always succeeds.\n- Assert that:\n  - A per-lane log CSV is created with one row per job, including status and timestamps.\n  - A per-lane state file `scheduler_state_<lane>.json` is created (unless `--no-state-file` equivalent is used) and reflects final job statuses.\n- Verify that running the worker twice appends or updates state correctly without corrupting the file (e.g. simulate partial failures).\n- Confirm no file named `scheduler_state_parallel.json` is created or touched during test runs.\n\n4. **Unit tests for strict ADB gating integration**\n- Mock the strict ADB gate function from Task 27 (`wait_for_adb_device_ready`) to simulate:\n  - Immediate readiness: ensure the post flow is invoked.\n  - Delayed readiness: ensure multiple checks occur before proceeding (if observable from logs or call counts).\n  - Timeout/failure: ensure the job is marked as failed, error is logged, and behavior matches Task 27’s hard-fail semantics.\n\n5. **Unit tests for SmartInstagramPoster wiring**\n- Replace SmartInstagramPoster with a test double that records the parameters it was constructed with and when `run()` is called.\n- Assert that for a given lane and overrides, the poster is instantiated with the correct `appium_url` and `system_port`.\n- Test that exceptions raised by the poster propagate to the worker loop as job failures but do not crash the entire script unless configured as fatal.\n\n6. **Integration / smoke tests**\n- In a development environment with a single test device:\n  - Start a dedicated Appium server for a lane with a known URL and systemPort base.\n  - Run `posting_lane.py` with that lane and a small job CSV (1–2 jobs) and observe:\n    - ADB gate logs showing the device reaching `device` status before Appium connects.\n    - Successful completion of jobs with expected posts on the test account.\n  - Intentionally misconfigure the systemPort (e.g. overlapping with a busy port) and confirm that errors are clearly logged and the script exits or marks jobs failed in line with the failure policy and the cleanup rules from Tasks 18/26/27.\n\n7. **Graceful shutdown tests**\n- Start `posting_lane.py` on a longer job list; send SIGINT during a job (if test harness allows signals).\n- Verify that:\n  - The current job’s status is updated appropriately (success or fail depending on the interruption point).\n  - The per-lane state and log files are flushed and not corrupted.\n  - The process exits with a non-zero code if interrupted mid-job, or zero if allowed to finish cleanly.\n",
        "status": "done",
        "dependencies": [
          "2",
          "3",
          "6",
          "9",
          "18",
          "20",
          "26",
          "27",
          "30"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-12T11:17:51.496Z"
      },
      {
        "id": "32",
        "title": "Clean Up posting_scheduler_parallel.py Dangerous Behaviors for Lane-Based Architecture",
        "description": "Refactor posting_scheduler_parallel.py to eliminate unsafe state loading, implement true fresh starts, fix Appium error handling with bounded health checks, and deprecate shared Appium multi-worker mode in favor of lane isolation.",
        "details": "Implement the five specified cleanups following Appium best practices for parallel execution stability and resource isolation[1][2].\n\n### 1. **NEVER Load num_workers from scheduler_state_parallel.json - Use CLI --workers Only**\n- Locate state loading logic (~lines where `scheduler_state_parallel.json` is read).\n- Remove **all** `num_workers` restoration from JSON; hard-fail if CLI `--workers` missing.\n```python\n# Replace any state['num_workers'] loading with:\nparser = argparse.ArgumentParser()\nparser.add_argument('--workers', type=int, required=True, help='Number of parallel workers')\nargs = parser.parse_args()\nnum_workers = args.workers  # CLI only, no state fallback\n```\n\n### 2. **True Fresh State with --fresh-state Flag**\n- Add `--fresh-state` CLI flag.\n- When present: **delete** `scheduler_state_parallel.json` + `job_state_parallel.json`, skip **all** job/settings restoration, start with empty queues/settings.\n```python\nimport os\nif args.fresh_state:\n    for state_file in ['scheduler_state_parallel.json', 'job_state_parallel.json']:\n        if os.path.exists(state_file):\n            os.unlink(state_file)\n    logger.info('Fresh state: all state files deleted, starting empty')\n    # Skip restoration blocks entirely\n```\n\n### 3. **Appium Connectivity Errors: Single Failure, No Tight Retries**\n- Replace all tight retry loops (e.g., `while True: try: connect()`) with **single attempt + exponential backoff** (max 1 retry, 30s total).\n- Use `tenacity` library or manual backoff; log failure and **queue job for later** instead of infinite retry.\n```python\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n@retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=4, max=16))\ndef connect_appium_with_backoff(self):\n    return self.connect_appium()\n```\n\n### 4. **Implement check_appium_health() with Bounded HTTP Timeout**\n- Create `check_appium_health(appium_url: str, timeout: float = 5.0) -> bool`.\n- Use `requests.get(f'{appium_url}/status', timeout=timeout)`; return `True` only if `status['ready'] == true`.\n```python\nimport requests\nfrom typing import Optional\n\ndef check_appium_health(appium_url: str, timeout: float = 5.0) -> bool:\n    try:\n        resp = requests.get(f'{appium_url}/status', timeout=timeout)\n        if resp.status_code == 200:\n            data = resp.json()\n            return data.get('value', {}).get('ready', False)\n    except requests.RequestException:\n        pass\n    return False\n```\n- Call before every session; fail fast if unhealthy.\n\n### 5. **Deprecate Multi-Worker Shared Appium Mode**\n- Add `DEPRECATION_WARNING = 'Multi-worker shared Appium mode deprecated; use posting_lane.py per-lane workers'`.\n- When detecting shared Appium (same `appium_url` for multiple workers): log warning + **reduce to 1 worker**.\n- Print startup banner directing to Task 31 lane-based CLI: `python posting_lane.py --lane-name lane1`.\n- **Remove shared mode code paths** once lane testing confirms stability (post-Task 31).\n\n### Additional Best Practices\n- Integrate `LaneConfig` from Task 30 for per-lane Appium URLs/systemPorts.\n- Ensure cleanup calls Task 29 `_cleanup_appium_resources()` before health checks.\n- Log **all** Appium URLs used + health status at DEBUG level.",
        "testStrategy": "### Comprehensive Test Strategy\n\n#### 1. **CLI Argument & Fresh State Tests** (test_posting_scheduler_parallel.py)\n```python\nclass TestFreshStateAndWorkers:\n    @patch('argparse.ArgumentParser')\n    def test_workers_cli_only_no_state_fallback(self, mock_parser):\n        # Verify num_workers ONLY from CLI, crashes without --workers\n        with pytest.raises(SystemExit):\n            run_scheduler(['--other-arg'])  # missing --workers\n\n    def test_fresh_state_deletes_files(self, tmp_path):\n        state_file = tmp_path / 'scheduler_state_parallel.json'\n        state_file.write_text('{\"num_workers\": 99}')\n        run_scheduler(['--fresh-state', '--workers', '1', f'--state-dir={tmp_path}'])\n        assert not state_file.exists()  # deleted\n        # Verify starts with empty queues\n```\n\n#### 2. **Appium Health Check Unit Tests**\n```python\nclass TestAppiumHealth:\n    @patch('requests.get')\n    def test_healthy_response(self, mock_get):\n        mock_get.return_value.json.return_value = {'value': {'ready': True}}\n        assert check_appium_health('http://127.0.0.1:4723') == True\n\n    @patch('requests.get')\n    def test_unhealthy_timeout(self, mock_get):\n        mock_get.side_effect = requests.Timeout\n        assert check_appium_health('http://127.0.0.1:4723') == False\n```\n\n#### 3. **Integration Tests (Docker Compose + Real Appium)**\n```bash\n# Test 1: Fresh state + single failure Appium\nmkdir test-state\necho '{\"jobs\":[]}' > test-state/scheduler_state_parallel.json\ndocker-compose up appium  # healthy server\npython posting_scheduler_parallel.py --fresh-state --workers 1 --appium-url http://localhost:4723\n# Verify: state dir empty after, no infinite retries\n\n# Test 2: Appium unhealthy → single fail + job requeue\npython posting_scheduler_parallel.py --workers 1 --appium-url http://localhost:9999  # bad port\n# Verify: logs show 1 attempt + backoff, job requeued, no tight loop\n```\n\n#### 4. **Deprecation Warning Tests**\n```python\n    def test_shared_appium_deprecation_warning(self, caplog):\n        run_scheduler(['--workers', '3', '--appium-url', 'http://shared:4723'])\n        assert 'Multi-worker shared Appium mode deprecated' in caplog.text\n        assert 'use posting_lane.py per-lane workers' in caplog.text\n```\n\n#### 5. **End-to-End Lane Migration Test**\n- Run scheduler with `--lane-name lane1` (integrates Task 30).\n- Verify uses correct `LaneConfig.appium_url` + health check passes.\n- Confirm no state contamination between lanes.",
        "status": "done",
        "dependencies": [
          "28",
          "29",
          "30",
          "31"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-12T11:19:19.112Z"
      },
      {
        "id": "33",
        "title": "Add Multi-Lane Appium Startup Helper and Documentation for process_tracker.py",
        "description": "Extend process_tracker.py and project docs to support starting multiple isolated Appium servers on different ports and launching matching single-lane posting processes, with clear examples for N-phone parallel runs using the recommended 1-phone-per-lane architecture.",
        "details": "### Goals\n- Add **first-class support** in `process_tracker.py` (or a adjacent helper) for starting multiple Appium server processes on specified ports (e.g. 4723, 4725, 4727), aligned with the LANES configuration.\n- Provide **documented workflows** (README section and/or helper script) that show how to:\n  1) Start 2–3 Appium instances via process_tracker on different ports.\n  2) Start 2–3 lane processes, each pointing at its own `appium_url` and `system_port_base`.\n  3) Run N phones in parallel without cross-talk, using **one device per lane per Appium server** as the recommended architecture.\n- Follow Appium best practices for **multi-port / multi-instance setups** and **per-device isolation**.[2][5][7]\n\n### Implementation Plan\n\n1. **Analyze Existing process_tracker.py Capabilities**\n- Inspect `process_tracker.py` to understand:\n  - How it currently defines and launches named processes (e.g. via a config dict or CLI arguments).\n  - How it handles environment variables, logging, PID tracking, and graceful shutdown.\n- Confirm that the environment already benefits from:\n  - Stable Android SDK environment (`ANDROID_HOME` / `ANDROID_SDK_ROOT`) from Task 16.\n  - Improved Appium connection, device readiness, and cleanup logic from Tasks 18, 26, 28, 29.\n  - Lane-based architecture and per-lane Appium URLs/system ports from Tasks 30 and 31.\n\n2. **Design a Multi-Appium Launch Interface**\n- Add a **declarative configuration** (e.g. Python dict or small YAML/JSON file read by process_tracker) describing the Appium instances to launch, for example:\n  ```python\n  APPIUM_INSTANCES = [\n      {\"name\": \"appium_4723\", \"port\": 4723, \"bootstrap_port\": 5723, \"log_suffix\": \"4723\"},\n      {\"name\": \"appium_4725\", \"port\": 4725, \"bootstrap_port\": 5725, \"log_suffix\": \"4725\"},\n      {\"name\": \"appium_4727\", \"port\": 4727, \"bootstrap_port\": 5727, \"log_suffix\": \"4727\"},\n  ]\n  ```\n- Use recommended flags from Appium multi-instance discussions:\n  - `-p <port>` for the main server port.[2][5]\n  - `-bp <bootstrap_port>` and distinct `--chromedriver-port` (if needed) per instance to avoid clashes on Android/UIAutomator2.[5]\n- Build Appium command lines like:\n  ```bash\n  appium -p 4723 -bp 5723 --log-level info\n  appium -p 4725 -bp 5725 --log-level info\n  appium -p 4727 -bp 5727 --log-level info\n  ```\n\n3. **Extend process_tracker.py to Launch Multiple Appium Servers**\n- Implement a helper in `process_tracker.py`, e.g. `start_appium_instance(name, port, bootstrap_port, extra_args=None)` that:\n  - Spawns the Appium server as a child process using the existing tracking abstraction.\n  - Writes logs to per-instance files (`logs/appium_<port>.log`).\n  - Exposes status (running/stopped, PID) and supports graceful shutdown.\n- Add a higher-level command (subcommand or function), e.g. `start_multi_appium(num_instances=3)` that:\n  - Iterates over the first N entries in `APPIUM_INSTANCES`.\n  - Starts each instance, checking that requested ports are not already in use (best practice to avoid intermittent failures).\n  - Optionally waits for health checks (`/status` HTTP check) to pass before returning; this is aligned with ensuring server readiness before client sessions.\n- Ensure process_tracker keeps **per-instance isolation**:\n  - No reuse of the same port or bootstrap port.\n  - Distinct log files and names for easy troubleshooting.\n\n4. **Integrate with LANES and posting_lane.py**\n- Make sure the lane configuration (Task 30) defines for each lane:\n  - `appium_url` (e.g. `http://127.0.0.1:4723`, `http://127.0.0.1:4725`, `http://127.0.0.1:4727`).\n  - `system_port_base` and non-overlapping ranges per lane.\n- Confirm that `posting_lane.py` (Task 31):\n  - Accepts a `--lane-name` argument.\n  - Resolves the lane configuration and passes `appium_url` and `system_port_base` into `SmartInstagramPoster`, which was made configurable in Task 28.\n- Optionally add convenience helpers in process_tracker:\n  - `start_lane(name)` that shells out to `posting_lane.py --lane-name <name> ...` with recommended defaults.\n  - `start_all_lanes()` that launches a set of lanes matching the available Appium instances.\n\n5. **Create a Multi-Lane Startup Helper Script (Optional but Recommended)**\n- Add a small top-level script (e.g. `multi_lane_helper.py` or `scripts/start_multi_lane.py`) that:\n  - Uses process_tracker’s API to:\n    1) Start N Appium instances on their configured ports.\n    2) Start N corresponding lanes (1 lane per Appium).\n  - Accepts CLI arguments such as:\n    - `--lanes lane1,lane2,lane3`\n    - `--start-appium` / `--skip-appium`\n    - `--dry-run` to only print commands.\n- Implement logging that prints clear, copy-pastable examples of what is being run.\n\n6. **Documentation / README Additions**\n- Add or extend a dedicated section, e.g. **\"Running Multiple Phones in Parallel (Multi-Lane Architecture)\"** including:\n  - **Conceptual overview**:\n    - Explain that the **recommended parallel architecture** is **1 phone per lane**, with each lane bound to its own Appium URL and its own `system_port_base`.\n    - Explain why this reduces cross-talk: each Appium instance handles exactly one device, and each lane uses distinct UiAutomator2 `systemPort` ranges.[2][4][7]\n  - **Step-by-step examples**:\n    1) *Starting 2–3 Appium instances via process_tracker*:\n       ```bash\n       # Example: start 3 Appium servers on 4723, 4725, 4727\n       python process_tracker.py start-multi-appium --count 3\n       ```\n    2) *Starting 2–3 lanes mapped to those URLs and system_port ranges*:\n       ```bash\n       # Each lane uses its own Appium URL and system_port_base from LANES config\n       python process_tracker.py start-lane --lane-name lane_4723\n       python process_tracker.py start-lane --lane-name lane_4725\n       python process_tracker.py start-lane --lane-name lane_4727\n       ```\n    3) *End-to-end N-phone example*:\n       ```bash\n       # N-phone parallel run (N=3)\n       python scripts/start_multi_lane.py --lanes lane_4723,lane_4725,lane_4727\n       ```\n  - **Troubleshooting and best practices**:\n    - Note that each Appium instance must be started on a **unique port**, with unique `-bp` and `--chromedriver-port` where relevant.[2][5]\n    - Emphasize that Android SDK environment must be valid (`ANDROID_HOME` / `ANDROID_SDK_ROOT` set; see Task 16) before launching servers.\n    - Point to systemPort-related cleanup and stability improvements (Tasks 26, 29) as the foundation for stable long-running parallel sessions.\n  - Clearly label previous multi-worker-on-one-Appium designs as **deprecated**, reinforcing the new lane-based model from Task 32.\n\n7. **Logging and Observability**\n- Ensure all new helpers log:\n  - The exact Appium commands and ports used.\n  - For each lane process: its lane name, `appium_url`, and `system_port_base`.\n- Optionally add a `status` command in process_tracker to show running Appium instances and lanes, aiding operators.\n\n8. **Robust Shutdown Behavior**\n- Ensure that process_tracker cleanly terminates Appium instances and lane processes:\n  - On explicit stop commands (`stop-appium`, `stop-lane`, or `stop-all`).\n  - On SIGINT/SIGTERM (Ctrl+C) with graceful teardown order: stop lanes first, then Appium servers.\n- Leverage the cleanup improvements from Tasks 18, 26, and 29 to make shutdowns leave no busy ports or zombie UiAutomator2 servers.\n",
        "testStrategy": "1. **Unit / Integration Tests for process_tracker Multi-Appium Support**\n- Add tests (or integration-style checks) for new helpers:\n  - Given a configuration with 3 instances, `start-multi-appium --count 2` only starts the first 2 and returns their PIDs.\n  - Port conflict handling: simulate a busy port (e.g. binding a dummy server) and verify that process_tracker either skips that instance with a clear log or fails with a descriptive error.\n  - Verify that each instance’s command includes the correct `-p` and `-bp` arguments.\n\n2. **Lane to Appium Mapping Tests**\n- In tests for `posting_lane.py` and LANES config:\n  - Ensure that for a given `lane-name`, the resolved `appium_url` matches one of the configured Appium instance ports.\n  - Verify that `system_port_base` ranges do not overlap between lanes used in parallel.\n  - Confirm that `SmartInstagramPoster` receives the correct `appium_url` and `system_port_base` when started via process_tracker.\n\n3. **Manual End-to-End Parallel Run (2 Devices)**\n- Connect 2 Android devices (or emulators) with known `udid`s.\n- Configure 2 Appium instances: ports 4723 and 4725, each bound to a distinct device (via capabilities / `-U` in server or client config as already supported by existing tasks).\n- Run:\n  ```bash\n  python process_tracker.py start-multi-appium --count 2\n  python process_tracker.py start-lane --lane-name lane_4723\n  python process_tracker.py start-lane --lane-name lane_4725\n  ```\n- Observe:\n  - Both Appium servers start and respond on `/status`.\n  - Each lane posts to its own device, with no cross-talk (logs show different `udid` and systemPort values).\n\n4. **Manual End-to-End Parallel Run (3 Devices, N-Phone Example)**\n- Add a third device and a third lane/Appium mapping (4727).\n- Run the helper script:\n  ```bash\n  python scripts/start_multi_lane.py --lanes lane_4723,lane_4725,lane_4727\n  ```\n- Verify:\n  - Three Appium servers are running on 4723/4725/4727.\n  - Three lane processes are active, each bound to a distinct Appium URL and non-overlapping systemPort range.\n  - No device receives actions intended for another device; monitor both device screens and logs.\n\n5. **Shutdown and Cleanup Tests**\n- While lanes and Appium instances are running, send SIGINT (Ctrl+C) to the controlling process.\n- Confirm that:\n  - Lane processes exit gracefully.\n  - Appium servers are terminated.\n  - No `systemPort` busy errors occur when restarting the same multi-lane run immediately afterward (leveraging cleanup from Tasks 26 and 29).\n\n6. **Documentation Verification**\n- Have a team member unfamiliar with the implementation follow the new README section from scratch:\n  - Install prerequisites.\n  - Start multiple Appium instances and lanes as described.\n  - Run parallel posting on at least 2 devices.\n- Collect feedback and confirm that:\n  - Commands are copy-pasteable and work as written.\n  - The **1-phone-per-lane** recommendation and rationale are clearly understood.\n",
        "status": "done",
        "dependencies": [
          "16",
          "18",
          "26",
          "28",
          "30",
          "31",
          "32"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-12T11:21:01.991Z"
      },
      {
        "id": "34",
        "title": "Replace Windows Port Killing with ADB Forward Cleanup for systemPort Busy Errors",
        "description": "Fix systemPort busy errors by replacing the ineffective netstat/taskkill-based kill_port() function with proper ADB forward cleanup using `adb forward --remove`, implementing stale forward enumeration at lane startup, and making port-busy errors fatal to skip to the next account instead of retrying.",
        "details": "## Problem Analysis\n\nThe current `kill_port()` function in `posting_lane.py` (lines 207-228) uses Windows `netstat` and `taskkill` to kill processes by PID. This is **completely ineffective** because:\n1. The systemPort (e.g., 8201) is **not held by a Windows process listening on that port**.\n2. Appium's UiAutomator2 driver uses **ADB port forwarding** (`adb forward tcp:8201 tcp:6790`) to tunnel traffic to the device.\n3. The port appears \"busy\" because the **ADB forward mapping** persists after failed sessions, not because a Windows process is listening.\n\n## Implementation Plan\n\n### 1. Replace `kill_port()` with `remove_adb_forward()` in posting_lane.py\n\nRemove the current implementation (lines 207-228) and replace with:\n\n```python\ndef remove_adb_forward(port: int, device: str = None) -> bool:\n    \"\"\"Remove ADB port forward for a specific local port.\n    \n    This is the correct way to free systemPorts - NOT Windows taskkill.\n    Appium's systemPort is an ADB forward, not a process listener.\n    \"\"\"\n    import subprocess\n    ADB_PATH = r\"C:\\Users\\asus\\Downloads\\platform-tools-latest-windows\\platform-tools\\adb.exe\"\n    \n    try:\n        # If device specified, use device-specific removal\n        if device:\n            cmd = [ADB_PATH, \"-s\", device, \"forward\", \"--remove\", f\"tcp:{port}\"]\n        else:\n            cmd = [ADB_PATH, \"forward\", \"--remove\", f\"tcp:{port}\"]\n        \n        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n        \n        if result.returncode == 0:\n            log(f\"Removed ADB forward tcp:{port}\")\n            return True\n        else:\n            # Not an error if forward didn't exist\n            if \"not found\" in result.stderr.lower() or result.stderr.strip() == \"\":\n                return True\n            log(f\"ADB forward remove warning: {result.stderr}\")\n            return False\n    except Exception as e:\n        log(f\"Error removing ADB forward tcp:{port}: {e}\")\n        return False\n```\n\n### 2. Add `cleanup_all_stale_forwards()` helper for lane startup\n\n```python\ndef cleanup_all_stale_forwards(system_port_base: int, device: str = None) -> int:\n    \"\"\"List and remove ALL ADB forwards in the lane's port range at startup.\n    \n    Returns count of forwards removed.\n    \"\"\"\n    import subprocess\n    ADB_PATH = r\"C:\\Users\\asus\\Downloads\\platform-tools-latest-windows\\platform-tools\\adb.exe\"\n    \n    removed_count = 0\n    \n    try:\n        # List all current forwards\n        result = subprocess.run([ADB_PATH, \"forward\", \"--list\"], \n                               capture_output=True, text=True, timeout=10)\n        \n        if result.returncode != 0:\n            log(f\"Warning: Could not list ADB forwards: {result.stderr}\")\n            return 0\n        \n        # Parse format: \"emulator-5554 tcp:8201 tcp:6790\"\n        for line in result.stdout.strip().split('\\n'):\n            if not line.strip():\n                continue\n            parts = line.split()\n            if len(parts) >= 2:\n                local_spec = parts[1]  # \"tcp:8201\"\n                if local_spec.startswith(\"tcp:\"):\n                    port = int(local_spec.split(\":\")[1])\n                    # Check if port is in our lane's range\n                    if system_port_base <= port < system_port_base + 100:\n                        log(f\"Removing stale forward: {line}\")\n                        remove_adb_forward(port)\n                        removed_count += 1\n    except Exception as e:\n        log(f\"Error listing ADB forwards: {e}\")\n    \n    return removed_count\n```\n\n### 3. Update `cleanup_ports()` to use ADB forward removal\n\nReplace current implementation (lines 231-236):\n\n```python\ndef cleanup_ports(system_port_base: int, device: str = None):\n    \"\"\"Clean up all ADB forwards in the lane's systemPort range.\n    \n    IMPORTANT: This removes ADB FORWARDS, not Windows processes.\n    The systemPort is held by ADB port forwarding, not process listeners.\n    \"\"\"\n    log(f\"Cleaning up ADB forwards for ports {system_port_base} to {system_port_base+10}...\")\n    \n    # First, enumerate and remove ALL stale forwards in range\n    removed = cleanup_all_stale_forwards(system_port_base, device)\n    log(f\"Removed {removed} stale ADB forwards\")\n    \n    # Belt-and-suspenders: explicitly remove the exact ports we'll use\n    for offset in range(1, 6):\n        port = system_port_base + offset\n        remove_adb_forward(port, device)\n```\n\n### 4. Add pre-session cleanup in post_reel_smart.py `connect_appium()`\n\nUpdate the TASK 29 cleanup section (lines 840-867) to be more robust:\n\n```python\n# TASK 29/34: Pre-Appium cleanup - remove stale ADB forwards\nif self.system_port and self.device:\n    print(f\"  Pre-Appium cleanup for {self.device} on port {self.system_port}...\")\n    \n    # 1. Force-stop UiAutomator2 on device (releases device-side port)\n    try:\n        subprocess.run(\n            [ADB_PATH, \"-s\", self.device, \"shell\", \"am\", \"force-stop\", \n             \"io.appium.uiautomator2.server\"],\n            capture_output=True, timeout=10\n        )\n        subprocess.run(\n            [ADB_PATH, \"-s\", self.device, \"shell\", \"am\", \"force-stop\",\n             \"io.appium.uiautomator2.server.test\"],\n            capture_output=True, timeout=10\n        )\n    except Exception as e:\n        print(f\"    Warning: UiAutomator2 force-stop failed: {e}\")\n    \n    # 2. CRITICAL: Remove ADB forward (this is what actually holds systemPort)\n    try:\n        result = subprocess.run(\n            [ADB_PATH, \"forward\", \"--remove\", f\"tcp:{self.system_port}\"],\n            capture_output=True, text=True, timeout=10\n        )\n        if result.returncode == 0 or \"not found\" in result.stderr.lower():\n            print(f\"    Cleared ADB forward tcp:{self.system_port}\")\n        else:\n            print(f\"    ADB forward remove: {result.stderr}\")\n    except Exception as e:\n        print(f\"    Warning: ADB forward remove failed: {e}\")\n```\n\n### 5. Make systemPort busy errors FATAL in posting_lane.py\n\nUpdate the exception handling in `run_lane()` (around line 382):\n\n```python\nexcept Exception as e:\n    error_msg = str(e)[:200]\n    \n    # Check for systemPort busy error - this is FATAL, don't retry\n    if \"local port\" in error_msg.lower() and \"busy\" in error_msg.lower():\n        log(f\"FATAL PORT ERROR: {account} - systemPort busy, skipping to next account\", lane_name)\n        write_result_to_csv(video.shortcode, account, \"error\", f\"systemPort busy (fatal): {error_msg}\")\n        # Clean up and continue to next account - DO NOT retry\n        continue\n    \n    if \"cannot find any free port\" in error_msg.lower():\n        log(f\"FATAL PORT ERROR: {account} - no free ports in range, skipping\", lane_name)\n        write_result_to_csv(video.shortcode, account, \"error\", f\"No free ports (fatal): {error_msg}\")\n        continue\n    \n    # Regular errors can still be logged normally\n    log(f\"ERROR: {account} - {error_msg}\", lane_name)\n    write_result_to_csv(video.shortcode, account, \"error\", error_msg)\n```\n\n### 6. Add diagnostic logging at lane startup\n\nIn `run_lane()` after cleanup_ports(), add diagnostic logging:\n\n```python\n# Log current ADB forward state for debugging\ntry:\n    result = subprocess.run([ADB_PATH, \"forward\", \"--list\"],\n                           capture_output=True, text=True, timeout=10)\n    if result.stdout.strip():\n        log(f\"Active ADB forwards after cleanup:\\n{result.stdout}\", lane_name)\n    else:\n        log(f\"No active ADB forwards (clean state)\", lane_name)\nexcept Exception as e:\n    log(f\"Warning: Could not list ADB forwards: {e}\", lane_name)\n```\n\n## Key Implementation Notes\n\n1. **ADB_PATH constant**: Both files need the same ADB path. Consider moving to a shared config.\n\n2. **Device-specific vs global removal**: Use device-specific removal (`-s device`) when device is known, global removal when cleaning up at lane startup before device connection.\n\n3. **Idempotent cleanup**: All cleanup operations should be safe to call multiple times - \"not found\" is not an error.\n\n4. **Error classification**: systemPort busy errors are now FATAL and skip to next account, preventing infinite retry loops that waste time.",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Tests for ADB Forward Cleanup Functions\n\n**Test: remove_adb_forward() with existing forward**\n```bash\n# Setup: Create a dummy forward\nadb forward tcp:8201 tcp:5555\n# Verify it exists\nadb forward --list | grep 8201\n# Run cleanup\npython -c \"from posting_lane import remove_adb_forward; remove_adb_forward(8201)\"\n# Verify removed\nadb forward --list | grep 8201  # Should return nothing\n```\n\n**Test: remove_adb_forward() with non-existent forward**\n```bash\n# Ensure port is not forwarded\nadb forward --remove tcp:9999 2>/dev/null || true\n# Run cleanup (should not error)\npython -c \"from posting_lane import remove_adb_forward; assert remove_adb_forward(9999) == True\"\n```\n\n**Test: cleanup_all_stale_forwards() enumerates and removes**\n```bash\n# Setup: Create multiple forwards in a range\nadb forward tcp:8201 tcp:5555\nadb forward tcp:8202 tcp:5556\nadb forward tcp:8203 tcp:5557\n# Run cleanup for base 8200\npython -c \"from posting_lane import cleanup_all_stale_forwards; print(cleanup_all_stale_forwards(8200))\"\n# Should print \"3\" and all forwards should be gone\nadb forward --list | grep -E \"820[1-3]\"  # Should return nothing\n```\n\n### 2. Integration Tests for Lane Startup Cleanup\n\n**Test: Fresh lane startup clears stale forwards**\n```bash\n# Create stale forwards simulating crashed session\nadb forward tcp:8201 tcp:6790\nadb forward tcp:8202 tcp:6791\n\n# Start lane with same port range\npython posting_lane.py --lane-name lane1 --accounts-file test_accounts.txt \\\n    --add-folder test_folder --system-port-base 8200 2>&1 | head -20\n\n# Verify logs show cleanup messages:\n# - \"Removing stale forward: ...\"\n# - \"No active ADB forwards (clean state)\" or enumeration\n```\n\n### 3. Fatal Error Handling Tests\n\n**Test: systemPort busy error is fatal (no retry)**\n```bash\n# Simulate port busy by creating forward and not removing\nadb forward tcp:8201 tcp:6790\n\n# Mock/patch remove_adb_forward to always fail\n# Run single job - should fail with \"fatal\" message and move to next account\n# NOT retry the same account\n\n# Verify in batch_results_*.csv:\n# - Status should be \"error\"\n# - Error message should contain \"systemPort busy (fatal)\"\n# - Next account should be attempted, not the same one\n```\n\n**Test: Regular errors still logged normally**\n```bash\n# Simulate a regular error (e.g., ADB readiness)\n# Verify error message does NOT contain \"fatal\"\n# Verify retry behavior is unchanged for non-port errors\n```\n\n### 4. End-to-End Lane Test\n\n**Test: Full posting run with proper cleanup**\n```bash\n# Stop any running Appium servers\ntaskkill /F /IM node.exe 2>/dev/null || true\n\n# Clear all ADB forwards\nadb forward --remove-all\n\n# Start fresh Appium on lane1 port\nappium --address 127.0.0.1 --port 4723 &\n\n# Run lane with 2-3 test accounts\npython posting_lane.py --lane-name lane1 --accounts-file test_accounts.txt \\\n    --add-folder chunk_01c --system-port-base 8200\n\n# After completion (success or fail), verify:\n# 1. No orphaned ADB forwards: adb forward --list | grep 820\n# 2. Logs show cleanup at startup and between accounts\n# 3. If any account hit port-busy, it was marked fatal and skipped\n```\n\n### 5. Pre-Session Cleanup Test in post_reel_smart.py\n\n**Test: connect_appium() removes forwards before connecting**\n```bash\n# Create stale forward for the port we'll use\nadb forward tcp:8201 tcp:6790\n\n# Run a single post (this should clean up and succeed)\npython post_reel_smart.py test_account video.mp4 \"test\" --system-port 8201\n\n# Verify logs show:\n# - \"Cleared ADB forward tcp:8201\"\n# - Successful Appium connection\n```\n\n### 6. Regression Tests\n\n**Test: Normal posting flow unchanged**\n- Run standard posting workflow\n- Verify cleanup doesn't break working sessions\n- Verify timing is not significantly impacted (cleanup should be <1s)\n\n**Test: Multi-lane isolation preserved**\n- Run 2 lanes simultaneously with different port ranges\n- Verify lane1 cleanup doesn't affect lane2 forwards\n- Verify no cross-lane port conflicts\n\n### 7. Diagnostic Logging Verification\n\n**Test: ADB forward state logged at startup**\n```bash\npython posting_lane.py --lane-name lane1 ... 2>&1 | grep -E \"(ADB forward|Active|stale)\"\n# Should see:\n# - \"Cleaning up ADB forwards for ports...\"\n# - \"Removed X stale ADB forwards\" or \"No active ADB forwards\"\n```",
        "status": "done",
        "dependencies": [
          "18",
          "26",
          "27",
          "28",
          "31"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T12:11:39.820Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-12-12T12:11:39.821Z",
      "taskCount": 34,
      "completedCount": 29,
      "tags": [
        "posting"
      ]
    }
  }
}
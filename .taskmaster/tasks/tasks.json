{
  "master": {
    "tasks": [
      {
        "id": 11,
        "title": "Project skeleton, configuration, and input/output handling",
        "description": "Set up the Python project, config management, and basic CLI to load inputs (photos, bios, follow list, proxies, API keys) and write CSV output for created accounts.",
        "details": "Implementation details:\n- Use Python 3.11+ with a simple, single-package structure (e.g., `geelark_ig_bot/`).\n- Create `config.py` to load configuration from a `.env` file (using `python-dotenv`) or a `config.yaml` file (using `pyyaml`). Keys: DAISY_SMS_KEY, TWO_CAPTCHA_KEY, ANTHROPIC_KEY, PROXY_ROTATE_URL, GEELARK_DEVICE_ID or connection params, paths for PHOTOS_DIR, BIOS_FILE, FOLLOW_FILE, OUTPUT_CSV.\n- Implement a small `models.py` with dataclasses such as `AccountProfile(photo_path, bio, follow_targets)` and `RunContext(proxy_url, device_id, session_id, logs_path)`.\n- Implement `io_inputs.py`:\n  - Load all image paths from the photos folder (validate file extensions and existence).\n  - Load bios from a text file, one bio per non-empty line.\n  - Load accounts-to-follow from a text file, one username per non-empty line.\n- Implement `io_outputs.py` with function `append_created_account(csv_path, username, password, phone, status, extra=None)` that appends a row; ensure the CSV is created with a header if missing.\n- Implement `main.py` with a CLI (using `argparse`) that supports parameters like `--accounts N`, `--device-id`, `--start-index`, `--output-csv`.\n- Add logging (built-in `logging` module) with INFO for high-level steps and DEBUG for low-level details; log to both console and a rotating file handler.\n- Ensure paths and config values are validated at startup, with clear error messages and non-zero exit codes on failure.\n- Keep architecture minimal: a main loop that calls a `create_single_account(profile: AccountProfile)` function implemented in later tasks.\n\nPseudo-code sketch:\n```python\n# main.py\nfrom config import load_config\nfrom io_inputs import load_photos, load_bios, load_follow_targets\nfrom io_outputs import append_created_account\nfrom workflow import create_single_account\n\nif __name__ == \"__main__\":\n    cfg = load_config()\n    photos = load_photos(cfg.PHOTOS_DIR)\n    bios = load_bios(cfg.BIOS_FILE)\n    follows = load_follow_targets(cfg.FOLLOW_FILE)\n\n    for i in range(cfg.NUM_ACCOUNTS):\n        profile = build_profile(photos, bios, follows, i)\n        result = create_single_account(profile, cfg)\n        append_created_account(\n            cfg.OUTPUT_CSV,\n            result.username,\n            result.password,\n            result.phone,\n            result.status,\n        )\n```",
        "testStrategy": "- Unit test config loading with missing/invalid keys.\n- Unit test input loaders with temporary directories and sample files.\n- Unit test CSV writer: create temp file, append multiple rows, verify header and data.\n- Run a dry-run mode (no device interaction) that uses mock `create_single_account` to verify CLI, logging, and CSV pipeline behave correctly.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Geelark device control abstraction (RPA/ADB/API)",
        "description": "Research and implement a minimal Python abstraction to control Geelark cloud phones (screenshot, tap, type, scroll) using the most reliable available method (RPA, ADB, or API).",
        "details": "Implementation details:\n- Investigate GeeLark’s RPA feature and any documented APIs from their dashboard/help center.[1]\n- Decide on a practical option:\n  - **Option A (preferred, if accessible):** Use GeeLark RPA/Custom tasks via HTTP or WebSocket if they expose an API to trigger actions on a running device (tap, input text, wait), or via a local bridge component.\n  - **Option B:** Connect via ADB over TCP to the cloud phone (if GeeLark exposes an ADB endpoint per phone). Use `adbutils` or `pure-python-adb` for screenshots and input events.\n  - **Option C:** If GeeLark has an official REST API to interact with cloud phones, wrap the relevant endpoints.\n- Define a Python interface `GeelarkDeviceController` in `geelark_device.py` with methods:\n  - `screenshot() -> bytes` (PNG/JPEG data)\n  - `tap(x: int, y: int)`\n  - `type_text(text: str)`\n  - `scroll(direction: Literal[\"up\",\"down\",\"left\",\"right\"], amount: int=500)`\n  - `back()` to press back button\n  - `home()` to go home\n  - `wait(seconds: float)` for simple delays.\n- Implement at least one concrete subclass, e.g., `AdbGeelarkDeviceController` or `RpaGeelarkDeviceController`, depending on what is feasible with GeeLark.\n- Include a simple device discovery/attachment function: `connect_device(device_id_or_host) -> GeelarkDeviceController`.\n- Ensure screenshot capturing is performant (e.g., ADB `exec-out screencap -p`), and images are in a format accepted by Claude Vision.\n\nExample using ADB-style pseudo-code:\n```python\nclass AdbGeelarkDeviceController(GeelarkDeviceController):\n    def __init__(self, serial: str):\n        self.adb = adbutils.AdbDevice(serial=serial)\n\n    def screenshot(self) -> bytes:\n        return self.adb.screencap()\n\n    def tap(self, x, y):\n        self.adb.shell(f\"input tap {x} {y}\")\n\n    def type_text(self, text):\n        safe = text.replace(\" \", \"%s\")\n        self.adb.shell(f\"input text '{safe}'\")\n\n    def scroll(self, direction, amount=500):\n        if direction == \"up\":\n            self.adb.shell(f\"input swipe 500 1000 500 {1000-amount}\")\n        # etc.\n```",
        "testStrategy": "- If ADB is used, test against a local Android emulator: verify that screenshot bytes are non-empty and tapping/types produce visible effects.\n- If GeeLark RPA/API is used, integration test on a disposable cloud phone: tap a known coordinate (e.g., Settings icon) and verify manually.\n- Add a `--test-device` CLI option that runs a quick health-check: take screenshot, tap a test area, log success/failure.\n- Use mocks in unit tests to assert high-level code calls `tap`, `type_text`, etc., with expected parameters.",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Claude Vision screen understanding client",
        "description": "Implement a Python client that sends device screenshots and the current context to Claude Vision, parses its response into actionable steps with coordinates and text.",
        "details": "Implementation details:\n- Use Anthropic’s Python SDK or plain HTTP with API key from config.\n- Define a prompt template that instructs Claude Vision explicitly:\n  - Provide **screen description**.\n  - Provide **next action** in a strict JSON format with fields like `{\"action\": \"tap\"|\"type\"|\"scroll\"|\"done\"|\"wait\",\"coordinates\": {\"x\": int, \"y\": int},\"text\": \"...\", \"reason\": \"...\"}`.\n  - Ask it to always respond with a single JSON object and no extra text.\n  - Instruct it that the goal is to create and fully set up an Instagram account according to the step list (birthday, phone, SMS, username, password, skip optional, photo, bio, creator, follow accounts).\n- Implement `claude_vision.py` with:\n  - `class ClaudeVisionClient:`\n    - `propose_action(image_bytes: bytes, state: dict) -> dict` where `state` includes progress markers (e.g., `has_entered_birthday`, `has_verified_phone`).\n- Implement robust JSON parsing:\n  - Strip any non-JSON prefix/suffix if Claude accidentally adds text.\n  - Validate that required keys exist; if not, log error and request again with a clarifying system message.\n- Include rate limiting/backoff and simple retry for network errors or malformed responses.\n- Maintain a small `state` object that encodes goal progress to share with Claude in the system/user message so it can choose the next step more reliably.\n\nPseudo-code:\n```python\nSYSTEM_PROMPT = \"\"\"You are controlling an Android phone to create a new Instagram account...\"\"\"\n\ndef propose_action(self, img, state):\n    msg = self._build_message(state)\n    resp = self.client.messages.create(\n        model=\"claude-3.5-sonnet\",  # or latest vision-capable model\n        max_tokens=300,\n        temperature=0.1,\n        messages=[\n          {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n          {\"role\": \"user\", \"content\": [\n              {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": base64.b64encode(img).decode()}},\n              {\"type\": \"text\", \"text\": msg},\n          ]},\n        ],\n    )\n    json_str = extract_json(resp)\n    return json.loads(json_str)\n```",
        "testStrategy": "- Unit test prompt-building and JSON parsing with canned Claude-like responses.\n- Add an offline mode that uses a fake vision client returning predetermined actions for known test screenshots to validate the loop without spending API credits.\n- Log each request/response pair to a file (with redaction of secrets) and manually inspect a few runs to ensure action JSON is consistent.",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "DaisySMS integration for phone number and SMS code retrieval",
        "description": "Implement integration with DaisySMS to rent phone numbers and poll for Instagram verification SMS codes.",
        "details": "Implementation details:\n- Review DaisySMS API docs to identify endpoints for:\n  - Requesting a number for a specific service/country.\n  - Checking SMS status and retrieving the code.\n  - Canceling/finishing an activation.\n- Implement `daisysms_client.py` with:\n  - `request_number(service=\"instagram\", country=None) -> Activation` where `Activation` holds `id`, `phone_number`.\n  - `wait_for_sms(activation_id, timeout=300, poll_interval=5) -> str` returning the numeric code.\n  - `cancel_activation(activation_id)` and `finish_activation(activation_id)`.\n- Handle common failure cases: no numbers, timeout waiting for SMS, banned/invalid numbers.\n- Mask phone number in logs for privacy.\n- Provide helper to format phone for entering on the device (e.g., strip `+` if needed, or let Claude decide how to input it given the screenshot).\n\nPseudo-code sketch:\n```python\nclass DaisySmsClient:\n    def request_number(self):\n        # call API, parse JSON\n        return Activation(id=act_id, phone=phone)\n\n    def wait_for_sms(self, act_id, timeout=300):\n        # loop: GET status, parse text, extract 6-digit code via regex\n```",
        "testStrategy": "- Unit test JSON parsing with sample DaisySMS responses.\n- Use a mock HTTP server (e.g., `responses` or `httpretty`) for DaisySMS endpoints to validate retry and timeout behavior.\n- In a staging run, manually request a number and send a test SMS from another phone to verify code extraction logic.\n- Simulate failure modes (no number, timeout, malformed SMS) and confirm the calling workflow handles them gracefully (marks account as failed, logs reason, releases activation).",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "2Captcha integration for solving Instagram captchas",
        "description": "Integrate with 2Captcha to submit Instagram captchas (image or possibly hCaptcha/ReCaptcha) and retrieve solutions when they appear during signup.",
        "details": "Implementation details:\n- Check 2Captcha docs for supported captcha types on Instagram flows (likely image captcha or hCaptcha/ReCaptcha). Implement at least generic image captcha support; leave hooks for sitekey-based captchas if needed.\n- Implement `twocaptcha_client.py` with:\n  - `submit_image_captcha(image_bytes) -> captcha_id`.\n  - `wait_for_solution(captcha_id, timeout=180, poll_interval=5) -> str`.\n- Integrate with the main flow via a simple contract: when Claude identifies a captcha on the screen and indicates an `action: \"captcha\"` (we can define this), capture a high-resolution screenshot and crop if necessary:\n  - Either ask Claude to provide bounding box coordinates, then crop the relevant region before sending to 2Captcha.\n- After receiving the solution string, pass it back to the device using `type_text` or `tap`/`type` sequences as directed by Claude.\n- Implement error handling: if 2Captcha returns an error or times out, mark run as failed and log details.\n\nPseudo-code:\n```python\nclass TwoCaptchaClient:\n    def submit_image_captcha(self, img):\n        # POST multipart/form-data to 2Captcha\n\n    def wait_for_solution(self, cap_id, timeout):\n        # poll /res.php until status=1\n```",
        "testStrategy": "- Unit test polling and response parsing using mocked 2Captcha HTTP endpoints.\n- Manual integration test with a known captcha image to confirm that 2Captcha returns the expected text.\n- Simulate failures such as `ERROR_CAPTCHA_UNSOLVABLE` and ensure workflow either retries with a new captcha or aborts with a clear status.",
        "priority": "medium",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Mobile proxy rotation and network setup",
        "description": "Implement proxy rotation via a mobile proxy rotation URL before each new account and ensure all external calls from the device go through the rotated IP.",
        "details": "Implementation details:\n- Use the provided `PROXY_ROTATE_URL` config: before starting each new account creation, send a simple HTTP GET to this URL and wait a short delay (e.g., 5–10 seconds) for IP to change.\n- If GeeLark supports per-device proxy assignment, ensure the cloud phone is configured to use the mobile proxy; otherwise, rely on proxy at network edge.\n- Implement `proxy.py` with:\n  - `rotate_proxy() -> bool` which returns True on HTTP 2xx, False otherwise.\n- Add logging to record rotation attempts and results.\n- Optionally verify IP change using a cheap `https://api.ipify.org` style service via the device’s browser or host network (config-driven; disabled by default to avoid extra calls).\n- Integrate into `create_single_account` workflow: call `rotate_proxy()` once at the very beginning of each account run.\n\nPseudo-code:\n```python\ndef rotate_proxy(url, timeout=10):\n    try:\n        r = requests.get(url, timeout=timeout)\n        r.raise_for_status()\n        logger.info(\"Proxy rotated\")\n        time.sleep(8)\n        return True\n    except Exception as e:\n        logger.error(f\"Proxy rotation failed: {e}\")\n        return False\n```",
        "testStrategy": "- Unit test `rotate_proxy` with mocked HTTP responses (success, timeout, non-200).\n- In staging, call rotation multiple times and verify IP change manually using an external IP-check service.\n- Add a debug flag to log detected IPs (host-level) before and after rotation for manual verification.",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Core action loop: screenshot → Claude → device actions",
        "description": "Implement the main control loop that repeatedly screenshots the Geelark device, asks Claude Vision what to do, executes the returned action (tap/type/scroll), and tracks progress toward account creation.",
        "details": "Implementation details:\n- Implement `workflow.py` with a function `run_screen_loop(device: GeelarkDeviceController, vision: ClaudeVisionClient, state: dict, max_steps=200) -> state`.\n- Loop behavior:\n  - For each step:\n    - Take screenshot via `device.screenshot()`.\n    - Call `vision.propose_action(image_bytes, state)`.\n    - Parse action JSON and execute:\n      - `action == \"tap\"`: call `device.tap(x, y)`.\n      - `action == \"type\"`: call `device.type_text(text)`.\n      - `action == \"scroll\"`: call `device.scroll(direction, amount)`.\n      - `action == \"wait\"`: call `device.wait(seconds)`.\n      - `action == \"back\"`/`\"home\"`: call corresponding methods.\n      - `action == \"done\"`: break loop and return.\n      - `action == \"captcha\"`: delegate to 2Captcha handler (Task 15) then feed solution back.\n    - Update `state` with any progress hints returned (e.g., `state[\"phase\"] = resp[\"phase\"]`).\n    - Add random small delays (0.5–1.5 s) to mimic human interaction and let UI update.\n- Implement safety guards:\n  - If `max_steps` reached without `done`, mark run as failed.\n  - Detect repeated identical actions (same tap coordinates for many steps) and break to avoid loops.\n- Ensure the state encodes key information for later steps (e.g., whether phone number has been used, SMS verified, username set, account switched to creator, followed 20 accounts).\n\nPseudo-code:\n```python\ndef run_screen_loop(device, vision, state, max_steps=200):\n    for i in range(max_steps):\n        img = device.screenshot()\n        action = vision.propose_action(img, state)\n        if action[\"action\"] == \"done\":\n            state[\"status\"] = \"done\"\n            break\n        execute_action(device, action, state)\n    return state\n```",
        "testStrategy": "- Implement unit tests for `execute_action` using a mock `GeelarkDeviceController` to verify correct calls for each action type.\n- Use an offline fake-vision client (from Task 13 tests) returning a deterministic series of actions to validate that the loop terminates correctly and state progresses.\n- On a test device with Instagram already on a simple form screen, run a short loop and confirm taps and typing correspond roughly to what Claude suggests (manual spot check using logs and video capture).",
        "priority": "high",
        "dependencies": [
          12,
          13,
          15
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Account setup orchestration and Instagram-specific flow",
        "description": "Orchestrate a full Instagram account setup run, coordinating proxy rotation, device control, Claude-driven steps, DaisySMS phone/SMS handling, captchas, and the business logic for username/password, bio, photo, creator switch, and following accounts.",
        "details": "Implementation details:\n- Implement `create_single_account(profile: AccountProfile, cfg) -> AccountResult` in `workflow.py`.\n- High-level sequence:\n  1. Rotate proxy using Task 16.\n  2. Connect to Geelark device (Task 12) and ensure Instagram app is launched (via explicit launch intent or by tapping icon; you can teach Claude to tap the Instagram icon from home screen as part of loop).\n  3. Initialize `state` with:\n     - `target_bio`, `target_photo_path`, `follow_targets`.\n     - Flags: `birthday_entered`, `phone_requested`, `sms_verified`, `username_set`, `password_set`, `creator_switched`, `followed_count`.\n  4. Request DaisySMS number when the flow reaches phone entry stage:\n     - Either pre-request the number before starting, or better, when `state` indicates phone will be needed (e.g., when Claude says \"now enter phone number\").\n     - Store number and activation id in `state`.\n  5. Run `run_screen_loop` until `state[\"status\"] == \"done\"` or error.\n  6. In the loop integration, insert hooks based on `state`:\n     - When a screen expects the phone number, programmatically supply the DaisySMS number (you may give Claude the number in the context so it types it itself).\n     - After submitting phone, start a background `wait_for_sms` and when code is received, provide it to Claude in the next prompt so it can type it.\n     - For username/password, either auto-generate values in Python (e.g., random letters+digits) and provide them to Claude, or let Claude propose them but ensure Python records them in `state` so they can be output to CSV.\n  7. Ensure optional steps (such as contacts, notifications, etc.) are skipped—rely on Claude’s screen understanding but mention this explicitly in the prompt.\n  8. After reaching home feed, direct Claude (via state goal) to:\n     - Add profile photo from gallery: upload `target_photo_path` to the device or ensure the device already has a set of photos (outside of script scope) and instruct Claude accordingly.\n     - Add bio using `target_bio`.\n     - Switch to Creator account via settings (state flag `creator_switched=True` when done).\n     - Follow ~20 accounts from `follow_targets` list (give the list or next target to Claude in context, track `followed_count`).\n- Implement `AccountResult(username, password, phone, status, error_message=None)` dataclass.\n- On any unrecoverable error (DaisySMS/2Captcha failure, loop timeout, device disconnection), set `status=\"failed\"` and include `error_message`.\n",
        "testStrategy": "- Unit test orchestration logic with mocks for DaisySMS, 2Captcha, device controller, and Claude client to ensure correct call ordering and state changes.\n- Implement a dry-run mode that skips actual external calls and produces synthetic `AccountResult` to verify CSV output and control flow.\n- Run an end-to-end test on a single GeeLark device with manual observation, logging all key decisions; verify that a full account is created and appears in Instagram.\n- After a successful single-account run, test a small batch (e.g., 3 accounts) in series to validate that proxy rotation and resource cleanup between runs behave correctly.",
        "priority": "high",
        "dependencies": [
          12,
          13,
          14,
          15,
          16,
          17
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Profile data assignment and username/password generation",
        "description": "Implement logic to map input photos, bios, and follow lists to each new account and deterministically generate secure usernames and passwords.",
        "details": "Implementation details:\n- Implement `profiles.py` with:\n  - `build_profile(photos, bios, follows, index) -> AccountProfile` using round-robin or randomized selection.\n  - `generate_username(index, base=None) -> str` using a configurable pattern (e.g., random adjectives+noun+digits) and allowed Instagram constraints.\n  - `generate_password() -> str` with 12–16 chars including letters, digits, and symbols.\n- Ensure that for each account run, `AccountProfile` includes:\n  - `photo_path`: may be None if fewer photos than accounts; handle gracefully (skip photo step).\n  - `bio`: may be randomly chosen or selected sequentially.\n  - `follow_targets`: either the full list or a subset of ~20 selected per account.\n- Pass generated username and password into `state` to be shared with Claude so it types them when appropriate.\n- Avoid reusing the same username; if Instagram rejects a username, have Claude propose alternatives but keep track in state and update `AccountResult` accordingly.\n\nPseudo-code:\n```python\n@dataclass\nclass AccountProfile:\n    username: str\n    password: str\n    photo_path: Optional[str]\n    bio: Optional[str]\n    follow_targets: list[str]\n```",
        "testStrategy": "- Unit test profile building to ensure fair rotation of bios/photos and correct slicing of follow targets (~20 per account).\n- Unit test username/password generation for uniqueness and complexity constraints.\n- Use a mock Claude client to simulate username rejection; verify that state and `AccountResult` update to the new accepted username.",
        "priority": "medium",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Logging, error handling, and basic scaling for multiple accounts",
        "description": "Harden the script with structured logging, error handling, cleanup between runs, and simple sequential multi-account execution.",
        "details": "Implementation details:\n- Extend logging from Task 11:\n  - Include per-account correlation ID in all logs.\n  - Log key milestones (proxy rotated, number acquired, SMS received, captcha solved, account created, failures).\n- Implement a central exception handler in `main.py` that catches unexpected errors per account, records a failed `AccountResult`, and continues to the next account instead of crashing the whole batch.\n- Add cleanup hooks:\n  - Release DaisySMS activations on error.\n  - Optionally reset Instagram app state between runs (e.g., clear data or log out via Claude instructions at end of run).\n- For scaling:\n  - Keep initial implementation strictly sequential (one account after another) to minimize complexity.\n  - Design the code to allow future parallelization (e.g., by making `create_single_account` stateless other than its arguments and return value), but do not add concurrency yet.\n- Expose a few runtime knobs via CLI/config: `MAX_STEPS`, `SMS_TIMEOUT`, `CAPTCHA_TIMEOUT`, `RETRY_LIMIT`.\n",
        "testStrategy": "- Simulate multiple account runs with mocks where some accounts succeed and others fail; verify that all results are written to CSV and script exits cleanly.\n- Inject failures (e.g., raise exceptions from DaisySMS/2Captcha/Claude clients) and confirm they are caught and logged and do not stop subsequent accounts.\n- Manual multi-account test (2–3 accounts) to verify logs are readable and correlated with account IDs.",
        "priority": "medium",
        "dependencies": [
          18
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-12-10T03:59:28.494Z",
      "updated": "2025-12-10T04:21:12.826Z",
      "description": "Tasks for master context"
    }
  },
  "posting": {
    "tasks": [
      {
        "id": "1",
        "title": "Set up project structure, configuration, and API key management",
        "description": "Initialize a Python project for Geelark Instagram posting automation, including config management for CSV paths, Claude Vision, proxy rotation URL, and 2Captcha keys.",
        "details": "Implementation details:\n- Use Python 3.11+.\n- Create a package structure, e.g. `geelark_ig_bot/` with modules: `config.py`, `csv_io.py`, `geelark_control.py`, `instagram_flow.py`, `logging_utils.py`, `main.py`.\n- Use `python-dotenv` or similar to load secrets from `.env` (ANTHROPIC_API_KEY, CAPTCHA_API_KEY, PROXY_ROTATION_URL, etc.).\n- Define a `Config` dataclass in `config.py` holding: `input_csv_path`, `output_log_csv_path`, `video_root_dir`, `proxy_rotation_url`, `anthropic_api_key`, `captcha_api_key`, `geelark_api_base`, `mvp_mode` (single device vs multi-account).\n- Add a `requirements.txt` including: `requests`, `pandas` or `python-csv` (standard), `python-dotenv`, `anthropic` (official Claude client), and any chosen Geelark control SDK or ADB wrapper.\n- Provide a simple YAML or JSON config file for non-secret settings (file paths, default timeouts, retry counts).\n- Pseudo-code example:\n```python\n# config.py\nfrom dataclasses import dataclass\nimport os\n\n@dataclass\nclass Config:\n    input_csv_path: str\n    output_log_csv_path: str\n    video_root_dir: str\n    proxy_rotation_url: str\n    anthropic_api_key: str\n    captcha_api_key: str | None\n    geelark_api_base: str\n    mvp_mode: bool = True\n\n\ndef load_config() -> Config:\n    return Config(\n        input_csv_path=os.getenv(\"INPUT_CSV\", \"input.csv\"),\n        output_log_csv_path=os.getenv(\"OUTPUT_LOG_CSV\", \"post_log.csv\"),\n        video_root_dir=os.getenv(\"VIDEO_ROOT_DIR\", \"./videos\"),\n        proxy_rotation_url=os.getenv(\"PROXY_ROTATION_URL\", \"\"),\n        anthropic_api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n        captcha_api_key=os.getenv(\"CAPTCHA_API_KEY\"),\n        geelark_api_base=os.getenv(\"GEELARK_API_BASE\", \"http://localhost:8000\"),\n    )\n```",
        "testStrategy": "- Unit test `load_config()` with different environment variable scenarios.\n- Verify that secrets are not hardcoded (only read from env/.env).\n- Run a dry `python -m geelark_ig_bot.main --dry-run` to confirm project imports and config loading work without runtime errors.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:00:11.686Z"
      },
      {
        "id": "2",
        "title": "Implement CSV input parsing and output logging",
        "description": "Create robust utilities to read posting instructions from a CSV and log results to an output CSV log file.",
        "details": "Implementation details:\n- Define required input columns: `account_name`, `video_path`, `caption`.\n- Implement `read_jobs(csv_path: str) -> list[PostJob]` where `PostJob` is a dataclass with `account_name`, `video_path`, `caption`.\n- Validate CSV: check mandatory columns exist; trim whitespace; skip or flag empty rows.\n- Normalize `video_path` by joining with `video_root_dir` if it is not absolute.\n- Implement `append_log_row(log_path, account, video, status, error=None, timestamp=None)` that appends to CSV, creating header if file does not exist.\n- Ensure logs are flushed after every job for crash resilience.\n- Pseudo-code:\n```python\n# csv_io.py\nfrom dataclasses import dataclass\nimport csv, os, datetime\n\n@dataclass\nclass PostJob:\n    account_name: str\n    video_path: str\n    caption: str\n\n\ndef read_jobs(path: str, video_root_dir: str) -> list[PostJob]:\n    jobs = []\n    with open(path, newline='', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            if not row.get('account_name') or not row.get('video_path'):\n                continue\n            vp = row['video_path']\n            if not os.path.isabs(vp):\n                vp = os.path.join(video_root_dir, vp)\n            jobs.append(PostJob(row['account_name'].strip(), vp, row.get('caption', '')))\n    return jobs\n\n\ndef append_log_row(path: str, account: str, video: str, status: str, error: str | None = None):\n    file_exists = os.path.exists(path)\n    with open(path, 'a', newline='', encoding='utf-8') as f:\n        writer = csv.writer(f)\n        if not file_exists:\n            writer.writerow(['timestamp', 'account', 'video', 'status', 'error'])\n        ts = datetime.datetime.utcnow().isoformat()\n        writer.writerow([ts, account, video, status, error or ''])\n```",
        "testStrategy": "- Unit test `read_jobs` with:\n  - Valid CSV.\n  - Missing columns (expect exception or empty list based on design).\n  - Relative vs absolute video paths.\n- Unit test `append_log_row`:\n  - First write creates header.\n  - Subsequent calls append new rows.\n  - Inspect resulting CSV to match expected line count and fields.\n- Perform an end-to-end dry run reading a small sample CSV and writing a sample log.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:00:20.864Z"
      },
      {
        "id": "3",
        "title": "Design Geelark device control abstraction",
        "description": "Create an abstraction layer to control Geelark cloud phones for screenshots, taps, typing, app launching, and file transfer, independent of the underlying mechanism (RPA, ADB, or API).",
        "details": "Implementation details:\n- Define an interface `GeelarkDeviceController` with methods:\n  - `connect(account_name: str) -> DeviceHandle`\n  - `launch_app(device: DeviceHandle, app_id: str)` (e.g. Instagram)\n  - `tap(device, x: int, y: int)`\n  - `type_text(device, text: str)`\n  - `screenshot(device) -> bytes` (PNG/JPEG bytes)\n  - `swipe(device, x1, y1, x2, y2, duration_ms)`\n  - `upload_file(device, local_path: str, remote_path: str) -> str` (returns remote path or URI).\n- Implement an initial MVP adapter that talks to Geelark via whichever is available first (e.g. ADB over TCP or a Geelark HTTP API). For now, define stub methods that raise `NotImplementedError` but with clear signatures.\n- Provide a mapping from `account_name` to `device_id` (config or simple dict) for the MVP single device.\n- Include sensible timeouts and retry wrappers around network calls.\n- Pseudo-code skeleton:\n```python\n# geelark_control.py\nfrom dataclasses import dataclass\n\n@dataclass\nclass DeviceHandle:\n    id: str\n\n\nclass GeelarkDeviceController:\n    def connect(self, account_name: str) -> DeviceHandle:\n        # map account -> device_id (MVP: single device)\n        raise NotImplementedError\n\n    def launch_app(self, device: DeviceHandle, app_id: str):\n        raise NotImplementedError\n\n    def tap(self, device: DeviceHandle, x: int, y: int):\n        raise NotImplementedError\n\n    def type_text(self, device: DeviceHandle, text: str):\n        raise NotImplementedError\n\n    def screenshot(self, device: DeviceHandle) -> bytes:\n        raise NotImplementedError\n\n    def upload_file(self, device: DeviceHandle, local_path: str, remote_path: str) -> str:\n        raise NotImplementedError\n```\n- Later tasks will fill implementations using the chosen low-level mechanism.",
        "testStrategy": "- Unit test that the interface exists and that stub methods raise `NotImplementedError`.\n- Create a fake/mock implementation `MockGeelarkDeviceController` for testing higher-level logic without real devices.\n- Verify that `account_name` to `device_id` mapping works as expected using the MVP single-device configuration.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:00:29.298Z"
      },
      {
        "id": "4",
        "title": "Implement low-level Geelark control (screenshots, taps, typing, file transfer)",
        "description": "Provide a concrete implementation of the Geelark device control abstraction using the chosen RPA/ADB/API mechanism.",
        "details": "Implementation details:\n- Decide a concrete mechanism based on what Geelark exposes:\n  - If Geelark offers an HTTP API: implement calls like `POST /devices/{id}/tap`, `POST /devices/{id}/type`, `GET /devices/{id}/screenshot`, etc.\n  - If using ADB: use `subprocess` to call `adb -s <serial> shell input tap x y`, `input text`, `screencap -p`, and `adb push` for file transfer.\n- Implement `GeelarkDeviceController` methods:\n  - `connect`: resolve `account_name` to a device identifier (e.g. `device_serial`), possibly via config mapping; validate connectivity.\n  - `launch_app`: `adb shell monkey -p com.instagram.android 1` or equivalent API.\n  - `tap`: execute appropriate tap command.\n  - `type_text`: escape special characters for ADB; for longer captions, implement paste via clipboard if device API supports it.\n  - `screenshot`: capture and return raw bytes; ensure correct image format for Claude Vision.\n  - `upload_file`: transfer video from host to device; return the device-side file path.\n- Add minimal rate limiting to avoid overwhelming Geelark/API.\n- Pseudo-code example (ADB-style):\n```python\nimport subprocess, io\n\nclass AdbGeelarkDeviceController(GeelarkDeviceController):\n    def __init__(self, mapping: dict[str, str]):\n        self.mapping = mapping\n\n    def connect(self, account_name: str) -> DeviceHandle:\n        serial = self.mapping.get(account_name) or next(iter(self.mapping.values()))\n        return DeviceHandle(serial)\n\n    def tap(self, device: DeviceHandle, x: int, y: int):\n        subprocess.run([\"adb\", \"-s\", device.id, \"shell\", \"input\", \"tap\", str(x), str(y)], check=True)\n\n    def screenshot(self, device: DeviceHandle) -> bytes:\n        out = subprocess.check_output([\"adb\", \"-s\", device.id, \"exec-out\", \"screencap\", \"-p\"])\n        return out\n\n    def upload_file(self, device: DeviceHandle, local_path: str, remote_path: str) -> str:\n        subprocess.run([\"adb\", \"-s\", device.id, \"push\", local_path, remote_path], check=True)\n        return remote_path\n```",
        "testStrategy": "- If using ADB: run integration tests against a test device or emulator.\n  - Verify `connect` returns a valid handle.\n  - Call `screenshot` and confirm returned bytes decode as an image.\n  - Call `tap` and `type_text` while observing the device screen.\n  - Transfer a small dummy video file and confirm existence on the device.\n- If using HTTP API: use a mock server to validate request payloads, paths, and error handling.\n- Add negative tests: simulate command/API failures and verify that exceptions are raised and propagated up.",
        "priority": "high",
        "dependencies": [
          "3"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:00:38.036Z"
      },
      {
        "id": "5",
        "title": "Integrate proxy rotation before each post",
        "description": "Implement a simple proxy rotation step that hits the configured rotation URL before each posting job.",
        "details": "Implementation details:\n- Add a `rotate_proxy()` function in a `network_utils.py` module.\n- Use `requests.get(config.proxy_rotation_url, timeout=10)` or equivalent; treat non-2xx responses as failures.\n- Add small backoff and retry (e.g. 3 attempts with exponential backoff) because this is a network call.\n- Pseudo-code:\n```python\nimport time, requests\n\ndef rotate_proxy(url: str, retries: int = 3, base_delay: float = 1.0) -> bool:\n    for attempt in range(retries):\n        try:\n            r = requests.get(url, timeout=10)\n            if 200 <= r.status_code < 300:\n                return True\n        except requests.RequestException:\n            pass\n        time.sleep(base_delay * (2 ** attempt))\n    return False\n```\n- Hook `rotate_proxy()` into the main posting loop: call it before connecting to the Geelark device for each row.\n- Log proxy rotation success/failure per job (but continue posting even if rotation fails if that is acceptable per requirements).",
        "testStrategy": "- Unit test `rotate_proxy` using a requests-mock server returning:\n  - 200: expect success on first attempt.\n  - 500: expect retries and final failure.\n  - Network timeout: expect retries and final failure.\n- In an integration-like test, configure a local HTTP server as rotation URL and verify that it is hit once per job in a multi-row CSV.",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:00:56.695Z"
      },
      {
        "id": "6",
        "title": "Implement Claude Vision client for Instagram UI navigation",
        "description": "Create a module that sends device screenshots and minimal context to Claude Vision and receives structured navigation instructions for the Instagram posting flow.",
        "details": "Implementation details:\n- Use the official Anthropic Python SDK (`anthropic` package) and Claude Vision model.\n- Define a `ClaudeNavigator` class with:\n  - `plan_next_action(screenshot_bytes: bytes, context: dict) -> Action` where `Action` is a dataclass describing an operation such as `tap(x,y)`, `type(text)`, `wait(seconds)`, `verify_posted`.\n- Provide a system prompt that explains the device context (Android Instagram app on a cloud phone), the goal (post a Reel/video with a given caption), and a JSON schema for response.\n- Example pseudo-code:\n```python\nfrom anthropic import Anthropic\nimport base64, json\n\n@dataclass\nclass Action:\n    kind: str  # 'tap', 'type', 'wait', 'done', 'error'\n    x: int | None = None\n    y: int | None = None\n    text: str | None = None\n    seconds: float | None = None\n\n\nclass ClaudeNavigator:\n    def __init__(self, api_key: str):\n        self.client = Anthropic(api_key=api_key)\n\n    def plan_next_action(self, screenshot_bytes: bytes, context: dict) -> Action:\n        img_b64 = base64.b64encode(screenshot_bytes).decode('ascii')\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"input_image\",\n                        \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": img_b64},\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": json.dumps(context),\n                    },\n                ],\n            }\n        ]\n        resp = self.client.messages.create(\n            model=\"claude-3-5-sonnet\",  # example vision-capable model\n            max_tokens=512,\n            messages=messages,\n            system=\"You control an Android Instagram app. Respond ONLY with a JSON object describing the next action to create and publish a video post.\",\n        )\n        action_dict = json.loads(resp.content[0].text)\n        return Action(**action_dict)\n```\n- The `context` should include the current step: e.g. `{\"step\": \"open_plus\", \"caption\": \"...\"}`.\n- Keep actions atomic and loop until `kind == 'done'` or an error is detected.",
        "testStrategy": "- Unit test `ClaudeNavigator` parsing: mock Anthropic client responses with known JSON and ensure `Action` is constructed correctly.\n- Add validation on returned actions (e.g. coordinates within screen bounds, non-empty `text` for `type` actions) and test these validators.\n- For manual testing, feed screenshots of Instagram app (from a real device) and confirm that the model returns sensible next-step actions by logging them without executing on device.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:01:05.856Z"
      },
      {
        "id": "7",
        "title": "Orchestrate Instagram posting flow with device control and Claude Vision",
        "description": "Combine CSV jobs, Geelark control, proxy rotation, and Claude Vision navigation to automate the full Instagram posting flow per row, including caption entry and success verification for one device (MVP).",
        "details": "Implementation details:\n- Implement a high-level function `run_post_job(job: PostJob, config: Config, controller: GeelarkDeviceController, navigator: ClaudeNavigator)` that:\n  1) Rotates proxy.\n  2) Connects to the Geelark device for `job.account_name`.\n  3) Ensures the Instagram app is running (`launch_app`).\n  4) Transfers the video file to the device via `upload_file` and records the device path.\n  5) Enters a loop to perform the posting flow:\n     - Take a `screenshot`.\n     - Provide `context` to Claude, including:\n       - `goal`: \"Post the specified video to this Instagram account as a Reel or standard video post.\"\n       - `step_state`: track state such as `{\"video_uploaded\": false, \"caption_pasted\": false}`.\n       - `video_device_path` and `caption`.\n     - Receive `Action` from `ClaudeNavigator`.\n     - Map `Action` to `GeelarkDeviceController` calls (`tap`, `type_text`, etc.).\n     - Track timeouts and max steps (e.g. 30 steps) to avoid infinite loops.\n  6) After `Action.kind == 'done'`, confirm success by having Claude inspect a final screenshot with a `verify_posted` context.\n- Ensure that errors (exceptions, invalid actions, timeouts) raise a `PostJobError` that carries a human-readable message.\n- Pseudo-code skeleton:\n```python\ndef run_post_job(job, config, controller, navigator):\n    rotate_proxy(config.proxy_rotation_url)\n    device = controller.connect(job.account_name)\n    controller.launch_app(device, app_id=\"com.instagram.android\")\n    remote_video_path = controller.upload_file(device, job.video_path, \"/sdcard/Download/post_video.mp4\")\n\n    state = {\"video_uploaded\": False, \"caption_pasted\": False, \"remote_video_path\": remote_video_path}\n    for step in range(30):\n        screenshot = controller.screenshot(device)\n        context = {\"goal\": \"post_video\", \"caption\": job.caption, \"state\": state}\n        action = navigator.plan_next_action(screenshot, context)\n        if action.kind == \"tap\":\n            controller.tap(device, action.x, action.y)\n        elif action.kind == \"type\":\n            controller.type_text(device, action.text)\n        elif action.kind == \"wait\":\n            time.sleep(action.seconds)\n        elif action.kind == \"done\":\n            break\n        else:\n            raise PostJobError(f\"Unknown action: {action.kind}\")\n\n    # final verification screenshot\n    final_shot = controller.screenshot(device)\n    verify_action = navigator.plan_next_action(final_shot, {\"goal\": \"verify_posted\"})\n    if verify_action.kind != \"done\":\n        raise PostJobError(\"Unable to verify post was successful\")\n```\n- Make the orchestrator initially target MVP: one device and single job; then scale to loop over all jobs from CSV in `main.py`.\n- Capture and return a success/failure status and error message to the caller for logging.",
        "testStrategy": "- Implement integration tests in a `--dry-run` mode where `GeelarkDeviceController` is a mock and `ClaudeNavigator` is replaced by a deterministic fake that returns a scripted sequence of actions; verify steps executed in correct order.\n- On a real Geelark device, manually run one job and visually confirm that Instagram opens, video is selected, caption is filled, and post is shared.\n- Test failure paths: simulate `upload_file` failure, invalid actions from navigator, and assert that errors propagate to logging.\n- Verify that the loop stops when max steps are reached and logs an appropriate error.",
        "priority": "high",
        "dependencies": [
          "2",
          "4",
          "5",
          "6"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:01:14.920Z"
      },
      {
        "id": "8",
        "title": "Handle login prompts, captchas, and rate limits",
        "description": "Add edge-case handling for Instagram login requests, captchas via 2Captcha, and rate-limit detection with backoff and retry.",
        "details": "Implementation details:\n- Extend Claude prompts to explicitly ask it to identify when the screen shows:\n  - A login screen.\n  - A captcha challenge.\n  - A rate-limit or \"try again later\" message.\n- In `ClaudeNavigator`, allow an `Action.kind` of `\"login_required\"`, `\"captcha\"`, or `\"rate_limited\"` with additional metadata if needed.\n- Implement logic in the orchestrator:\n  - `login_required`: for MVP, either skip the job and log `login_required`, or if credentials are available in config, allow navigator-guided login by providing `username`/`password` in context.\n  - `captcha`: integrate 2Captcha by:\n    - Taking a screenshot of the captcha area (or whole screen) and sending to 2Captcha's image API.\n    - Polling for the solved text and then issuing `type_text` or `tap` actions accordingly.\n  - `rate_limited`: pause posting for a configurable cooldown (e.g. 10–30 minutes per account/device) before retrying the current job once; if still rate limited, mark as failed and move on.\n- Pseudo-code snippet for 2Captcha integration:\n```python\nimport requests, time\n\nclass CaptchaSolver:\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n\n    def solve_image(self, image_bytes: bytes) -> str:\n        # send\n        resp = requests.post(\"http://2captcha.com/in.php\", data={\n            \"key\": self.api_key,\n            \"method\": \"base64\",\n            \"body\": base64.b64encode(image_bytes).decode('ascii'),\n            \"json\": 1,\n        })\n        captcha_id = resp.json()[\"request\"]\n        # poll result\n        for _ in range(24):\n            r = requests.get(\"http://2captcha.com/res.php\", params={\n                \"key\": self.api_key,\n                \"action\": \"get\",\n                \"id\": captcha_id,\n                \"json\": 1,\n            })\n            data = r.json()\n            if data[\"status\"] == 1:\n                return data[\"request\"]\n            time.sleep(5)\n        raise TimeoutError(\"Captcha solving timed out\")\n```\n- Log all edge-case events distinctly so they can be monitored later.",
        "testStrategy": "- Unit test captcha solver using mocked 2Captcha endpoints with typical success and timeout responses.\n- Extend fake `ClaudeNavigator` in tests to return `login_required`, `captcha`, and `rate_limited` actions and verify that the orchestrator:\n  - For `login_required`, either skips or performs login based on test configuration.\n  - For `captcha`, calls `CaptchaSolver.solve_image` and then attempts to type the solution.\n  - For `rate_limited`, waits the configured cooldown and retries at most once.\n- Manually induce a login-required state on a test account and confirm that it is handled as designed and logged appropriately.",
        "priority": "medium",
        "dependencies": [
          "6",
          "7"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:01:25.731Z"
      },
      {
        "id": "9",
        "title": "Implement main loop, error handling, and structured logging",
        "description": "Create the main entrypoint that iterates over CSV rows, invokes the posting orchestrator per job, and writes structured logs with status, timestamp, and errors.",
        "details": "Implementation details:\n- In `main.py`, implement:\n  - `load_config()`.\n  - Initialize `GeelarkDeviceController`, `ClaudeNavigator`, and optionally `CaptchaSolver`.\n  - Load jobs via `read_jobs(config.input_csv_path, config.video_root_dir)`.\n  - For each job:\n    - Call `run_post_job` inside a `try/except` block.\n    - On success, call `append_log_row(..., status=\"success\")`.\n    - On failure, log `status=\"fail\"` with the exception message.\n- Use Python `logging` module with JSON-ish log format (e.g. `%(asctime)s %(levelname)s %(message)s`) and include job identifiers.\n- Allow CLI flags/env for:\n  - `--mvp` (single job from CSV).\n  - `--max-jobs` to limit for testing.\n- Pseudo-code:\n```python\ndef main():\n    config = load_config()\n    controller = AdbGeelarkDeviceController(mapping=load_account_device_mapping())\n    navigator = ClaudeNavigator(api_key=config.anthropic_api_key)\n    jobs = read_jobs(config.input_csv_path, config.video_root_dir)\n\n    for i, job in enumerate(jobs):\n        try:\n            run_post_job(job, config, controller, navigator)\n            append_log_row(config.output_log_csv_path, job.account_name, job.video_path, \"success\")\n        except Exception as e:\n            append_log_row(config.output_log_csv_path, job.account_name, job.video_path, \"fail\", str(e))\n```\n- Ensure that an exception in one job does not terminate the loop; always continue to next row.\n- Optionally, add a small random delay between jobs to reduce pattern-like behavior and mitigate rate limits.",
        "testStrategy": "- Use a mock controller and navigator to simulate successful and failing jobs; verify that the main loop continues after failures and that the log CSV contains correct rows.\n- Run end-to-end in a test environment with 2–3 dummy jobs, visually inspect logs and confirm that timestamps and statuses are correct.\n- Intentionally raise an exception inside `run_post_job` for one job and confirm that others are still processed.",
        "priority": "high",
        "dependencies": [
          "2",
          "5",
          "7"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:01:36.963Z"
      },
      {
        "id": "10",
        "title": "MVP validation and scaling to multiple accounts/devices",
        "description": "Validate the MVP by successfully posting one video with caption on a single Geelark device, then extend to handle multiple accounts/devices from the spreadsheet.",
        "details": "Implementation details:\n- MVP validation steps:\n  - Configure one `account_name` in the CSV, one `video_path`, and a simple caption.\n  - Map that account to a Geelark device in the controller configuration.\n  - Run the tool and visually confirm that the video is posted with the correct caption.\n  - Confirm that the output log records `success` for this job.\n- Scaling steps:\n  - Extend account-to-device mapping to support many accounts; use a config file like `devices.yaml` with entries `{account_name, device_id}`.\n  - In `connect(account_name)`, look up the correct `device_id` and fall back to a default or raise an error if unmapped.\n  - If Geelark supports parallel control, optionally add a future-ready abstraction to run jobs concurrently (e.g. via a worker pool); for now keep them sequential to minimize complexity.\n  - Ensure that proxy rotation is still called once per job and that rate-limit logic is per account/device.\n- Add documentation (README) describing:\n  - How to prepare the CSV.\n  - How to organize video files.\n  - How to configure API keys and device mappings.\n  - Known edge cases and limitations.",
        "testStrategy": "- For MVP:\n  - Run manual test: verify the real post appears on Instagram from the target account with the expected caption and time.\n  - Check that logs show a single `success` entry with accurate timestamp and video path.\n- For multi-account:\n  - Prepare a CSV with at least 2 accounts mapped to different devices (or sequential runs on same device if that is the Geelark constraint).\n  - Run and verify that each account posts its respective video.\n  - Inspect logs to ensure each row has correct `account`, `video`, and `status`.\n- Perform a small load test with ~10 rows to confirm there are no memory leaks or unhandled exceptions across many iterations.",
        "priority": "medium",
        "dependencies": [
          "4",
          "7",
          "8",
          "9"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:01:47.805Z"
      },
      {
        "id": "11",
        "title": "Fix ADBKeyboard installation on Geelark cloud phones",
        "description": "Pivot to using Appium for Unicode text input on Geelark cloud phones, abandoning the ADBKeyboard approach due to Android 15 incompatibility where the package is hidden at the framework level and cannot be restored via ADB commands.",
        "status": "done",
        "dependencies": [
          "4"
        ],
        "priority": "high",
        "details": "PROBLEM SUMMARY:\n- ADBKeyboard works on Android 12/13 (SDK 31-33) but is blocked on Android 15 (SDK 35) with hidden=true flag at Android framework level\n- All ADB remediation attempts failed: pm uninstall, pm enable, pm unhide, cmd package install-existing all return success but package remains hidden\n- Geelark phones do not provide root access (su returns command not found)\n- ClipboardHelper + KEYCODE_PASTE fallback tested and FAILED - keyboard not visible during paste, text does not appear\n- Only podmindstudio (Android 13) works; reelwisdompod_ and talktrackhub (Android 15) are broken\n\nNEW APPROACH - APPIUM:\nPivot to using Appium for text input, which handles Unicode natively across all Android versions without requiring a custom keyboard IME.\n\nAppium UIAutomator2 driver can:\n- Type text directly into focused fields via send_keys() or mobile:type command\n- Works with Unicode/emojis natively\n- No need for ADBKeyboard, ClipboardHelper, or any IME installation\n- Connects to devices via ADB (same as current setup)\n- Cross-platform Android version support (works on Android 15)\n\nIMPLEMENTATION PLAN:\n1. Set up Appium server (can run locally or on a server)\n   - Install Node.js if not present\n   - npm install -g appium\n   - appium driver install uiautomator2\n\n2. Install Python Appium client:\n   - pip install Appium-Python-Client\n\n3. Connect to Geelark phones via Appium:\n   - Use ADB connection info from Geelark API (same as current flow)\n   - Create Appium driver session with desired capabilities:\n     - platformName: Android\n     - automationName: UiAutomator2\n     - deviceName: {adb_device_id}\n     - noReset: true\n     - appPackage/appActivity for Instagram\n\n4. Update post_reel_smart.py to use Appium:\n   - Create new AppiumController class or add Appium methods to SmartInstagramPoster\n   - Replace type_text() method (lines 225-245) with Appium's send_keys()\n   - Keep ADB for non-typing operations (tap, swipe, screenshot)\n   - Or migrate entirely to Appium for all interactions\n\n5. Testing:\n   - Test on Android 15 device (reelwisdompod_) first\n   - Verify Unicode/emoji typing works correctly\n   - Test full Instagram posting flow\n\nRELEVANT FILES TO MODIFY:\n- post_reel_smart.py: Replace type_text() with Appium-based implementation\n- requirements.txt: Add Appium-Python-Client dependency\n- New file: appium_controller.py (optional, for Appium setup logic)\n\nEXISTING ASSETS:\n- appium-uiautomator2-server.apk already exists in project root\n- package/ directory contains io.appium.settings source (UnicodeIME) but not needed with direct Appium approach\n- ADB connection flow in post_reel_smart.py connect() method can be reused",
        "testStrategy": "- Set up Appium server locally\n- Test Appium connection to reelwisdompod_ (Android 15) device first\n- Create test script that: 1) connects via Appium, 2) opens Instagram, 3) navigates to caption field, 4) types text with emojis using send_keys()\n- Verify text appears correctly in the caption field including Unicode characters and emojis\n- Run full posting flow on Android 15 device\n- Verify same flow still works on Android 13 device (podmindstudio) for backwards compatibility\n- Compare posting success rates before/after migration",
        "subtasks": [
          {
            "id": 3,
            "title": "Complete ADBKeyboard remediation research and document Android 15 blocker",
            "description": "Document the comprehensive ADBKeyboard remediation attempts and confirm that Android 15 hidden=true state is an unresolvable blocker without root access, leading to pivot to Appium.",
            "dependencies": [
              1,
              2
            ],
            "details": "All ADBKeyboard remediation approaches exhausted:\n- pm uninstall/install: Returns success but package remains hidden\n- cmd package install-existing: Returns success but pm path empty\n- pm enable/unhide: Requires root access not available on Geelark\n- Alternative keyboards: Same hidden=true issue affects new installs\n- ClipboardHelper fallback: FAILED - keyboard not visible during paste\n- Root API: Error 43016 indicates phones don't support root\n\nConclusion: ADBKeyboard approach is fundamentally incompatible with Android 15 on Geelark phones. Pivoting to Appium which handles Unicode typing natively without requiring IME installation.",
            "status": "done",
            "testStrategy": "Document all attempted remediation commands and their results. Confirm Android version correlation (SDK 35 = broken, SDK <= 33 = working).",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Set up Appium server and install UiAutomator2 driver",
            "description": "Install and configure Appium server locally with UiAutomator2 driver for Android automation that supports native Unicode text input across all Android versions.",
            "dependencies": [],
            "details": "Installation steps:\n1. Verify Node.js is installed (node --version), install if needed from https://nodejs.org\n2. Install Appium globally: npm install -g appium\n3. Install UiAutomator2 driver: appium driver install uiautomator2\n4. Verify installation: appium driver list (should show uiautomator2)\n5. Start Appium server: appium --allow-insecure chromedriver_autodownload\n6. Verify server is running on http://localhost:4723\n\nServer configuration:\n- Default port: 4723\n- May need to configure ANDROID_HOME environment variable pointing to Android SDK\n- May need to ensure platform-tools (adb) is in PATH\n\nFiles to create:\n- requirements.txt: Add 'Appium-Python-Client>=3.0.0'\n- Optional: appium_setup.py script to verify/start Appium service\n<info added on 2025-12-11T04:22:32.422Z>\nCOMPLETED SETUP STATUS:\n- Appium version: 3.1.2 installed globally via npm\n- UiAutomator2 driver: installed via appium driver install uiautomator2\n- Android SDK: ANDROID_HOME=C:/Users/asus/Downloads/android-sdk with platform-tools symlinked\n- Successfully connected to Geelark cloud phone at 98.98.125.37:20865 running Android 15 (SDK 35)\n- Connection verified via test_appium.py script which captured screenshot (appium_test.png) proving connection works\n- Appium-Python-Client needs to be added to requirements.txt (currently only has python-dotenv, requests, anthropic)\n- Platform version confirmed via driver.capabilities after successful Remote connection to http://127.0.0.1:4723\n</info added on 2025-12-11T04:22:32.422Z>",
            "status": "done",
            "testStrategy": "1) Run 'appium --version' to verify installation\n2) Run 'appium driver list' to verify uiautomator2 is installed\n3) Start Appium server and verify it responds on localhost:4723\n4) Create simple test script that imports appium and verifies client library version",
            "updatedAt": "2025-12-11T04:21:52.916Z",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Appium connection to Geelark cloud phones",
            "description": "Create AppiumController class that connects to Geelark devices via Appium using existing ADB connection info from GeelarkClient, enabling Unicode text input on Android 15.",
            "dependencies": [
              4
            ],
            "details": "Implementation in new file appium_controller.py:\n\nCreate AppiumController class with methods:\n- connect(): Get phone info from GeelarkClient, start phone, enable ADB, connect via Appium with UiAutomator2Options\n- type_text(text): Use driver.switch_to.active_element.send_keys(text) for Unicode support\n- close(): Quit Appium driver session\n\nKey Appium capabilities:\n- platformName: 'Android'\n- automationName: 'UiAutomator2'\n- deviceName: ADB device string (ip:port)\n- noReset: True (preserve app state)\n- newCommandTimeout: 300\n\nIntegration with existing code:\n- Reuse GeelarkClient for phone discovery and ADB setup\n- Reuse ADB connection logic from post_reel_smart.py lines 115-170\n- Add error handling for Appium connection failures",
            "status": "done",
            "testStrategy": "1) Connect to reelwisdompod_ (Android 15) via Appium\n2) Verify driver session is established\n3) Run driver.page_source to confirm UI access\n4) Take screenshot via driver.get_screenshot_as_png()\n5) Verify connection works on both Android 15 and Android 13 devices",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T04:35:32.773Z"
          },
          {
            "id": 6,
            "title": "Update post_reel_smart.py to use Appium for text input",
            "description": "Modify the SmartInstagramPoster class to use Appium's send_keys() for typing captions instead of ADBKeyboard broadcast, while keeping ADB for other operations.",
            "dependencies": [
              4,
              5
            ],
            "details": "Changes to post_reel_smart.py:\n\n1) Add Appium imports at top:\nfrom appium import webdriver\nfrom appium.options.android import UiAutomator2Options\n\n2) Add Appium driver initialization in connect() method\n\n3) Replace type_text() method (lines 225-245) with Appium-based implementation:\n- Use self.appium_driver.switch_to.active_element.send_keys(text)\n- Remove typing_method check since Appium works universally\n- Handle emojis and Unicode natively\n\n4) Add cleanup for Appium driver in disconnect/cleanup\n\n5) Keep existing ADB methods for tap(), swipe(), screenshot, etc.\n\nAlternative: Hybrid approach - try Appium first, fall back to ADBKeyboard if Appium unavailable for Android 13 devices",
            "status": "done",
            "testStrategy": "1) Start Appium server\n2) Run test on Android 15 device (reelwisdompod_) with caption containing emojis\n3) Verify caption appears correctly in Instagram caption field\n4) Run full posting flow and verify success\n5) Run same test on Android 13 device (podmindstudio) for backwards compatibility",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T04:35:36.719Z"
          },
          {
            "id": 7,
            "title": "Add Appium dependencies and update requirements.txt",
            "description": "Add Appium-Python-Client and any other required dependencies to the project requirements file.",
            "dependencies": [],
            "details": "Update requirements.txt to add:\nAppium-Python-Client>=3.0.0\nselenium>=4.0.0\n\nInstallation command: pip install Appium-Python-Client\n\nVerify installation:\nimport appium\nprint(appium.__version__)\n\nNote: Appium-Python-Client depends on selenium, which will be installed automatically.\n\nPreserve existing dependencies:\n- anthropic (for Claude API)\n- requests (for HTTP calls)\n- python-dotenv (for .env loading)",
            "status": "done",
            "testStrategy": "1) Run pip install -r requirements.txt\n2) Verify no dependency conflicts\n3) Test import: python -c \"from appium import webdriver; print('OK')\"\n4) Verify existing imports still work",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T04:22:51.801Z"
          },
          {
            "id": 8,
            "title": "Test full Instagram posting flow with Appium on Android 15",
            "description": "Perform end-to-end testing of the complete Instagram Reel posting workflow using Appium for text input on an Android 15 device to validate the pivot from ADBKeyboard.",
            "dependencies": [
              4,
              5,
              6,
              7
            ],
            "details": "Test procedure:\n\n1) Pre-requisites:\n- Appium server running\n- Android 15 device available (reelwisdompod_ or talktrackhub)\n- Test video file and caption with Unicode/emojis prepared\n\n2) Test execution:\nStart Appium server in terminal 1: appium\nRun posting script in terminal 2: python post_reel_smart.py reelwisdompod_ test_video.mp4 \"Test caption with emojis 🎉✨🔥\"\n\n3) Verification steps:\n- Phone connects successfully\n- Instagram app opens\n- Video upload works (existing ADB-based file transfer)\n- Caption field is focused\n- Appium types caption including emojis correctly\n- Post is shared successfully\n- Verify post appears on Instagram with correct caption\n\n4) Performance comparison:\n- Time to type caption: Appium vs ADBKeyboard\n- Overall posting time\n- Success rate over multiple posts",
            "status": "pending",
            "testStrategy": "1) Execute full posting flow on Android 15 device with emoji-rich caption\n2) Verify caption appears correctly on published post\n3) Repeat test 3-5 times to verify consistency\n4) Test on Android 13 device for backwards compatibility\n5) Run batch_post.py with mix of Android versions to verify multi-device support",
            "parentId": "undefined"
          },
          {
            "id": 1,
            "title": "Research Android package manager ghost package and signature mismatch behaviors (cloud phones)",
            "description": "Investigate how Android handles ghost/orphaned package entries and INSTALL_FAILED_UPDATE_INCOMPATIBLE errors, especially on non-rootable or cloud-hosted devices like Geelark, and document feasible ADB-only remedies.",
            "dependencies": [],
            "details": "Use Perplexity to search Android developer docs, StackOverflow, and XDA for: (1) causes and fixes of INSTALL_FAILED_UPDATE_INCOMPATIBLE when pm uninstall fails; (2) techniques to clear or bypass ghost/orphaned packages without root (e.g., user 0 uninstall, package clear, disabling users, testharness, or resetting app state); (3) behavior differences for system apps vs. user apps in /system/app and /system/priv-app. Summarize which approaches are viable when you only have adb shell and no root, and call out any device-OEM-specific caveats relevant to cloud/virtual devices.\n<info added on 2025-12-11T02:49:23.733Z>\nBased on the codebase analysis and research findings, here is the new text to append:\n\nResearch findings for ADB-only ghost package remediation on Geelark cloud phones:\n\n1) Ghost package removal without root: Use `pm uninstall --user 0 com.android.adbkeyboard` (do NOT use -k flag as it keeps data and leaves ghost state). This removes the package for the current user even when standard pm uninstall fails with DELETE_FAILED_INTERNAL_ERROR.\n\n2) Restoring orphaned system apps: If ADBKeyboard was previously a system app (like on podmindstudio at /system/app/AdbKeyboard/AdbKeyboard.apk), use `cmd package install-existing com.android.adbkeyboard` to restore it from the system image.\n\n3) Alternative for DELETE_FAILED_INTERNAL_ERROR: Try `pm disable-user --user 0 com.android.adbkeyboard` first to disable the ghost entry before attempting uninstall.\n\n4) Detecting ghost packages: Compare output of `pm list packages` (installed) vs `pm list packages -u` (includes uninstalled-but-retained). Packages appearing only in -u output are ghosts.\n\n5) Fallback typing without ADBKeyboard: The codebase already has ClipboardHelper (setup_clipboard_helper.py) which sets clipboard via `am start -n com.geelark.clipboard/.CopyActivity -a com.geelark.clipboard.COPY --es base64 <b64text>`. After setting clipboard, use `input keyevent 279` (KEYCODE_PASTE) to paste content. This approach supports Unicode and emojis without requiring ADBKeyboard.\n\n6) Current setup_adbkeyboard.py (line 102) uses basic `pm uninstall` which fails on ghost packages. Fix requires updating to use `pm uninstall --user 0` approach.\n\nSources: XDA Forums, bayton.org, droidwin.com\n</info added on 2025-12-11T02:49:23.733Z>",
            "status": "done",
            "testStrategy": null,
            "updatedAt": "2025-12-11T02:49:43.751Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Probe Geelark cloud phones for ADBKeyboard package state and system app presence",
            "description": "Systematically inspect all relevant Geelark devices to understand current ADBKeyboard installation state, including ghost entries and potential system app copies.",
            "dependencies": [
              1
            ],
            "details": "On each Geelark phone (podmindstudio, miccliparchive, reelwisdompod_, talktrackhub), run a scripted adb diagnostic sequence: (1) `pm list packages | grep adbkeyboard`; (2) `pm list packages -s` and `-3` to see if it’s system or user; (3) `pm path com.android.adbkeyboard`; (4) `cmd package resolve-activity` and `dumpsys package com.android.adbkeyboard` to detect ghost entries or disabled states; (5) search filesystem for the APK (e.g., `/system/app`, `/system/priv-app`, `/product/app`) using `ls` patterns where allowed; (6) check `settings get secure default_input_method` and `ime list -a` to see if the IME is registered but disabled. Capture outputs in logs per device and infer whether each device has a system app copy, a broken/ghost entry, or no trace at all.\n<info added on 2025-12-11T02:52:30.810Z>\nDiagnosis Results:\n\n1) podmindstudio: INSTALLED and working - System app located at /system/app/AdbKeyboard/AdbKeyboard.apk. IME properly set to com.android.adbkeyboard/.AdbIME. No remediation needed.\n\n2) miccliparchive: GHOST PACKAGE - APK exists in /system/app but package uninstalled for user 0. Current IME set to Google keyboard (com.google.android.inputmethod.latin). Package appears in `pm list packages -u` but not in `pm list packages`. Remediation: Use `cmd package install-existing com.android.adbkeyboard` to restore system app for current user, then set IME.\n\n3) reelwisdompod_: GHOST PACKAGE - APK exists in /system/app but package uninstalled for user 0. IME setting still points to ADBKeyboard but keyboard non-functional since package not installed for user. Remediation: Same as miccliparchive - use `cmd package install-existing com.android.adbkeyboard` to restore.\n\n4) talktrackhub: NOT INSTALLED - Clean slate, no ADBKeyboard APK anywhere on the filesystem. No ghost package entries. Remediation options: (a) Copy APK from podmindstudio via `adb pull/push` and install, or (b) Use clipboard-based text input as fallback.\n\nFix Strategy for setup_adbkeyboard.py: Add detection logic to differentiate ghost package vs clean slate states. For ghost packages (miccliparchive, reelwisdompod_), use `cmd package install-existing com.android.adbkeyboard` instead of standard pm install. For clean installs (talktrackhub), either pull APK from working phone or use local ADBKeyboard.apk with pm install.\n</info added on 2025-12-11T02:52:30.810Z>",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-11T02:52:49.620Z"
          }
        ],
        "updatedAt": "2025-12-12T02:01:58.948Z"
      },
      {
        "id": "12",
        "title": "Investigate and fix empty Appium page_source on Android 15 Instagram sessions",
        "description": "Debug and instrument the Appium-based Android 15 setup so that page_source / dump_ui() returns a non-empty, correctly structured UI hierarchy for the Instagram app, and determine whether issues stem from app launch, hierarchy generation, or XML parsing.",
        "details": "Implementation plan:\n\n1. **Set up a focused Android 15/Appium debug harness**\n- Create a standalone Python script or test module (e.g. `debug/appium_source_debug.py`) that:\n  - Connects to the same Geelark Android 15 device configuration used in Task 11.\n  - Uses the same Appium capabilities (platformName, platformVersion, deviceName/UDID, automationName=UiAutomator2, appPackage/appActivity for Instagram, noReset, etc.).\n  - Logs all capabilities and the Appium server version at startup for reproducibility.\n- Ensure the harness is runnable independently from the main posting flow to speed up iteration.\n\n2. **Compare Appium page_source vs. uiautomator dump formats**\n- Use Appium’s `driver.page_source` and log the raw return value to a file (e.g. `artifacts/appium_source_raw.xml`) for multiple states: before Instagram launch, after launch, and after navigating to a known screen.[2]\n- On the same device and screen, use `adb shell uiautomator dump /sdcard/view.xml && adb pull /sdcard/view.xml artifacts/uiautomator_view.xml` and compare:\n  - Root element tag names and attributes (`hierarchy`, `node`, bounds, text, resource-id, content-desc).\n  - Character encoding and XML declaration.\n  - Presence/absence of expected views (e.g., Instagram home feed, buttons, bottom nav).\n- Document differences in a short markdown note (`docs/appium_vs_uiautomator.md`), highlighting any fields Appium normalizes or omits and confirming that Appium is returning **application hierarchy XML**, not a raw uiautomator dump.[2]\n\n3. **Verify that Instagram is truly launching and in foreground**\n- From the debug harness, add explicit steps:\n  - Call `driver.start_activity(appPackage, appActivity)` (or equivalent) and wait for a few seconds.\n  - Use `adb shell dumpsys window windows | grep -E 'mCurrentFocus|mFocusedApp'` to verify that the Instagram activity is in the foreground; log this output.\n  - Capture a screenshot via Appium (`driver.get_screenshot_as_png()`) and save to `artifacts/instagram_launch.png`; visually confirm the app is open.\n- If Appium connects but Instagram is not foregrounded, log this and add retries/explicit waits (e.g. wait for known accessibility id or resource-id) before calling `page_source`.\n\n4. **Instrument the page_source / dump_ui() call itself**\n- Wrap `driver.page_source` and any `dump_ui()` helper used in Task 11 in a small utility (e.g. `debug/get_hierarchy.py`) that:\n  - Measures call latency.\n  - Catches and logs exceptions.\n  - Logs the length of the returned XML string and the first 500–1000 characters.\n- Add verbose Appium server logging (log level `debug`) for these calls, capturing:\n  - The `Get Page Source` requests and responses.\n  - Any UiAutomator2/Android errors when traversing the hierarchy.\n- If `page_source` returns an empty hierarchy but no exception, investigate whether this is a known limitation with background apps, webviews, or Android 15 specifics.[1][6]\n\n5. **Check for webview / context or invisible-element issues**\n- Enumerate contexts using `driver.contexts` and log them; if a `WEBVIEW_` context exists for Instagram, switch contexts and compare `page_source` results to the native context.\n- Confirm whether the expected elements are off-screen or lazily created (e.g., lists or RecyclerViews)[3]; scroll a small amount and re-fetch `page_source` to see if the hierarchy populates.\n- Ensure that the harness requests **native context** when expecting native XML, and document how Instagram’s UI composition (native vs webview) affects what Appium can see.[4]\n\n6. **Rule out XML parsing issues in our code**\n- If Appium returns non-empty XML but our `dump_ui()` / parser reports no nodes, add unit-level diagnostics:\n  - Create a minimal parser module (e.g. `ui_parsing/xml_utils.py`) that loads the raw Appium XML using both `xml.etree.ElementTree` and `lxml` (if available) to handle any namespace/encoding quirks.\n  - Log any parsing errors, invalid characters, or namespace prefixes.\n  - Add defensive parsing: strip BOMs, normalize encoding to UTF‑8, and handle default namespaces.\n- Implement a small CLI (`python -m ui_parsing.debug_parse artifacts/appium_source_raw.xml`) that prints root tag, number of nodes, and a few sample attributes to quickly validate parsing.\n\n7. **Constrain work to Android 15 devices**\n- Ensure the harness inspects the device’s SDK level from `adb shell getprop ro.build.version.sdk` and asserts it is 35 (Android 15); otherwise, exit with a clear message.\n- If needed, parameterize the target device but keep the scope of this task to documenting and resolving the Android 15 behavior (other OS versions can be future work).\n\n8. **Output and documentation**\n- Produce a short troubleshooting doc `docs/android15_appium_empty_source.md` summarizing:\n  - Root cause(s): app not foregrounded, context mismatch, Android 15 UiAutomator behavior, or XML parsing bug.\n  - The final, recommended way to:\n    - Confirm Instagram is open.\n    - Fetch reliable page source.\n    - Parse and inspect the hierarchy.\n  - Any Appium capabilities or flags that improved results (e.g., waitForIdleTimeout, disableWindowAnimation, etc., if changed).\n- Expose any reusable utilities (e.g., `get_page_source_debug()`, `assert_instagram_foreground()`) in a `debug_utils` module so other tasks (like Task 11 and orchestrator work) can reuse them.\n",
        "testStrategy": "1. **Environment and connectivity sanity checks**\n- Run the debug harness against an Android 15 Geelark device and verify:\n  - Appium session is created without errors.\n  - Device SDK level is detected as 35; the script exits with an error on non‑15 devices.\n\n2. **Instagram launch verification**\n- Execute the harness with Instagram launch enabled and confirm:\n  - `dumpsys window` logs show an Instagram activity in `mCurrentFocus`/`mFocusedApp`.\n  - The saved screenshot clearly shows Instagram in the foreground.\n\n3. **Page source vs uiautomator comparison**\n- On the same screen, generate both `artifacts/appium_source_raw.xml` and `artifacts/uiautomator_view.xml`.\n- Manually inspect or script-compare them to confirm:\n  - Non-empty XML in both files.\n  - Similar numbers of nodes and presence of expected Instagram UI elements.\n\n4. **XML parsing validation**\n- Run the XML parser CLI against `appium_source_raw.xml` and verify it prints:\n  - Correct root element name.\n  - A positive node count (> 0).\n  - At least a few nodes with sensible attributes (e.g., text/resource-id not all empty).\n- Intentionally corrupt the XML file (e.g., truncate it) and confirm the parser reports clear parsing errors instead of silently returning zero nodes.\n\n5. **Context and visibility behavior tests**\n- From the harness, log `driver.contexts` and switch between native and any webview context, calling `page_source` in each and confirming non-empty output where expected.\n- Scroll within Instagram and re-run `page_source`, verifying the hierarchy updates and that elements entering/leaving the visible region appear/disappear from the XML.\n\n6. **Regression guard for empty source condition**\n- Add an automated check in the harness that fails if `page_source` length is below a small threshold (e.g., < 1 KB) while Instagram is reported as foreground.\n- Run the harness multiple times (at least 5) and confirm the check consistently passes on Android 15.\n\n7. **Documentation review**\n- Have a team member follow `docs/android15_appium_empty_source.md` on a fresh environment and verify they can reproduce the debug steps and obtain non-empty page source and parsed node counts without additional help.",
        "status": "done",
        "dependencies": [
          "4",
          "11"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:02:09.371Z"
      },
      {
        "id": "13",
        "title": "Apply Appium stability fixes with extended timeouts and crash recovery",
        "description": "Improve Appium connection reliability in post_reel_smart.py by adding missing timeout capabilities, increasing existing timeouts, implementing phone restart logic for UiAutomator2 crashes, and creating a typed exception for startup failures.",
        "details": "## Implementation Details\n\n### 1. Create typed UiAutomatorStartupError exception (top of file, after imports ~line 35)\n\n```python\nclass UiAutomatorStartupError(Exception):\n    \"\"\"Raised when UiAutomator2 fails to start on the device\"\"\"\n    pass\n```\n\n### 2. Update connect_appium() function (lines 730-763) with new capabilities\n\nAdd these capabilities to the `options` object:\n\n```python\ndef connect_appium(self, retries=3):\n    \"\"\"Connect Appium driver - REQUIRED for automation to work\"\"\"\n    print(\"Connecting Appium driver...\")\n\n    options = UiAutomator2Options()\n    options.platform_name = \"Android\"\n    options.automation_name = \"UiAutomator2\"\n    options.device_name = self.device\n    options.udid = self.device\n    options.no_reset = True\n    options.new_command_timeout = 60\n    \n    # Extended timeouts for stability (Android 15 devices need longer)\n    options.set_capability(\"appium:uiautomator2ServerLaunchTimeout\", 90000)  # NEW: 90s (was missing, defaulted to 30s)\n    options.set_capability(\"appium:uiautomator2ServerInstallTimeout\", 120000)  # INCREASED: 120s (was 60s)\n    options.set_capability(\"appium:adbExecTimeout\", 120000)  # INCREASED: 120s (was 30s)\n    options.set_capability(\"appium:androidDeviceReadyTimeout\", 60000)  # NEW: 60s device ready wait\n\n    last_error = None\n    for attempt in range(retries):\n        try:\n            self.appium_driver = webdriver.Remote(\n                command_executor=APPIUM_SERVER,\n                options=options\n            )\n            platform_ver = self.appium_driver.capabilities.get('platformVersion', 'unknown')\n            print(f\"  Appium connected! (Android {platform_ver})\")\n            return True\n        except Exception as e:\n            last_error = e\n            print(f\"  Appium connection failed (attempt {attempt + 1}/{retries}): {e}\")\n            self.appium_driver = None\n            \n            # Check if UiAutomator2 crashed - may need phone restart\n            if self.is_uiautomator2_crash(e):\n                print(f\"  [RECOVERY] UiAutomator2 crash detected, attempting phone restart...\")\n                self._restart_phone_for_recovery()\n            \n            if attempt < retries - 1:\n                print(f\"  Retrying in 15 seconds...\")  # INCREASED: 15s (was 5s)\n                time.sleep(15)\n\n    # All retries failed - raise typed exception\n    raise UiAutomatorStartupError(f\"Appium connection failed after {retries} attempts: {last_error}\")\n```\n\n### 3. Add phone restart recovery method (new method in SmartInstagramPoster class)\n\nAdd this method after `reconnect_appium()` (around line 84):\n\n```python\ndef _restart_phone_for_recovery(self):\n    \"\"\"Restart the Geelark phone to recover from UiAutomator2 crash\"\"\"\n    if not self.phone_id:\n        print(\"    Cannot restart phone - phone_id not set\")\n        return False\n    \n    try:\n        print(\"    Stopping phone...\")\n        self.client.stop_phone(self.phone_id)\n        time.sleep(5)\n        \n        print(\"    Starting phone...\")\n        self.client.start_phone(self.phone_id)\n        \n        # Wait for phone to boot (similar to connect() logic)\n        print(\"    Waiting for phone to boot...\")\n        for i in range(60):\n            time.sleep(2)\n            status_result = self.client.get_phone_status([self.phone_id])\n            items = status_result.get(\"successDetails\", [])\n            if items and items[0].get(\"status\") == 0:\n                print(f\"    Phone ready after restart! (took ~{(i+1)*2}s)\")\n                break\n            if i % 5 == 0:\n                print(f\"    Booting... ({(i+1)*2}s)\")\n        else:\n            print(\"    Warning: Phone boot timeout after restart\")\n            return False\n        \n        # Re-enable ADB after restart\n        time.sleep(3)\n        print(\"    Re-enabling ADB...\")\n        self.client.enable_adb(self.phone_id)\n        time.sleep(5)\n        \n        # Reconnect ADB\n        adb_info = self.client.get_adb_info(self.phone_id)\n        self.device = f\"{adb_info['ip']}:{adb_info['port']}\"\n        password = adb_info['pwd']\n        \n        import subprocess\n        subprocess.run([ADB_PATH, \"connect\", self.device], capture_output=True)\n        self.adb(f\"glogin {password}\")\n        time.sleep(3)\n        \n        print(\"    Phone restart recovery complete\")\n        return True\n        \n    except Exception as e:\n        print(f\"    Phone restart failed: {e}\")\n        return False\n```\n\n### 4. Update reconnect_appium() to use new exception (line 74-84)\n\n```python\ndef reconnect_appium(self):\n    \"\"\"Reconnect Appium driver after UiAutomator2 crash\"\"\"\n    print(\"  [RECOVERY] Reconnecting Appium driver...\")\n    try:\n        if self.appium_driver:\n            self.appium_driver.quit()\n    except:\n        pass\n    self.appium_driver = None\n    time.sleep(2)\n    try:\n        return self.connect_appium()\n    except UiAutomatorStartupError:\n        # If reconnect also fails, try phone restart\n        if self._restart_phone_for_recovery():\n            return self.connect_appium()\n        raise\n```\n\n### Summary of Changes\n\n| Item | Before | After |\n|------|--------|-------|\n| `uiautomator2ServerLaunchTimeout` | Missing (30s default) | 90000ms |\n| `uiautomator2ServerInstallTimeout` | 60000ms | 120000ms |\n| `adbExecTimeout` | 30000ms | 120000ms |\n| `androidDeviceReadyTimeout` | Missing | 60000ms |\n| Retry sleep | 5s | 15s |\n| Phone restart on crash | Not implemented | Implemented |\n| Typed exception | Generic Exception | UiAutomatorStartupError |\n\n### Files Modified\n- `post_reel_smart.py`: Add exception class, update `connect_appium()`, add `_restart_phone_for_recovery()`, update `reconnect_appium()`",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Tests for Exception Class\n- Verify `UiAutomatorStartupError` can be raised and caught\n- Verify it inherits from `Exception`\n- Verify error message is preserved correctly\n\n### 2. Timeout Configuration Tests\n- Start Appium with a mock device and verify the capabilities are set correctly:\n  - `uiautomator2ServerLaunchTimeout == 90000`\n  - `uiautomator2ServerInstallTimeout == 120000`\n  - `adbExecTimeout == 120000`\n  - `androidDeviceReadyTimeout == 60000`\n- Log the capabilities object before connection to verify values\n\n### 3. Retry Logic Tests\n- Mock Appium connection failures and verify:\n  - Retry happens 3 times\n  - Sleep between retries is 15 seconds (measure with time.time())\n  - `UiAutomatorStartupError` is raised after all retries fail\n\n### 4. Phone Restart Recovery Tests\n- Mock `is_uiautomator2_crash()` to return `True`\n- Verify `_restart_phone_for_recovery()` is called\n- Mock GeelarkClient methods (`stop_phone`, `start_phone`, `get_phone_status`, `enable_adb`, `get_adb_info`)\n- Verify the correct sequence of recovery calls\n\n### 5. Integration Test with Real Device\n```bash\n# Test on a Geelark Android 15 device\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\n\nposter = SmartInstagramPoster('test_phone_name')\nposter.connect()\n\n# Verify capabilities by checking the driver\ncaps = poster.appium_driver.capabilities\nprint(f'Platform: {caps.get(\\\"platformVersion\\\")}')\nprint(f'Appium connected successfully with extended timeouts')\n\nposter.cleanup()\n\"\n```\n\n### 6. Crash Recovery Simulation\n- Force a UiAutomator2 crash by killing the server process\n- Verify the recovery logic kicks in:\n  ```bash\n  # In a separate terminal while test is running:\n  adb shell \"pkill -f uiautomator\"\n  ```\n- Observe that phone restart and Appium reconnection occur\n\n### 7. End-to-End Test\n- Run full posting flow: `python post_reel_smart.py <phone> <video> <caption>`\n- Monitor logs for timeout-related errors\n- Verify no more \"30s timeout\" errors appear\n- Verify successful connection even under slow network conditions\n\n### 8. Regression Testing\n- Run the existing test suite to ensure no regressions\n- Verify `posting_scheduler.py` still works with the updated `connect_appium()`\n- Test with multiple concurrent phones to verify stability",
        "status": "done",
        "dependencies": [
          "11",
          "12"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:02:20.479Z"
      },
      {
        "id": "14",
        "title": "Analyze overnight scheduler run results (Dec 11-12)",
        "description": "Review batch_results_20251211*.csv files and scheduler logs to compute comprehensive metrics including success rates, error patterns, time correlations, and priority fixes needed.",
        "details": "## Implementation Details\n\n### 1. Create analysis script `analyze_scheduler_results.py`\n\n```python\nimport os\nimport csv\nimport json\nfrom datetime import datetime\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple, Optional\nimport statistics\n\nclass SchedulerAnalyzer:\n    def __init__(self, csv_pattern: str = \"batch_results_20251211*.csv\"):\n        self.csv_pattern = csv_pattern\n        self.records = []\n        \n    def load_data(self):\n        \"\"\"Load all matching CSV files\"\"\"\n        import glob\n        for filepath in glob.glob(self.csv_pattern):\n            with open(filepath, 'r', encoding='utf-8') as f:\n                reader = csv.DictReader(f)\n                for row in reader:\n                    row['source_file'] = filepath\n                    row['timestamp_parsed'] = datetime.fromisoformat(row['timestamp']) if row.get('timestamp') else None\n                    self.records.append(row)\n```\n\n### 2. Metric Calculations\n\n#### Success Rate by Account\n```python\ndef success_rate_by_account(self) -> Dict[str, dict]:\n    \"\"\"Calculate success/fail/error counts per phone/account\"\"\"\n    by_account = defaultdict(lambda: {'success': 0, 'failed': 0, 'error': 0, 'total': 0})\n    for r in self.records:\n        account = r.get('phone', 'unknown')\n        status = r.get('status', 'unknown')\n        by_account[account][status] = by_account[account].get(status, 0) + 1\n        by_account[account]['total'] += 1\n    # Calculate rates\n    for acc, data in by_account.items():\n        data['success_rate'] = data['success'] / data['total'] * 100 if data['total'] > 0 else 0\n    return dict(sorted(by_account.items(), key=lambda x: x[1]['success_rate']))\n```\n\n#### Success Rate by Hour\n```python\ndef success_rate_by_hour(self) -> Dict[int, dict]:\n    \"\"\"Calculate success rates grouped by hour of day\"\"\"\n    by_hour = defaultdict(lambda: {'success': 0, 'total': 0})\n    for r in self.records:\n        if r.get('timestamp_parsed'):\n            hour = r['timestamp_parsed'].hour\n            by_hour[hour]['total'] += 1\n            if r.get('status') == 'success':\n                by_hour[hour]['success'] += 1\n    for hour, data in by_hour.items():\n        data['success_rate'] = data['success'] / data['total'] * 100 if data['total'] > 0 else 0\n    return dict(sorted(by_hour.items()))\n```\n\n#### Error Type Classification\n```python\ndef classify_errors(self) -> Dict[str, List[dict]]:\n    \"\"\"Categorize errors by type based on error message patterns\"\"\"\n    error_patterns = {\n        'upload_timeout': ['Upload timeout', 'status: 1'],\n        'uiautomator_crash': ['UiAutomator2', 'instrumentation process is not running', 'crashed'],\n        'adb_timeout': ['timed out after', 'adb.exe'],\n        'connection_failed': ['connection', 'offline', 'refused'],\n        'instagram_blocked': ['action blocked', 'suspended', 'captcha'],\n    }\n    \n    classified = defaultdict(list)\n    for r in self.records:\n        if r.get('status') in ['error', 'failed']:\n            error_msg = r.get('error', '')\n            error_type = 'unknown'\n            for etype, patterns in error_patterns.items():\n                if any(p.lower() in error_msg.lower() for p in patterns):\n                    error_type = etype\n                    break\n            classified[error_type].append(r)\n    return dict(classified)\n```\n\n#### Average Attempts Before Success\n```python\ndef avg_attempts_before_success(self) -> dict:\n    \"\"\"Calculate average attempts needed for successful posts.\n    Requires correlation with scheduler_state.json for attempt tracking.\"\"\"\n    # Load from scheduler_state.json if available\n    state_file = \"scheduler_state.json\"\n    attempts_data = []\n    try:\n        with open(state_file, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        for job in data.get('jobs', []):\n            if job.get('status') == 'success':\n                attempts_data.append(job.get('attempts', 1))\n    except Exception:\n        pass\n    \n    if attempts_data:\n        return {\n            'mean': statistics.mean(attempts_data),\n            'median': statistics.median(attempts_data),\n            'max': max(attempts_data),\n            'samples': len(attempts_data)\n        }\n    return {'error': 'No attempt data available'}\n```\n\n#### Phones with Highest Failure Rates\n```python\ndef phones_by_failure_rate(self, min_attempts: int = 2) -> List[Tuple[str, float, int]]:\n    \"\"\"Return phones sorted by failure rate (highest first)\"\"\"\n    rates = self.success_rate_by_account()\n    failures = []\n    for phone, data in rates.items():\n        if data['total'] >= min_attempts:\n            failure_rate = 100 - data['success_rate']\n            failures.append((phone, failure_rate, data['total']))\n    return sorted(failures, key=lambda x: -x[1])\n```\n\n#### Time Patterns in Failures\n```python\ndef failure_time_patterns(self) -> dict:\n    \"\"\"Analyze when failures occur - time of day, day of week, gaps between attempts\"\"\"\n    failures_by_hour = defaultdict(int)\n    failures_by_minute_bucket = defaultdict(int)  # 10-min buckets\n    \n    for r in self.records:\n        if r.get('status') in ['error', 'failed'] and r.get('timestamp_parsed'):\n            ts = r['timestamp_parsed']\n            failures_by_hour[ts.hour] += 1\n            bucket = ts.hour * 6 + ts.minute // 10\n            failures_by_minute_bucket[bucket] += 1\n    \n    return {\n        'by_hour': dict(failures_by_hour),\n        'peak_failure_hour': max(failures_by_hour.items(), key=lambda x: x[1]) if failures_by_hour else None,\n        'failure_distribution': failures_by_minute_bucket\n    }\n```\n\n#### Video Size Correlation (placeholder - needs video file access)\n```python\ndef video_size_correlation(self, video_folder: str = \"chunk_01c\") -> dict:\n    \"\"\"Correlate video file sizes with success/failure rates.\n    Requires access to video files to get sizes.\"\"\"\n    # Map shortcodes to file sizes\n    shortcode_sizes = {}\n    success_sizes = []\n    fail_sizes = []\n    \n    # Walk video folder to build size map\n    for root, dirs, files in os.walk(video_folder):\n        for f in files:\n            if f.endswith('.mp4'):\n                shortcode = f.replace('.mp4', '')\n                path = os.path.join(root, f)\n                shortcode_sizes[shortcode] = os.path.getsize(path)\n    \n    for r in self.records:\n        shortcode = r.get('shortcode', '')\n        if shortcode in shortcode_sizes:\n            size_mb = shortcode_sizes[shortcode] / (1024 * 1024)\n            if r.get('status') == 'success':\n                success_sizes.append(size_mb)\n            else:\n                fail_sizes.append(size_mb)\n    \n    return {\n        'avg_success_size_mb': statistics.mean(success_sizes) if success_sizes else 0,\n        'avg_fail_size_mb': statistics.mean(fail_sizes) if fail_sizes else 0,\n        'success_samples': len(success_sizes),\n        'fail_samples': len(fail_sizes),\n    }\n```\n\n### 3. Report Generator\n\n```python\ndef generate_report(self) -> str:\n    \"\"\"Generate a comprehensive markdown report\"\"\"\n    report = []\n    report.append(\"# Scheduler Run Analysis Report - Dec 11, 2025\\n\")\n    \n    # Overall stats\n    total = len(self.records)\n    success = sum(1 for r in self.records if r.get('status') == 'success')\n    report.append(f\"## Overall Statistics\")\n    report.append(f\"- Total attempts: {total}\")\n    report.append(f\"- Successful: {success} ({success/total*100:.1f}%)\")\n    report.append(f\"- Failed/Error: {total - success}\")\n    \n    # Add each metric section...\n    # (success by account, by hour, error types, etc.)\n    \n    # Priority Fixes section\n    report.append(\"\\n## Priority Fixes Needed\")\n    errors = self.classify_errors()\n    if errors.get('upload_timeout'):\n        report.append(\"1. **Upload Timeout** - Increase upload timeout beyond 180s or implement chunked upload\")\n    if errors.get('uiautomator_crash'):\n        report.append(\"2. **UiAutomator2 Crashes** - Implement phone restart recovery per Task 13\")\n    \n    return \"\\n\".join(report)\n```\n\n### 4. CLI Interface\n\n```python\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser(description='Analyze scheduler results')\n    parser.add_argument('--date', default='20251211', help='Date pattern YYYYMMDD')\n    parser.add_argument('--output', default='scheduler_analysis_report.md', help='Output report file')\n    parser.add_argument('--json', action='store_true', help='Output raw data as JSON')\n    \n    args = parser.parse_args()\n    \n    analyzer = SchedulerAnalyzer(f\"batch_results_{args.date}*.csv\")\n    analyzer.load_data()\n    \n    if args.json:\n        data = {\n            'by_account': analyzer.success_rate_by_account(),\n            'by_hour': analyzer.success_rate_by_hour(),\n            'errors': analyzer.classify_errors(),\n            'failure_patterns': analyzer.failure_time_patterns(),\n        }\n        print(json.dumps(data, indent=2, default=str))\n    else:\n        report = analyzer.generate_report()\n        with open(args.output, 'w', encoding='utf-8') as f:\n            f.write(report)\n        print(f\"Report saved to {args.output}\")\n```\n\n### 5. Files to Read\n\n- `batch_results_20251211*.csv` - All CSV files from Dec 11 runs\n- `scheduler_state.json` - For attempt counts and job metadata\n- `geelark_batch.log` - For detailed error stack traces and phase timing\n- `chunk_01c/` - Video folder for file size analysis",
        "testStrategy": "## Test Strategy\n\n### 1. Data Loading Tests\n- Verify all Dec 11 CSV files are found and loaded (expect ~14 files based on glob results)\n- Confirm all expected columns are present: shortcode, phone, status, error, timestamp\n- Test handling of empty error fields and malformed timestamps\n\n### 2. Metric Calculation Validation\n- **Success rate by account**: Cross-reference with manual count from sample CSV files\n- **Success rate by hour**: Verify hour extraction from ISO timestamps (e.g., \"2025-12-11T18:22:34\" → hour 18)\n- **Error classification**: Test pattern matching against known error strings:\n  - \"Upload timeout after 180s (last status: 1)\" → upload_timeout\n  - \"UiAutomator2 server...instrumentation process is not running\" → uiautomator_crash\n  - \"timed out after 30 seconds\" → adb_timeout\n\n### 3. Report Verification\n- Run analysis and verify report includes all 7 requested metrics\n- Compare overall success count with sum across all CSVs\n- Verify phones with highest failure rates list shows accounts that appear in error records\n\n### 4. Edge Cases\n- Test with empty CSV files\n- Test with single-record files\n- Test when scheduler_state.json is unavailable or malformed\n- Test when video folder doesn't exist (video size correlation should gracefully report 0 samples)\n\n### 5. Manual Spot-Check\n```bash\n# Quick validation commands\npython analyze_scheduler_results.py --json | jq '.by_account | length'\n# Should return number of unique accounts\n\npython analyze_scheduler_results.py --json | jq '.errors | keys'\n# Should show error type categories found\n```\n\n### 6. Cross-Reference with Raw Data\n- Compare report findings with direct CSV inspection\n- Verify error messages in report match actual error strings from CSVs\n- Confirm time patterns align with file timestamps on batch_results_*.csv files",
        "status": "cancelled",
        "dependencies": [
          "2",
          "9"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T00:29:55.262Z"
      },
      {
        "id": "15",
        "title": "Integrate reliability features into posting_scheduler worker loop",
        "description": "Wire up existing but unused reliability mechanisms (Appium health checks, account cooldown backoff) into the scheduler's worker loop, add a heartbeat thread to keep the lock file fresh, and classify infrastructure errors to trigger account-level backoff.",
        "details": "## Current State Analysis\n\nThe codebase already has several reliability features that are **implemented but NOT wired up**:\n\n1. **Single-instance lock** (lines 30-111): Fully working but uses static lock file without heartbeat\n2. **Appium health checks** (lines 114-170): `check_appium_health()` and `restart_appium()` exist but never called\n3. **Account cooldown** (lines 435-471): `is_on_cooldown()` and `record_post(is_infra_error)` exist but:\n   - `is_on_cooldown()` is NOT checked in `get_next_job()` (line 661-703)\n   - `record_post()` is always called with just `False`, never passing `is_infra_error=True` (line 815)\n\n## Implementation Details\n\n### 1. Add Heartbeat Thread for Lock Freshness\n\nUpdate the lock file with a timestamp periodically so other instances can detect truly stale locks:\n\n```python\n# Add to PostingScheduler.__init__()\nself.heartbeat_thread: Optional[threading.Thread] = None\nself.heartbeat_interval = 30  # seconds\n\n# Add heartbeat method\ndef _heartbeat_loop(self):\n    \"\"\"Periodically update lock file to prove we're still alive\"\"\"\n    while self.running:\n        try:\n            if os.path.exists(LOCK_FILE):\n                with open(LOCK_FILE, 'r') as f:\n                    lock_data = json.load(f)\n                if lock_data.get('pid') == os.getpid():\n                    lock_data['last_heartbeat'] = datetime.now().isoformat()\n                    with open(LOCK_FILE, 'w') as f:\n                        json.dump(lock_data, f)\n        except Exception as e:\n            logger.warning(f\"Heartbeat error: {e}\")\n        time.sleep(self.heartbeat_interval)\n```\n\nUpdate `acquire_lock()` to check heartbeat staleness:\n```python\n# In acquire_lock(), after is_process_running check:\nlast_heartbeat = lock_data.get('last_heartbeat')\nif last_heartbeat:\n    hb_time = datetime.fromisoformat(last_heartbeat)\n    stale_threshold = timedelta(minutes=2)  # 2 minutes without heartbeat = stale\n    if datetime.now() - hb_time > stale_threshold:\n        print(f\"[LOCK] Lock heartbeat stale ({hb_time}). Taking over.\")\n        # Proceed to take over\n```\n\n### 2. Integrate Appium Health Check into Worker Loop\n\nIn `_worker_loop()`, before processing a job:\n\n```python\ndef _worker_loop(self):\n    \"\"\"Main worker loop\"\"\"\n    self._log(\"Worker started\")\n    \n    # Track consecutive Appium failures for restart logic\n    appium_consecutive_failures = 0\n    max_appium_failures_before_restart = 3\n    \n    while self.running:\n        if self.paused:\n            time.sleep(1)\n            continue\n        \n        # Check Appium health before each job\n        if not check_appium_health():\n            self._log(\"[APPIUM] Health check failed\")\n            appium_consecutive_failures += 1\n            \n            if appium_consecutive_failures >= max_appium_failures_before_restart:\n                self._log(\"[APPIUM] Attempting auto-restart...\")\n                if restart_appium():\n                    appium_consecutive_failures = 0\n                else:\n                    self._log(\"[APPIUM] Restart failed, waiting 60s...\")\n                    time.sleep(60)\n                    continue\n            else:\n                time.sleep(10)\n                continue\n        else:\n            appium_consecutive_failures = 0  # Reset on success\n        \n        job = self.get_next_job()\n        # ... rest of loop\n```\n\n### 3. Integrate Account Cooldown into get_next_job()\n\nUpdate `get_next_job()` to filter out accounts on cooldown:\n\n```python\ndef get_next_job(self) -> Optional[PostJob]:\n    \"\"\"Get next job that's ready to post\"\"\"\n    accounts_posted_today = get_accounts_posted_today()\n    \n    # Filter: can post today AND not on cooldown\n    available_accounts = [\n        acc for acc in self.accounts.values()\n        if acc.can_post_today(self.posts_per_account_per_day)\n        and acc.name not in accounts_posted_today\n        and not acc.is_on_cooldown()  # ADD THIS LINE\n    ]\n    # ... rest of method unchanged\n```\n\n### 4. Classify Infrastructure Errors in execute_job()\n\nUpdate the error handling in `execute_job()` to detect infrastructure errors:\n\n```python\n# In execute_job(), in the except block (around line 783):\nexcept Exception as e:\n    error_msg = str(e)\n    error_type_name = type(e).__name__\n    \n    # Classify infrastructure errors\n    infra_error_patterns = [\n        'ADB', 'adb', 'device offline', 'glogin', 'phone not running',\n        'Appium', 'appium', 'UiAutomator', 'WebDriver', \n        'connection refused', 'timeout', 'Timeout'\n    ]\n    is_infra_error = any(pattern in error_msg for pattern in infra_error_patterns) or \\\n                     any(pattern in error_type_name for pattern in infra_error_patterns)\n    \n    job.last_error = f\"[{phase}] {error_type_name}: {error_msg}\"\n    \n    # ... existing error handling ...\n    \n    # Pass is_infra_error to trigger backoff\n    self.accounts[job.account].record_post(False, is_infra_error=is_infra_error)\n```\n\n### 5. Start Heartbeat Thread in start()\n\n```python\ndef start(self):\n    \"\"\"Start the scheduler\"\"\"\n    if self.running:\n        return\n    \n    # ... existing phone cleanup ...\n    \n    self.running = True\n    self.paused = False\n    \n    # Start heartbeat thread\n    self.heartbeat_thread = threading.Thread(target=self._heartbeat_loop, daemon=True)\n    self.heartbeat_thread.start()\n    self._log(\"[HEARTBEAT] Started heartbeat thread\")\n    \n    # Start worker thread\n    self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)\n    self.worker_thread.start()\n    self._log(\"Scheduler started\")\n```\n\n### 6. Add Account Cooldown Status to get_stats()\n\n```python\ndef get_stats(self) -> dict:\n    \"\"\"Get current statistics\"\"\"\n    accounts_on_cooldown = [acc.name for acc in self.accounts.values() if acc.is_on_cooldown()]\n    \n    return {\n        # ... existing stats ...\n        'accounts_on_cooldown': accounts_on_cooldown,\n    }\n```\n\n## Files to Modify\n\n- `posting_scheduler.py`: All changes concentrated in this single file",
        "testStrategy": "## Test Strategy\n\n### 1. Single-Instance Lock with Heartbeat Tests\n\n**Test stale lock detection:**\n```bash\n# Create a stale lock file manually\necho '{\"pid\": 99999, \"started\": \"2024-01-01T00:00:00\", \"last_heartbeat\": \"2024-01-01T00:00:00\"}' > scheduler.lock\n\n# Run scheduler - should take over the stale lock\npython posting_scheduler.py --status\n# Expected: \"Lock heartbeat stale\" message, then acquires lock\n```\n\n**Test heartbeat updates:**\n```bash\n# Start scheduler in background\npython posting_scheduler.py --add-folder chunk_test --add-accounts test1 --run &\n\n# Check lock file updates every 30s\nwatch -n 10 'cat scheduler.lock | python -m json.tool | grep last_heartbeat'\n# Expected: last_heartbeat timestamp updates every ~30 seconds\n```\n\n**Test duplicate instance prevention:**\n```bash\n# Terminal 1: Start scheduler\npython posting_scheduler.py --run\n\n# Terminal 2: Try to start another\npython posting_scheduler.py --run\n# Expected: \"[LOCK ERROR] Another scheduler instance is already running!\"\n```\n\n### 2. Appium Health Check Integration Tests\n\n**Test health check detection:**\n```bash\n# Stop Appium server\ntaskkill /F /IM node.exe\n\n# Run scheduler - should detect Appium down\npython posting_scheduler.py --run\n# Expected: \"[APPIUM] Health check failed\" messages\n```\n\n**Test auto-restart:**\n```bash\n# With Appium stopped, scheduler should attempt restart after 3 failures\n# Expected log sequence:\n# [APPIUM] Health check failed (1)\n# [APPIUM] Health check failed (2) \n# [APPIUM] Health check failed (3)\n# [APPIUM] Attempting auto-restart...\n# [APPIUM] Server ready on port 4723\n```\n\n### 3. Account Cooldown Integration Tests\n\n**Test cooldown filtering in get_next_job:**\n```python\n# Unit test\nscheduler = PostingScheduler()\nscheduler.add_account(\"test1\")\nscheduler.accounts[\"test1\"].cooldown_until = (datetime.now() + timedelta(minutes=10)).isoformat()\n\n# get_next_job should not return jobs for test1\njob = scheduler.get_next_job()\nassert job is None or job.account != \"test1\"\n```\n\n**Test infrastructure error classification:**\n```python\n# Simulate infra error in execute_job\n# After 3 consecutive failures, account should be on cooldown\nassert scheduler.accounts[\"test1\"].is_on_cooldown() == True\nassert scheduler.accounts[\"test1\"].consecutive_failures >= 3\n```\n\n### 4. Status Command Verification\n\n```bash\npython posting_scheduler.py --status\n# Expected output includes:\n# - Lock status with last_heartbeat timestamp\n# - Accounts on cooldown list (if any)\n# - Appium health status\n```\n\n### 5. Error Log Verification\n\nAfter a test run with simulated failures:\n```bash\ngrep \"is_infra_error\" geelark_batch.log\n# Should show infrastructure errors being correctly classified\n\ngrep \"on cooldown\" geelark_batch.log  \n# Should show accounts being put on cooldown after consecutive failures\n```\n\n### 6. Integration Test with Real Posting\n\n```bash\n# Run with a small test batch\npython posting_scheduler.py --add-folder chunk_test --add-accounts phone1 --run\n\n# Monitor logs for:\n# 1. Heartbeat updates\n# 2. Appium health checks before each job\n# 3. Proper cooldown behavior if failures occur\n# 4. Clean shutdown releasing lock\n```",
        "status": "done",
        "dependencies": [
          "9",
          "11",
          "13"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-12T01:42:53.832Z"
      },
      {
        "id": "16",
        "title": "Ensure ANDROID_HOME / ANDROID_SDK_ROOT Are Recognized by Appium Server",
        "description": "Make Appium reliably detect the Android SDK by standardizing how ANDROID_HOME and ANDROID_SDK_ROOT are set, exported, and propagated into the Appium server process across all deployment environments.",
        "details": "## Goal\nGuarantee that when the Appium server is started (locally, via scripts, or inside workers/containers), it always has valid access to the Android SDK through **ANDROID_HOME** and/or **ANDROID_SDK_ROOT**, so errors like “Neither ANDROID_HOME nor ANDROID_SDK_ROOT environment variable was exported” do not occur.[7][8]\n\n## High-Level Approach\n1. **Standardize environment variable configuration** for Android SDK on all supported OSes (Linux/macOS; Windows only if relevant).\n2. **Ensure variables are set in a *non-interactive* context** (systemd services, cron, Docker, background workers), not just in interactive shells.[7]\n3. **Unify Appium startup** through a single entry point (Python helper or shell script) that validates and, if needed, sets or maps ANDROID_SDK_ROOT/ANDROID_HOME before launching the server.\n4. **Add diagnostics** so misconfiguration is obvious in logs.\n\n## Implementation Steps\n\n### 1. Discover Current SDK Paths and Usage\n- Inspect how Appium is currently started:\n  - Python wrapper (e.g., `post_reel_smart.py` / scheduler), direct `appium` CLI, Docker, or a service unit.\n  - Note whether `appium` is started via `subprocess` in Python.\n- On at least one working dev machine and one production-like host:\n  - Run `echo $ANDROID_HOME` and `echo $ANDROID_SDK_ROOT` (or `set` on Windows) to see what is set.[4][6]\n  - Run `sdkmanager --list` from the same shell that starts Appium to confirm SDK accessibility.\n  - If using Android Studio, open SDK Manager and capture the **Android SDK Location** to use as canonical ANDROID_HOME.[5][6]\n\n### 2. Standard OS-Level Environment Setup (Best Practices)\nFollow current best-practice patterns for SDK env configuration so that Appium’s CLI sees them by default.[2][4][5][6]\n\n**Linux/macOS:**\n- In the system or service user profile, set (example):\n  ```bash\n  export ANDROID_HOME=\"$HOME/Android/Sdk\"\n  export ANDROID_SDK_ROOT=\"$ANDROID_HOME\"\n  export PATH=\"$PATH:$ANDROID_HOME/emulator:$ANDROID_HOME/platform-tools:$ANDROID_HOME/tools:$ANDROID_HOME/tools/bin\"\n  ```[4][5][6]\n- Add to the appropriate file for non-interactive shells (e.g., `/etc/profile.d/android-sdk.sh` or the service user’s `.profile`), not just `.bashrc`.\n\n**Windows (if used for Appium host):**\n- In *System Properties → Environment Variables*:\n  - Add **ANDROID_HOME** and/or **ANDROID_SDK_ROOT** pointing to the SDK directory (e.g., `C:\\Users\\<User>\\AppData\\Local\\Android\\Sdk`).[2][3][5][6]\n  - Add to **PATH**:\n    - `%ANDROID_HOME%\\emulator`\n    - `%ANDROID_HOME%\\platform-tools`\n    - `%ANDROID_HOME%\\tools`\n    - `%ANDROID_HOME%\\tools\\bin`[2][5]\n- Reboot or restart relevant services after setting system variables.[5]\n\nDocument the canonical SDK path and env configuration in `docs/appium_env.md` so all environments can be made consistent.\n\n### 3. Central Appium Launcher With Env Validation\nCreate a central launcher responsible for starting Appium with a guaranteed-good environment.\n\n**Option A – Shell wrapper (for CLI/containers):**\n- Add a script `scripts/start_appium.sh`:\n  ```bash\n  #!/usr/bin/env bash\n  set -euo pipefail\n\n  # 1. Infer or normalize SDK env\n  if [[ -z \"${ANDROID_HOME:-}\" && -n \"${ANDROID_SDK_ROOT:-}\" ]]; then\n    export ANDROID_HOME=\"$ANDROID_SDK_ROOT\"\n  elif [[ -z \"${ANDROID_SDK_ROOT:-}\" && -n \"${ANDROID_HOME:-}\" ]]; then\n    export ANDROID_SDK_ROOT=\"$ANDROID_HOME\"\n  fi\n\n  # 2. Fallback: attempt to detect SDK in common locations (optional)\n  if [[ -z \"${ANDROID_HOME:-}\" ]]; then\n    for candidate in \"$HOME/Android/Sdk\" \\\n                    \"$HOME/Library/Android/sdk\" \\\n                    \"/usr/local/android-sdk\"; do\n      if [[ -d \"$candidate/platform-tools\" ]]; then\n        export ANDROID_HOME=\"$candidate\"\n        export ANDROID_SDK_ROOT=\"$candidate\"\n        break\n      fi\n    done\n  fi\n\n  # 3. Validate\n  if [[ -z \"${ANDROID_HOME:-}\" || ! -d \"$ANDROID_HOME/platform-tools\" ]]; then\n    echo \"[FATAL] ANDROID_HOME/ANDROID_SDK_ROOT not set or invalid. Please install Android SDK and configure env vars.\" >&2\n    exit 1\n  fi\n\n  export PATH=\"$PATH:$ANDROID_HOME/emulator:$ANDROID_HOME/platform-tools:$ANDROID_HOME/tools:$ANDROID_HOME/tools/bin\"\n\n  echo \"[INFO] Using ANDROID_HOME=$ANDROID_HOME\" >&2\n  echo \"[INFO] Using ANDROID_SDK_ROOT=${ANDROID_SDK_ROOT:-$ANDROID_HOME}\" >&2\n\n  # 4. Finally run Appium\n  exec appium \"$@\"\n  ```\n- Ensure all automation (scheduler, local dev docs, CI, systemd unit) uses this script instead of invoking `appium` directly.\n\n**Option B – Python-side launcher (if Appium is started from Python):**\n- Implement a helper (e.g., in a shared module `appium_env.py`):\n  ```python\n  import os\n  import shutil\n  import subprocess\n\n  class AndroidEnvError(RuntimeError):\n      pass\n\n  def ensure_android_env() -> dict:\n      env = os.environ.copy()\n      home = env.get(\"ANDROID_HOME\")\n      root = env.get(\"ANDROID_SDK_ROOT\")\n\n      if not home and root:\n          home = root\n          env[\"ANDROID_HOME\"] = root\n      elif not root and home:\n          root = home\n          env[\"ANDROID_SDK_ROOT\"] = home\n\n      if not home:\n          # Optional: probe common locations\n          for candidate in [\n              os.path.expanduser(\"~/Android/Sdk\"),\n              os.path.expanduser(\"~/Library/Android/sdk\"),\n              \"/usr/local/android-sdk\",\n          ]:\n              if os.path.isdir(os.path.join(candidate, \"platform-tools\")):\n                  home = root = candidate\n                  env[\"ANDROID_HOME\"] = candidate\n                  env[\"ANDROID_SDK_ROOT\"] = candidate\n                  break\n\n      if not home or not os.path.isdir(os.path.join(home, \"platform-tools\")):\n          raise AndroidEnvError(\n              \"ANDROID_HOME/ANDROID_SDK_ROOT not set or invalid; install Android SDK and configure env vars.\"\n          )\n\n      pt = os.path.join(home, \"platform-tools\")\n      emulator = os.path.join(home, \"emulator\")\n      tools = os.path.join(home, \"tools\")\n      tools_bin = os.path.join(tools, \"bin\")\n      extra = os.pathsep.join(p for p in [pt, emulator, tools, tools_bin] if os.path.isdir(p))\n      if extra:\n          env[\"PATH\"] = env.get(\"PATH\", \"\") + os.pathsep + extra\n\n      return env\n\n  def start_appium_server(args: list[str]) -> subprocess.Popen:\n      env = ensure_android_env()\n      appium_cmd = shutil.which(\"appium\") or \"appium\"\n      return subprocess.Popen([appium_cmd, *args], env=env)\n  ```\n- Refactor all places that start Appium (e.g., utilities used by Task 11 and 13 flows) to use `start_appium_server` instead of raw `subprocess.Popen`.\n\n### 4. Integrate with Existing Reliability / Health Logic\n- In the same place where Appium health checks and restarts are wired (Task 15) and connection stability is being improved (Task 13), ensure the restart path *also* uses the standardized launcher so restarted servers see the correct env.\n- When an Appium startup or health check fails due to env problems (e.g., server logs mention missing `adb` or ANDROID_HOME), log a distinct error code / message so future analysis (Task 14) can differentiate env configuration problems from device/Appium bugs.\n\n### 5. Diagnostics and Logging\n- At Appium startup, log the detected **ANDROID_HOME**, **ANDROID_SDK_ROOT**, and whether `adb` is found on PATH (e.g., `which adb` / `where adb`).\n- Optionally, run a lightweight `adb version` and `adb devices` check immediately after starting the server and log the output to quickly spot SDK vs. device issues.[4][6]\n- Update developer/ops documentation with:\n  - Required env vars and their purpose.\n  - Example configuration snippets for each OS.\n  - How to run `appium-doctor --android` to validate setup before running tests.[1][2][6]\n\n### 6. CI / Container Integration (If Applicable)\n- For Docker images, bake the SDK and env variables into the image:\n  ```dockerfile\n  ENV ANDROID_HOME=/opt/android-sdk \\\n      ANDROID_SDK_ROOT=/opt/android-sdk\n  ENV PATH=\"$PATH:$ANDROID_HOME/emulator:$ANDROID_HOME/platform-tools:$ANDROID_HOME/tools:$ANDROID_HOME/tools/bin\"\n  ```\n- Ensure the CI job that runs mobile tests uses either the shell or Python launcher above.\n\n## Notes / Best Practices\n- Prefer **ANDROID_SDK_ROOT** (more modern) but keep **ANDROID_HOME** for compatibility; set both to the same directory.[7]\n- Always ensure that at least `platform-tools` and `emulator` are on PATH for Appium Android testing.[2][4][5][6]\n- When changing system environment variables on Windows, restart services or the whole machine so Appium inherits them.[5]\n",
        "testStrategy": "1. **Env Sanity Checks**\n- On each supported OS:\n  - Open a shell configured the same way the Appium server is started (service user, CI container, or scheduler process).\n  - Run `echo $ANDROID_HOME` / `echo $ANDROID_SDK_ROOT` (or `set ANDROID_` on Windows) and confirm they point to the actual SDK directory.\n  - Run `adb version` and confirm it succeeds.\n  - Run `appium-doctor --android` and verify there are no Android SDK-related errors.[1][2][6]\n\n2. **Launcher-Level Tests (Shell Wrapper)**\n- Temporarily unset ANDROID_HOME/ANDROID_SDK_ROOT, then:\n  - Create a mock SDK directory at a common default path with a dummy `platform-tools` folder.\n  - Run `scripts/start_appium.sh --log-level debug` and confirm:\n    - The script discovers the SDK and sets ANDROID_HOME/ANDROID_SDK_ROOT (check printed logs).\n    - `adb` from the mock SDK is picked up (check `which adb` output if added).\n- Set only ANDROID_HOME and confirm the wrapper mirrors it to ANDROID_SDK_ROOT and logs both.\n- Set only ANDROID_SDK_ROOT and confirm the wrapper mirrors it to ANDROID_HOME.\n- Intentionally point ANDROID_HOME to a non-existent directory and verify the script exits non‑zero with a clear fatal error message.\n\n3. **Launcher-Level Tests (Python Helper, if implemented)**\n- Unit-test `ensure_android_env()` using `monkeypatch`/`os.environ` manipulation:\n  - Case: both vars absent, no SDK dirs → expect `AndroidEnvError`.\n  - Case: only ANDROID_HOME set → expect ANDROID_SDK_ROOT to be added and PATH extended.\n  - Case: only ANDROID_SDK_ROOT set → expect ANDROID_HOME to be added and PATH extended.\n  - Case: neither set but a test SDK directory exists in a probed path → expect both vars to be set to that directory.\n- Unit-test `start_appium_server()` by stubbing `subprocess.Popen` and asserting it receives an `env` with properly set ANDROID_HOME/ANDROID_SDK_ROOT and PATH.\n\n4. **Integration Test with Appium and Device**\n- From the worker/scheduler context that will run real jobs:\n  - Start Appium using the new launcher (shell or Python).\n  - Check the Appium server logs to confirm:\n    - ANDROID_HOME/ANDROID_SDK_ROOT values are logged as expected.\n    - No warnings like \"Neither ANDROID_HOME nor ANDROID_SDK_ROOT environment variable was exported\" appear.[8]\n  - Run a minimal Android session (e.g., from Task 11’s test harness):\n    - Create an Appium session to a real or cloud Android device.\n    - Verify the session initializes, `adb devices` lists the device, and a simple `driver.get_page_source()` succeeds.\n\n5. **Failure-Mode Regression Test**\n- Temporarily misconfigure env (e.g., unset ANDROID_HOME in the service config) and start Appium through the new launcher:\n  - Confirm the launcher fails fast with a clear error instead of starting a broken server.\n  - Ensure higher-level reliability/health logic (from Task 13 and Task 15) logs an explicit env-configuration error category and does not enter an infinite restart loop.\n\n6. **Documentation Validation**\n- Follow the updated `docs/appium_env.md` from a clean machine:\n  - Configure the SDK and env exactly as documented.\n  - Start Appium using the documented command.\n  - Confirm that an Android session can be created without additional manual tweaks, demonstrating the docs are accurate and sufficient.",
        "status": "done",
        "dependencies": [
          "11",
          "13"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-12T01:54:53.017Z"
      },
      {
        "id": "17",
        "title": "Reduce UiAutomator2 launch timeout from 90s to 10s",
        "description": "Optimize the Appium UiAutomator2 server launch timeout based on the observation that instrumentation either starts immediately (~1s) or times out completely - there is no middle ground, so waiting 90 seconds on failure wastes time unnecessarily.",
        "details": "## Background\n\nAnalysis documented in `geelark_uiautomator2_timeout_report.txt` reveals a **binary behavior pattern** for UiAutomator2 initialization on Geelark cloud phones:\n\n- **Success case**: Instrumentation starts in ~1 second (observed: 1104ms)\n- **Failure case**: Times out after the full timeout period (previously 90s)\n- **No middle ground**: There are no cases where initialization takes 30s, 50s, or any intermediate time\n\nThis means the previous 90-second timeout was wasteful - if UiAutomator2 doesn't start within a few seconds, it won't start at all until retry.\n\n## Implementation\n\nIn `post_reel_smart.py`, update the `connect_appium()` method (around line 743):\n\n### Before (Task 13 implementation):\n```python\noptions.set_capability(\"appium:uiautomator2ServerLaunchTimeout\", 90000)  # 90s\n```\n\n### After:\n```python\noptions.set_capability(\"appium:uiautomator2ServerLaunchTimeout\", 10000)  # 10s for launch - binary: works in ~1s or not at all\n```\n\n## Rationale\n\n1. **Time savings**: Failed attempts now waste 10s instead of 90s (80s saved per failure)\n2. **Faster retry cycle**: With 15s delay between retries, a full 3-attempt cycle takes:\n   - Before: 90s + 15s + 90s + 15s + 90s = 300s (5 minutes)\n   - After: 10s + 15s + 10s + 15s + 10s = 60s (1 minute)\n3. **No false negatives**: 10s is still generous given the observed ~1s success time\n4. **Buffer for edge cases**: 10s provides 10x buffer over the ~1s typical success time\n\n## Other timeouts remain unchanged\n\nThe following timeouts in `connect_appium()` should NOT be reduced as they serve different purposes:\n\n- `newCommandTimeout: 120` - For slow cloud phone operations during the session\n- `adbExecTimeout: 120000` - For slow ADB commands over network tunnels\n- `uiautomator2ServerInstallTimeout: 120000` - First-time APK installation can be slow\n- `androidDeviceReadyTimeout: 60` - Device boot/ready detection\n\nOnly `uiautomator2ServerLaunchTimeout` exhibits the binary behavior pattern.",
        "testStrategy": "## Test Strategy\n\n### 1. Verify timeout value is correctly set\n\nRun a quick Appium session and check the capabilities:\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nposter = SmartInstagramPoster('test_phone')\n# Connect to a phone and check capabilities\nposter.connect()\ncaps = poster.appium_driver.capabilities\nprint(f'Launch timeout: {caps.get(\\\"uiautomator2ServerLaunchTimeout\\\", \\\"not set\\\")}')\nposter.cleanup()\n\"\n```\n\n### 2. Timing verification on success\n\nStart a cloud phone and time the Appium connection:\n```bash\ntime python -c \"\nfrom post_reel_smart import SmartInstagramPoster\nposter = SmartInstagramPoster('podclipcrafters')\nposter.connect()\nprint('Connected successfully')\nposter.cleanup()\n\"\n```\n\nExpected: Total connection time should be well under 60 seconds on success.\n\n### 3. Timing verification on failure\n\nSimulate a failure scenario by connecting to an invalid device:\n```bash\ntimeout 20 python -c \"\nfrom appium import webdriver\nfrom appium.options.android import UiAutomator2Options\noptions = UiAutomator2Options()\noptions.device_name = 'invalid:12345'\noptions.udid = 'invalid:12345'\noptions.set_capability('appium:uiautomator2ServerLaunchTimeout', 10000)\ndriver = webdriver.Remote('http://127.0.0.1:4723', options=options)\n\" 2>&1 | grep -i timeout\n```\n\nExpected: Should timeout within ~15 seconds (10s timeout + overhead), not 90+ seconds.\n\n### 4. Full retry cycle timing\n\nRun a posting operation to a phone that may have intermittent connectivity:\n```bash\ntime python posting_scheduler.py --add-accounts podclipcrafters --add-folder test_videos --run --max-accounts 1\n```\n\nMonitor `geelark_batch.log` for retry timing. Expected:\n- If first attempt fails, retry should start within 25-30 seconds (10s timeout + 15s delay)\n- Full 3-attempt cycle should complete in under 2 minutes even with all failures\n\n### 5. Regression test - no false negatives\n\nRun the scheduler on 5-10 phones overnight and compare:\n- Success rate should remain the same or improve (not decrease)\n- Average time per attempt should decrease significantly",
        "status": "done",
        "dependencies": [
          "13"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-12T02:04:02.231Z"
      },
      {
        "id": "18",
        "title": "Fix Appium Connection Failures with Device-Ready Checks and Thread Cleanup",
        "description": "Resolve Appium connection instability by ensuring device readiness before glogin execution, fixing ThreadPoolExecutor cleanup to prevent orphaned sessions, and managing stale ADB connections through robust connect() flow improvements.",
        "details": "Implement comprehensive fixes for the three identified Appium connection failure root causes following Appium best practices for ADB stability and resource management[1][2][6].\n\n## 1. Device Readiness Check Before glogin (Primary Fix)\n\n**Current Problem**: glogin executes before ADB reports device as 'device' status, causing connection failures[1].\n\n**Implementation**:\n```python\n# In connect_appium() or connect() flow (~lines 730+ from Task 13)\nimport subprocess\nimport time\n\nfrom typing import Optional\n\ndef wait_for_device_ready(udid: str, timeout: int = 60) -> bool:\n    \"\"\"Wait for device to report 'device' status in ADB\"\"\"\n    start_time = time.time()\n    while time.time() - start_time < timeout:\n        result = subprocess.run(\n            ['adb', '-s', udid, 'get-state'],\n            capture_output=True, text=True, timeout=5\n        )\n        if result.returncode == 0 and 'device' in result.stdout.strip().lower():\n            return True\n        time.sleep(2)\n    return False\n\ndef safe_glogin(udid: str) -> None:\n    \"\"\"Only run glogin after device is confirmed ready\"\"\"\n    if not wait_for_device_ready(udid):\n        raise UiAutomatorStartupError(f\"Device {udid} never reached 'device' state\")\n    # Run glogin subprocess here (existing logic)\n    subprocess.run(['glogin', udid], check=True)\n```\n\n**Integration**: Call `safe_glogin(self.device_udid)` **before** Appium driver initialization in `connect_appium()`.\n\n## 2. Fix/Remove ThreadPoolExecutor Wrapper\n\n**Current Problem**: ThreadPoolExecutor timeouts leave orphaned Appium sessions/threads[6].\n\n**Best Practice**: Use context managers for guaranteed cleanup. Remove ThreadPoolExecutor wrapper entirely[6].\n\n**Implementation**:\n```python\n# REPLACE ThreadPoolExecutor wrapper pattern with direct context-managed sessions\n\nclass AppiumSessionManager:\n    def __enter__(self):\n        self.driver = self.connect_appium(retries=3)\n        return self.driver\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Guaranteed cleanup - even on exceptions/timeouts\"\"\"\n        if hasattr(self, 'driver') and self.driver:\n            try:\n                self.driver.quit()\n                # Force ADB session cleanup\n                subprocess.run(['adb', 'kill-server'])\n                subprocess.run(['adb', 'start-server'])\n            except Exception as e:\n                logger.warning(f\"Cleanup failed: {e}\")\n\n# Usage in posting logic:\nwith AppiumSessionManager() as driver:\n    # All automation here\n    pass  # Auto-cleanup guaranteed\n```\n\n## 3. Stale ADB Connection Management\n\n**Implementation**:\n```python\ndef refresh_adb_connection(udid: Optional[str] = None) -> None:\n    \"\"\"Kill/restart ADB server to clear stale connections[1][2]\"\"\"\n    subprocess.run(['adb', 'kill-server'])\n    time.sleep(2)\n    subprocess.run(['adb', 'start-server'])\n    if udid:\n        # Wait for specific device\n        wait_for_device_ready(udid)\n\n# Call refresh_adb_connection() at start of connect_appium() and on UiAutomatorStartupError\n```\n\n## 4. Updated connect_appium() Flow\n```python\ndef connect_appium(self, retries=3):\n    for attempt in range(retries):\n        try:\n            refresh_adb_connection(self.device_udid)\n            safe_glogin(self.device_udid)\n            \n            # Existing Appium connection logic with 10s UiAutomator2 timeout (Task 17)\n            options = UiAutomator2Options()\n            options.set_capability('uiautomator2ServerLaunchTimeout', 10000)  # 10s\n            self.driver = u2.connect(options)\n            return self.driver\n        except (UiAutomatorStartupError, Exception) as e:\n            logger.warning(f\"Attempt {attempt+1} failed: {e}\")\n            if attempt == retries - 1:\n                raise\n            time.sleep(5)\n```\n\n**Dependencies**: Builds directly on Task 13 (connect_appium() structure, UiAutomatorStartupError), Task 17 (10s timeout), Task 16 (ADB env vars).[1][2]",
        "testStrategy": "**Comprehensive Test Strategy** (Critical for production stability)\n\n### 1. Device Readiness Tests\n```bash\n# Test 1: Simulate offline→online transition\nadb disconnect <udid>\nsleep 5\n# Start device connection\npython -m pytest test_appium_connect.py::test_wait_device_ready\n```\n- Verify `wait_for_device_ready()` polls correctly\n- Confirm `safe_glogin()` blocks until 'device' state\n- Test 60s timeout raises `UiAutomatorStartupError`\n\n### 2. ThreadPoolExecutor Replacement Tests\n- Create unit test simulating timeout during session\n```python\nfrom contextlib import contextmanager\n\n@contextmanager\ndef failing_appium():\n    yield\n    raise TimeoutError(\"Simulated timeout\")\n\n# Verify __exit__ still executes cleanup\n```\n- Confirm `driver.quit()` and `adb kill-server` called even on exceptions\n\n### 3. End-to-End Connection Tests\n```bash\n# Test full connect() flow 50x\nfor i in {1..50}; do\n    python test_appium_stability.py --device <udid> || echo \"FAIL $i\"\ndone\n```\n**Success Criteria**:\n- 100% success rate (0 connection failures)\n- No orphaned Appium processes (`ps aux | grep appium`)\n- No stale ADB connections (`adb devices -l` shows clean list)\n\n### 4. ADB Stale Connection Tests\n```bash\n# Force stale connections\nadb kill-server\nadb start-server\n# Run multiple parallel sessions\npytest test_adb_cleanup.py -n auto\n```\n- Verify `refresh_adb_connection()` restores clean state\n- Confirm no 'offline' devices after cleanup\n\n### 5. Integration with Existing Codebase\n- Run full scheduler loop (Task 15) for 2+ hours\n- Monitor `batch_results_*.csv` for zero Appium connection errors\n- Validate no lockfile/heartbeat issues (Task 15)\n\n**Tools**:\n- `lsof -i :5037` (ADB port conflicts)\n- `ps aux | grep -E 'appium|glogin'` (orphaned processes)\n- Appium logs with `--log-level debug`",
        "status": "cancelled",
        "dependencies": [
          "13",
          "16",
          "17"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T00:29:47.030Z"
      },
      {
        "id": "19",
        "title": "Implement multi-process Appium worker orchestration with isolated servers and CSV-based progress tracking",
        "description": "Design and implement a multi-worker posting system where each Python process manages its own Appium server, device, and job subset, coordinated by a simple orchestrator and CSV-based progress tracking to avoid duplicate posts.",
        "details": "Implementation outline:\n\n1) Overall architecture and process model\n- Introduce a new module (e.g. `parallel_orchestrator.py`) that is responsible for:\n  - Spawning N worker processes using `multiprocessing.Process` or `subprocess` (not threads) for true isolation.\n  - Assigning each worker a unique Appium configuration (Appium port, systemPort range, device mapping, and log paths).\n  - Managing lifecycle: start all workers, monitor, and perform clean shutdown on SIGINT/SIGTERM.\n- Each worker process will:\n  - Start its own Appium server instance (local `appium` binary or programmatic Node call) on a unique port (4723, 4725, 4727, etc.), following best practice that each device has a dedicated Appium server and unique port.[1][3]\n  - Use a unique `systemPort` (or narrow range) per worker for UiAutomator2 to avoid conflicts between parallel Android sessions.[3][6]\n  - Initialize its own `GeelarkDeviceController` and posting flow stack (`ClaudeNavigator`, etc.) to reuse existing single-device logic from Task 7.\n\n2) Port and systemPort allocation strategy\n- Define a configuration structure (in `config.py` or a new `parallel_config.py`) mapping worker IDs to Appium ports and systemPort ranges, e.g.:\n  - worker 0: appium_port=4723, system_port_start=8200, system_port_end=8209\n  - worker 1: appium_port=4725, system_port_start=8210, system_port_end=8219\n- When constructing desired capabilities for a worker’s Appium session, set:\n  - `\"udid\"` or equivalent device ID for that worker’s Geelark device.\n  - `\"systemPort\"` to a value in the allocated range for that worker.\n- Enforce uniqueness at runtime (assert no two workers share the same appium_port or overlapping systemPort ranges) to follow Appium parallel execution best practices.[1][3]\n\n3) Appium server lifecycle per worker\n- Implement a helper in a dedicated module, e.g. `appium_server_manager.py`:\n  - `start_appium_server(port: int, log_path: str, extra_args: list[str]) -> subprocess.Popen` that:\n    - Spawns `appium` with `--port`, and for Android also passes any required UiAutomator2/Chromedriver arguments.\n    - Redirects stdout/stderr to a per-worker log file for easier debugging, as recommended for parallel runs.[1]\n    - Waits for health-check (HTTP call to `/status`) with timeout and retry before proceeding.\n  - `stop_appium_server(proc: subprocess.Popen, timeout: float = 10.0)` that sends SIGTERM, then SIGKILL if necessary.\n- In the worker entrypoint:\n  - Start Appium.\n  - Run the worker job-processing loop.\n  - In a `try/finally`, always stop the Appium server and perform device cleanup.\n\n4) Worker process design and job acquisition\n- Implement a `worker_main(worker_id: int, config: Config, shared_state_paths: ...)` entry function that:\n  - Sets up logging with worker-specific identifiers.\n  - Starts Appium with that worker’s dedicated ports.\n  - Enters a loop where it repeatedly:\n    - Claims the next unprocessed job from a shared CSV-based tracker (see section 5).\n    - Runs `run_post_job(job, config, controller, navigator)` from Task 7, reusing the existing single-job flow.\n    - On success/failure, writes to both the existing output log CSV (Task 2, 9) and the progress tracker.\n  - Terminates cleanly when there are no remaining unclaimed jobs.\n- Ensure workers do not share Python objects in memory; they should communicate only via the filesystem (CSV files) or simple IPC if needed.\n\n5) CSV-based shared progress tracking (no duplicate posts)\n- Design a dedicated progress CSV (e.g. `progress.csv`) separate from the result log:\n  - Columns: `job_id`, `account_name`, `video_path`, `status` (pending/claimed/success/fail/skip), `worker_id`, `timestamp`, `error`.\n- Implement a small `progress_tracker.py` module with concurrency-safe operations based on file locking:\n  - Use `fcntl.flock` (Unix) or `msvcrt.locking`/`portalocker` (cross-platform) to protect updates so that parallel workers cannot claim the same job simultaneously (a best-practice for shared resources in parallel execution).[1]\n  - `claim_next_job(worker_id) -> Optional[PostJob]`:\n    - Acquire an exclusive lock on `progress.csv`.\n    - Load rows (in-memory or via streaming), find the first `status == \"pending\"` row.\n    - Mark it as `claimed` with `worker_id` and timestamp, rewrite the file atomically (e.g. write to temp file then rename).\n    - Release the lock and return the corresponding `PostJob`, or `None` if no pending jobs remain.\n  - `update_job_status(job_id, status, worker_id, error=None)`:\n    - Lock file, update the row, rewrite atomically, unlock.\n- Provide a bootstrap utility that, at orchestrator startup, seeds `progress.csv` from the main input CSV if it does not exist, assigning `job_id` indices that remain stable across runs.\n\n6) Orchestrator script to start/stop all workers\n- Implement a CLI script (e.g. `python -m geelark_ig_bot.parallel_orchestrator`) that:\n  - Loads `Config` using existing config mechanisms from Task 1 and compatible with Task 9.\n  - Reads desired `num_workers` and per-worker device/Appium port mapping from configuration.\n  - Initializes/validates the `progress.csv` file, ensuring all jobs are marked `pending` or appropriately resumed from a previous run.\n  - Starts worker processes with `multiprocessing.Process(target=worker_main, args=(...))` or by calling the module’s CLI via `subprocess.Popen`.\n  - Monitors children: optionally capture exit codes and restart on transient failure, or log and shut down gracefully.\n  - Handles signals:\n    - On SIGINT/SIGTERM, set a shared shutdown flag (e.g. `multiprocessing.Event` or a `shutdown` file), wait for workers to finish their current job, then terminate any stuck processes.\n\n7) Integration with existing posting logic\n- Reuse existing modules:\n  - Use Task 2’s `read_jobs` only in the orchestrator seeding step; workers should rely on `progress_tracker` for job acquisition.\n  - Use Task 7’s `run_post_job` as the per-job worker function, passing the worker-specific `GeelarkDeviceController` and `ClaudeNavigator` instances.\n  - Ensure proxy rotation (Task 5) and error-handling/logging (Task 9) continue to function as-is within each worker.\n- Avoid global singletons where possible; instantiate controller/navigator inside each worker process to keep state isolated.\n\n8) Clean shutdown, device cleanup, and fault tolerance\n- Within each worker:\n  - Track the current device session (driver) and ensure that on normal loop exit or exceptions, you:\n    - Attempt to close the app session and release the device (following typical parallel execution guidance to avoid dangling sessions).[1][3]\n    - Stop the Appium server via `stop_appium_server`.\n  - Make all teardown operations idempotent so that repeated shutdown attempts (from orchestrator and OS) do not crash.\n- Implement defensive behavior:\n  - If Appium fails to start or health-check fails, mark the worker as failed, log the error, and exit with a non-zero code.\n  - If a job fails due to Appium/device issues, mark job status as `fail` with error details in both progress and result logs.\n\n9) Logging and observability\n- Configure per-worker log files for:\n  - Worker Python logs (info, warning, error) including job IDs and account names.\n  - Appium server stdout/stderr.\n- Include job IDs and worker IDs in all structured logs (Task 9) to simplify debugging parallel issues, as recommended for parallel test execution environments.[1]\n\n10) Documentation and configuration\n- Add documentation to `README` or internal docs:\n  - How to configure the number of workers and mapping to devices.\n  - Port and systemPort allocation strategy.\n  - How progress tracking works and how to resume a partially completed run.\n  - Operational notes: typical CPU/memory impact when running multiple Appium servers concurrently.[1][3]\n- Expose the most important knobs via config: `num_workers`, `appium_start_cmd`, base Appium port, systemPort ranges, and log directory.\n",
        "testStrategy": "1) Unit tests for progress tracking\n- Test `claim_next_job` and `update_job_status` sequentially:\n  - Seed a temporary `progress.csv` with multiple pending jobs.\n  - Verify that `claim_next_job` returns jobs in order and marks them as `claimed` with the correct `worker_id`.\n  - Verify that `update_job_status` transitions rows to `success`, `fail`, or `skip` and persists changes.\n- Simulate contention by spawning 2–3 lightweight Python processes in tests that concurrently call `claim_next_job` against the same file and assert that no `job_id` is returned more than once.\n\n2) Unit tests for Appium server manager (where feasible with mocks)\n- Mock `subprocess.Popen` and the HTTP health-check:\n  - Ensure `start_appium_server` is called with the expected port and arguments.\n  - Verify that failed health-checks raise a clear exception.\n- Test `stop_appium_server` behavior when the process exits normally vs. hangs (ensure SIGKILL path is exercised).\n\n3) Unit/integration tests for worker logic (with fakes/mocks)\n- Use a fake `GeelarkDeviceController` and `ClaudeNavigator` that simulate successful postings without real devices.\n- Run `worker_main` against a small `progress.csv` and confirm that:\n  - All jobs transition from `pending` to `success`.\n  - The existing result log CSV contains one row per job with the correct status.\n  - The worker exits cleanly when no pending jobs remain.\n\n4) Multi-process integration test (local)\n- Start the orchestrator with 2–3 workers using the fake controller/navigator and a small input CSV (e.g. 10 jobs).\n- Assert that:\n  - All jobs are processed exactly once (no duplicates, no missing jobs) by inspecting `progress.csv`.\n  - Work is distributed across workers (different `worker_id` values present).\n  - Orchestrator exits with zero status and all worker processes have exited.\n\n5) Signal handling and clean shutdown tests\n- In an integration-style test, start the orchestrator with long-running fake jobs (each job sleeps a few seconds).\n- Send SIGINT (or simulated shutdown signal) to the orchestrator process and assert that:\n  - Workers finish or abort their current job, update job status appropriately (e.g. leave unstarted jobs as `pending`).\n  - Appium server processes (mocked) receive `stop_appium_server` calls.\n\n6) Real-device/Appium smoke test (manual or CI environment)\n- Connect at least two Android devices (or Geelark cloud devices mapped via the controller) and configure two workers with distinct Appium ports and systemPorts.\n- Start the orchestrator with a small CSV (e.g. 2–4 jobs) and visually confirm:\n  - Two Appium servers run on the expected ports.\n  - Each device is driven only by its assigned worker.\n  - Posts are successfully created and logged once per job.\n- Inspect logs to verify that no UiAutomator2/systemPort conflicts or session collisions occur, aligning with recommended Appium parallel execution patterns.[1][3][6]\n\n7) Regression tests with existing single-worker flow\n- Run a single-worker configuration and verify that behavior matches the existing Task 9/10 MVP: same success rate, logging format, and proxy rotation behavior.\n- Confirm that enabling parallel mode does not require changes to the per-job posting logic (i.e. `run_post_job` remains unchanged aside from Appium config injection).",
        "status": "done",
        "dependencies": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "7",
          "9",
          "10"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Design parallel orchestration architecture and configuration for multi-process workers",
            "description": "Define the overall architecture for multi-process Appium workers, including process model, per-worker isolation strategy, and configuration structures for devices, ports, and logging.",
            "dependencies": [],
            "details": "- Specify how the orchestrator module (e.g. `parallel_orchestrator.py`) will spawn and manage N worker processes using `multiprocessing.Process` or `subprocess`.\n- Design a `ParallelConfig`/similar structure (in `config.py` or `parallel_config.py`) that maps worker IDs to: device identifier/UDID, Appium port, systemPort range, log directory paths, and any extra Appium args.\n- Define how configuration is loaded from existing config (Task 1/9) and extended with parallel-specific fields like `num_workers`, `base_appium_port`, `system_port_block_size`.\n- Document invariants (e.g. unique Appium ports, non-overlapping systemPort ranges, one device per worker) and how they will be validated at startup.\n- Decide on basic IPC/shared-state mechanisms (CSV files, optional shutdown flag via file or `multiprocessing.Event`) and how workers discover shared paths (e.g. `progress_csv_path`, `results_csv_path`, `logs_dir`).",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-13T00:34:05.624Z"
          },
          {
            "id": 2,
            "title": "Implement Appium server manager and per-worker process entrypoint",
            "description": "Create the Appium server lifecycle utilities and the worker main function that owns a dedicated Appium server, device controller, and posting loop.",
            "dependencies": [
              1
            ],
            "details": "- Implement `appium_server_manager.py` with:\n  - `start_appium_server(port: int, log_path: str, extra_args: list[str]) -> subprocess.Popen` that launches the `appium` binary with `--port` and other required arguments, redirects stdout/stderr to a per-worker log file, and performs `/status` health checks with retry and timeout.\n  - `stop_appium_server(proc: subprocess.Popen, timeout: float = 10.0)` that sends SIGTERM and escalates to SIGKILL if the server does not exit in time.\n- Implement `worker_main(worker_id: int, config: ParallelConfig, shared_paths: WorkerSharedPaths)` in a new worker module:\n  - Initialize worker-specific logging (file and console) including worker ID in log records.\n  - Resolve worker-specific device/Appium configuration (UDID, Appium port, systemPort range, log paths) from `ParallelConfig`.\n  - Start the Appium server using `start_appium_server` and construct desired capabilities with unique `udid` and `systemPort` values within the worker’s allocated range.\n  - Instantiate `GeelarkDeviceController`, `ClaudeNavigator`, and other dependencies, ensuring all state is local to the process.\n  - Implement a guarded `try/finally` to guarantee teardown: close active driver session, perform device cleanup, and call `stop_appium_server` even on errors or external shutdown.\n- Ensure no global singletons are shared across workers; all per-worker objects are created inside `worker_main`.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-13T00:35:25.054Z"
          },
          {
            "id": 3,
            "title": "Build CSV-based progress tracker with file locking and seeding from input jobs",
            "description": "Implement a concurrency-safe CSV progress tracking module that coordinates job claiming and status updates across workers, and a bootstrap step to seed it from the main input CSV.",
            "dependencies": [
              1
            ],
            "details": "- Design the schema for `progress.csv` with at least columns: `job_id`, `account_name`, `video_path`, `caption` (if needed for reconstruction), `status`, `worker_id`, `timestamp`, `error`.\n- Implement `progress_tracker.py` providing:\n  - Cross-platform file-locking utilities (e.g. using `fcntl.flock` on Unix and `msvcrt`/`portalocker` on Windows) to guard read-modify-write cycles.\n  - `claim_next_job(worker_id: int) -> Optional[PostJob]` that:\n    - Acquires an exclusive lock on `progress.csv`.\n    - Reads rows, finds the first `status == \"pending\"`, sets it to `\"claimed\"` with `worker_id` and timestamp.\n    - Rewrites the CSV atomically via a temp file and rename, then releases the lock.\n    - Returns a constructed `PostJob` (compatible with Task 2/7) or `None` if no pending jobs remain.\n  - `update_job_status(job_id: int, status: str, worker_id: int, error: str | None = None)` that locks, updates the row, rewrites atomically, and unlocks.\n- Implement a seeding/bootstrap utility (callable from the orchestrator) that:\n  - Uses existing `read_jobs` (Task 2) to load jobs from the main input CSV when `progress.csv` does not exist.\n  - Assigns stable `job_id`s, writes initial rows with `status=\"pending\"`, and preserves any existing progress when resuming.\n- Ensure functions are robust to partial files and can recover or fail clearly on CSV corruption.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-13T00:37:06.708Z"
          },
          {
            "id": 4,
            "title": "Implement worker job-processing loop integrating posting logic and progress tracking",
            "description": "Wire the worker loop to claim jobs from the progress tracker, run the existing posting flow, log outputs, and update job status with robust error handling and shutdown awareness.",
            "dependencies": [
              2,
              3
            ],
            "details": "- Inside `worker_main`, implement the main loop that:\n  - Periodically checks a shutdown flag (e.g. `multiprocessing.Event` passed from orchestrator or a special \"shutdown\" file) to determine whether to stop after the current job.\n  - Calls `claim_next_job(worker_id)` from `progress_tracker` and exits the loop when it returns `None` (no more pending jobs).\n  - For each claimed job:\n    - Ensures proxy rotation (Task 5) is invoked before posting, using existing network utilities.\n    - Calls `run_post_job(job, controller, navigator, config)` from Task 7 within a `try/except` block.\n    - On success, writes a row to the existing result log CSV via `append_log_row` (Task 2) and calls `update_job_status(job_id, \"success\", worker_id, error=None)`.\n    - On exception, logs structured error information, writes a `status=\"fail\"` row to the result log CSV, and calls `update_job_status(job_id, \"fail\", worker_id, error=str(exc))`.\n  - Ensures that any Appium/device-specific failures are surfaced clearly and that repeated failures do not corrupt `progress.csv`.\n- Include worker ID, job ID, and account name in all worker logs for observability consistent with Task 9.\n- Make the loop resilient to transient tracker I/O errors (e.g. small retry on file-lock failures) while avoiding duplicate job processing.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-13T00:38:09.363Z"
          },
          {
            "id": 5,
            "title": "Create orchestrator CLI to configure, launch, monitor, and gracefully shut down all workers",
            "description": "Implement the top-level orchestrator script that initializes configuration and progress tracking, spawns worker processes, monitors their lifecycle, and handles clean shutdown and restarts.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "- Implement a CLI entrypoint (e.g. `python -m geelark_ig_bot.parallel_orchestrator`) that:\n  - Loads the base `Config` and parallel extensions (num workers, device/port mappings, log directories).\n  - Validates the configuration invariants: unique Appium ports, non-overlapping systemPort ranges, valid device IDs, and accessible log directories.\n  - Initializes or resumes `progress.csv` via the seeding utility, ensuring consistent `job_id`s and correct `pending`/`claimed`/`success`/`fail` states.\n- Use `multiprocessing.Process` (or `subprocess.Popen` on the same module) to spawn one worker per configured device/worker ID, passing in the resolved `ParallelConfig` subset and shared paths.\n- Implement monitoring logic that:\n  - Tracks process handles, logs start/stop events with exit codes, and optionally restarts workers on transient failures according to a simple policy (e.g. limited restart count).\n  - Periodically checks for overall completion (all jobs non-pending and all workers idle/exited).\n- Implement signal handling for SIGINT/SIGTERM:\n  - Set a shared shutdown flag or create a `shutdown` file that workers poll, allowing them to finish the current job and exit their loops.\n  - After a grace period, terminate or kill any stuck worker processes and ensure Appium servers are torn down.\n- Add basic documentation/comments describing how to run the orchestrator, configure workers and ports, and resume from partial runs; update README or internal docs accordingly.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-13T00:40:29.724Z"
          }
        ],
        "updatedAt": "2025-12-13T00:40:29.724Z"
      },
      {
        "id": "20",
        "title": "Implement unified per-account daily posting limits and orchestrator safety controls",
        "description": "Implement a unified posting control system that enforces per-account daily limits, adds orchestrator process safety checks, and introduces a controlled daily reset flow with documentation updates.",
        "details": "Implementation should focus on two modules (`progress_tracker.py`, `parallel_orchestrator.py`) plus `CLAUDE.md`, building on the existing CSV-based progress tracking and multi-process orchestration from Task 19.\n\n1) Introduce unified per-account daily post limit configuration\n- Add a configuration entry (e.g. in existing `config.py` or equivalent used by Task 19) for **`max_posts_per_account_per_day`**.\n  - Default to **1**.\n  - Allow integer values 1–4 (validate and raise on invalid values; keep the API open for future extension).\n- Ensure this value is accessible to both `progress_tracker.py` and `parallel_orchestrator.py` without circular imports (pass via function parameters or a small config object where appropriate, instead of importing global state).\n- Follow current best practices for configuration: avoid hard-coded constants, and keep default in a single source of truth so tests can override it easily.\n\n2) Enhance `seed_from_scheduler_state()` for per-account limits\n- In `progress_tracker.py`, extend `seed_from_scheduler_state()` to:\n  - Read the existing progress CSV (the one introduced in Task 19) and compute a **`success_count_by_account: dict[str, int]`**.\n    - Count rows where the job is in a terminal **success** state (re-use existing status field semantics from Task 19 to avoid double-defining what “success” means).\n    - Use a streaming/iterator-based CSV read to avoid excessive memory use on large files.\n  - Filter candidate accounts when seeding from the scheduler/state so that only accounts where `success_count < max_posts_per_account_per_day` are considered for new assignments.\n  - Maintain an **in-memory** `success_count_by_account` during seeding and increment counts as new jobs are seeded, so that multiple jobs added in one seeding pass do not exceed the limit even before they are written back.\n- Refactor as needed:\n  - Extract small helpers such as `_load_success_counts(progress_path) -> dict[str, int]` for testability.\n  - Keep all file I/O atomic (e.g. write temp file and rename) if seeding mutates the CSV, to avoid corruption under concurrent reads.\n- Document function behavior clearly in docstrings so that both orchestrator and future tools can rely on the same semantics.\n\n3) Add orchestrator startup safety check for duplicate Python orchestrators\n- In `parallel_orchestrator.py`, before spawning worker processes, add a **startup guard** that checks for other running Python orchestrator processes.\n  - Use a cross-platform-friendly approach such as `psutil.process_iter()` if already available in the project; otherwise, add a minimal, well-scoped dependency or implement a simple `subprocess`-based check, keeping in mind:\n    - Only treat **other processes** as conflicts (ignore the current PID).\n    - Match by a robust criterion, e.g. command line including the orchestrator entrypoint/module name or a specific `--orchestrator` flag.\n  - If another orchestrator is detected, log a clear error and exit non-zero instead of starting new workers.\n- Ensure the check is **read-only** (no OS-level locks, per requirements) and fails fast before creating any worker processes or touching progress files.\n- Make the behavior configurable for tests (e.g. allow an environment variable or explicit flag to bypass the check in unit tests), but keep the default behavior strict in production.\n\n4) Implement controlled daily reset command with archival behavior\n- In `parallel_orchestrator.py` (or a small CLI wrapper if that’s where CLI parsing lives), add a **`--reset-day`** command/flag.\n  - On invocation, the command should:\n    - Locate the current progress CSV (respecting existing config from Task 19).\n    - Compute an archive filename: `parallel_progress_YYYYMMDD.csv` based on **current local date** or a configurable timezone; document the choice and keep it consistent.\n      - If a file with that name already exists, either:\n        - Append a suffix such as `_1`, `_2`, etc., or\n        - Fail with a clear error; pick one strategy and document it.\n    - Move/rename the current progress file to the archive filename (not copy+delete; use atomic rename where possible).\n    - Create a **fresh progress CSV** initialized with the correct header and any required initial state for a new day (e.g. pending jobs seeded from the scheduler, if that is part of reset semantics).\n  - Never delete the progress file outright; the reset command must only archive and recreate.\n- Implement reset logic in a dedicated function (e.g. `reset_day(progress_path, archive_dir=None)`), which can be called from CLI parsing and unit tests.\n- Consider concurrency: ensure the reset operation is performed when no orchestrator workers are running; if needed, add a defensive check that refuses to reset when an orchestrator process is currently detected (re-use the process detection logic).\n\n5) Update progress handling and `claim_next_job()` for defense in depth\n- In `progress_tracker.py`, update `claim_next_job()` (introduced in Task 19) to enforce the same **per-account daily limit** in addition to seeding-time checks.\n  - Before returning a job for a given account, compute or reuse `success_count_by_account` so that jobs are skipped when `success_count >= max_posts_per_account_per_day`.\n  - Decide on behavior when a job is skipped because of the limit (e.g. treat as permanently skipped with a specific status like `daily_limit_reached`, or simply not claim it and move on to the next row). Document this behavior and ensure it is consistent with reporting.\n  - Avoid O(N²) scans over the CSV for large inputs: if feasible, maintain a cached `success_count_by_account` that can be refreshed when needed, or compute counts once per orchestrator run rather than per-claim.\n- Ensure that both `seed_from_scheduler_state()` and `claim_next_job()` share the same limit logic and do not diverge over time (e.g. via a `_within_daily_limit(account, counts, max_per_day)` helper).\n\n6) Documentation updates in CLAUDE.md\n- Edit `CLAUDE.md` to include **strict operational rules** around progress tracking and resets:\n  - Explicitly state: **NEVER delete the progress file manually.**\n  - For starting a new operational day, always run the **`--reset-day`** command instead of deleting or editing progress CSVs by hand.\n  - Include a short explanation of the per-account daily limit behavior so human operators understand why some jobs may remain unposted once the limit is hit.\n  - Add example CLI invocations for:\n    - Starting the orchestrator normally.\n    - Running `--reset-day`.\n- Keep language concise and imperative so it can be used as a system prompt or operator runbook.\n\n7) General code quality and patterns\n- Maintain consistency with patterns established in Task 19: structured logging, error handling, and CLI parsing.\n- Add type hints and docstrings for new/changed functions, and keep them in sync with behavior.\n- Ensure any new dependencies (e.g. `psutil`) are declared in the project’s dependency management (requirements file, Poetry, etc.) and are optional where appropriate.\n- Where feasible, design new logic to be testable without real orchestrator processes or actual CSV files by abstracting filesystem and process listing behind small helpers that can be mocked.\n",
        "testStrategy": "1) Unit tests for per-account daily limits\n- Create a temporary progress CSV with multiple accounts and a mix of `success`, `fail`, and `pending` rows.\n- Test `_load_success_counts` (or equivalent) to ensure only successful posts are counted per account, and counts match expectations.\n- Configure `max_posts_per_account_per_day = 1` and verify that `seed_from_scheduler_state()` only seeds accounts with `success_count < 1`.\n- With `max_posts_per_account_per_day = 2`, simulate seeding multiple jobs for the same account in a single call and assert that in-memory counts prevent creating more than 2 total for that account.\n\n2) Unit tests for `claim_next_job()` enforcement\n- Seed a test progress CSV with:\n  - An account already at the daily limit (based on existing `success` rows).\n  - Another account below the limit.\n- Call `claim_next_job()` repeatedly and assert that:\n  - Jobs for the over-limit account are not claimed (either skipped or marked with `daily_limit_reached`, according to the chosen design).\n  - Jobs for accounts under the limit are claimed and marked as such, and that repeated calls never exceed the per-account limit.\n- Verify that performance is acceptable by running `claim_next_job()` over a CSV with hundreds or thousands of rows in tests (avoid quadratic behavior).\n\n3) Tests for orchestrator startup safety check\n- Implement the process-detection logic in a function that accepts a list of mock process descriptors to enable pure unit testing.\n- Provide fake process lists including:\n  - Only the current process (should not block startup).\n  - Another process whose command line clearly indicates it is an orchestrator (should block startup).\n  - Unrelated Python processes (should not block startup, assuming matching criteria are specific enough).\n- Assert that when a conflicting orchestrator is detected, the orchestrator entrypoint logs an appropriate error and exits with a non-zero code.\n\n4) Tests for `--reset-day` archival behavior\n- Use a temporary directory to host a fake progress CSV file with a known name and simple contents.\n- Invoke the reset function directly (e.g. `reset_day(path)`):\n  - Assert that the original file is no longer present and an archive file `parallel_progress_YYYYMMDD.csv` exists with identical contents.\n  - Assert that a new progress CSV is created with the correct header and no historical rows.\n- Test the behavior when an archive for the current date already exists:\n  - If the design appends a numeric suffix, verify the new name (e.g. `parallel_progress_YYYYMMDD_1.csv`).\n  - If the design is to fail, assert that an appropriate exception or error code is produced.\n- Add a test ensuring reset refuses to run (or logs a strong warning) if the process-detection logic indicates an orchestrator is currently running.\n\n5) Integration tests for end-to-end posting control\n- With a small test CSV of jobs for multiple accounts, run the orchestrator in a test mode that uses a mock/posting stub instead of real devices.\n- Set `max_posts_per_account_per_day = 1` and verify after a full run that:\n  - No account has more than one `success` row in the progress CSV.\n  - Jobs beyond the limit remain unposted or are marked according to the chosen policy.\n- Repeat with `max_posts_per_account_per_day = 2` to ensure the system respects higher limits as well.\n\n6) Documentation verification\n- Add a test (or CI check) that ensures `CLAUDE.md` contains the key phrases: `NEVER delete progress file` and `--reset-day` (e.g. a simple text search in a docs-checking script).\n- Optionally include a human-reviewed checklist item during code review to confirm that examples and instructions in `CLAUDE.md` match actual CLI flags and behavior.\n",
        "status": "done",
        "dependencies": [
          "1",
          "2",
          "9",
          "19"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Add configurable per-account daily posting limit to parallel_config.py",
            "description": "Introduce a unified max_posts_per_account_per_day configuration entry in parallel_config.py that can be used by both progress_tracker.py and parallel_orchestrator.py without circular imports.",
            "dependencies": [],
            "details": "Add max_posts_per_account_per_day field to ParallelConfig dataclass in parallel_config.py with default value of 1. Implement validation to allow only integer values 1-4 with clear error on invalid values. Update get_config() to accept this parameter. The config should be passable via function parameters to seed_from_scheduler_state() and claim_next_job() to avoid global state imports. Add a _validate_daily_limit() helper that raises ValueError for out-of-range values. Update print_config() to display the limit setting. Ensure the default is a single source of truth that tests can override.",
            "status": "pending",
            "testStrategy": "Unit test that ParallelConfig validates max_posts_per_account_per_day correctly: accepts 1-4, rejects 0, 5, negative numbers, and non-integers. Test that get_config() properly returns configs with custom limit values. Test that print_config() displays the limit.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Enhance seed_from_scheduler_state() with configurable per-account limits and success counting",
            "description": "Extend progress_tracker.py's seed_from_scheduler_state() to count successful posts per account from the progress CSV and filter accounts that have reached their daily limit, using the configurable max_posts_per_account_per_day parameter.",
            "dependencies": [
              1
            ],
            "details": "In progress_tracker.py: 1) Add _load_success_counts(progress_path) -> Dict[str,int] helper that iterates through CSV rows and counts STATUS_SUCCESS entries per account using a streaming reader to avoid memory issues. 2) Modify seed_from_scheduler_state() signature to accept max_posts_per_day: int = 1 parameter. 3) Replace the current hardcoded '1 post per account' logic with dynamic limit checking: compute success_count_by_account, filter available_accounts where count < max_posts_per_day, and maintain in-memory counts during seeding to prevent exceeding limit within a single seeding pass. 4) Add _within_daily_limit(account, counts, max_per_day) -> bool helper to share limit logic between seeding and claiming. 5) Ensure atomic file I/O is preserved - the existing temp file + rename pattern already handles this. Add clear docstrings documenting the behavior.",
            "status": "pending",
            "testStrategy": "Create temp progress CSV with multiple accounts having varying success counts (0, 1, 2 successes). Test _load_success_counts returns correct counts per account. Test seed_from_scheduler_state() with max_posts_per_day=2: verify accounts with 2+ successes are excluded, accounts with 0-1 are included. Test in-memory count tracking prevents assigning multiple jobs to same account in one seed pass when limit would be exceeded.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add orchestrator startup guard to detect duplicate running orchestrators",
            "description": "Add a process detection check at startup in parallel_orchestrator.py that prevents running multiple orchestrator instances simultaneously, using subprocess-based approach to avoid adding psutil dependency.",
            "dependencies": [],
            "details": "In parallel_orchestrator.py: 1) Add _check_duplicate_orchestrator() -> Tuple[bool, str] function that uses subprocess to check for other Python processes running parallel_orchestrator.py. On Windows: use 'wmic process where \"name='python.exe'\" get processid,commandline' or 'tasklist /v'. On Unix: use 'ps aux | grep parallel_orchestrator'. 2) Filter out current process by comparing PIDs (os.getpid()). 3) Match by command line containing 'parallel_orchestrator' to identify orchestrator processes. 4) Call this check at the very start of run_parallel_posting() before any cleanup or worker spawning. 5) If another orchestrator is detected, log a clear error message with the detected PID and exit with sys.exit(1). 6) Add BYPASS_ORCHESTRATOR_CHECK environment variable that tests can set to skip the check. 7) Keep the check read-only (no OS-level locks as specified in requirements). Add to main() CLI as well.",
            "status": "pending",
            "testStrategy": "Test _check_duplicate_orchestrator() in isolation by mocking subprocess output to simulate another orchestrator running. Verify it correctly identifies other orchestrator processes and ignores current PID. Verify BYPASS_ORCHESTRATOR_CHECK environment variable allows bypassing the check. Verify run_parallel_posting exits early when duplicate detected.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement --reset-day command with progress file archival",
            "description": "Add a --reset-day CLI flag to parallel_orchestrator.py that archives the current progress CSV to a dated filename and creates a fresh progress file, with safety checks to prevent reset while workers are running.",
            "dependencies": [
              3
            ],
            "details": "In parallel_orchestrator.py: 1) Add reset_day(progress_path: str, archive_dir: str = None) -> str function that: a) Computes archive filename as parallel_progress_YYYYMMDD.csv using local date; b) If archive file exists, append _1, _2, etc. suffix; c) Uses shutil.move() for atomic rename to archive; d) Creates fresh progress CSV with correct headers only. 2) Before reset, call _check_duplicate_orchestrator() to refuse reset if any orchestrator is running - reuse the detection logic from subtask 3. 3) Add --reset-day flag to argparse in main(). 4) When invoked, check no orchestrator running, perform reset, log archive path. 5) Document timezone assumption (local time) in docstring. 6) Return the archive filename for logging/testing. Handle case where progress file doesn't exist - just create empty one.",
            "status": "pending",
            "testStrategy": "Test reset_day() creates archive with correct YYYYMMDD format. Test suffix incrementing when archive already exists (_1, _2). Test fresh CSV has correct headers. Test reset refuses to run when orchestrator detection reports duplicate. Test reset works when progress file doesn't exist (creates new empty file).",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Update claim_next_job() for defense-in-depth limit enforcement and update CLAUDE.md documentation",
            "description": "Add per-account daily limit enforcement to claim_next_job() as defense-in-depth against seeding-time limit bypass, and update CLAUDE.md with operational rules for the new features.",
            "dependencies": [
              1,
              2
            ],
            "details": "In progress_tracker.py: 1) Modify claim_next_job() signature to accept max_posts_per_day: int = 1 parameter. 2) Within _claim_operation, compute success_count_by_account once at the start using the same counting logic as _load_success_counts. 3) When iterating pending jobs, add check: if success count for job's account >= max_posts_per_day, skip the job (log at debug level 'Skipping job X - account Y at daily limit'). 4) Reuse _within_daily_limit() helper from subtask 2 to ensure consistent logic. 5) Avoid O(N^2) by computing counts once before the loop. In CLAUDE.md: Add new section '## Parallel Posting Daily Limits' documenting: a) NEVER delete progress file manually - always use --reset-day; b) Per-account daily limit behavior and how to change the limit; c) Example CLI invocations for starting orchestrator and running --reset-day; d) Explain jobs may remain unposted when account limits are hit.",
            "status": "pending",
            "testStrategy": "Test claim_next_job() with progress file containing account at limit - verify it skips jobs for that account. Test claim_next_job() correctly claims jobs for accounts under limit. Test that seeding and claiming use consistent limit logic (same account excluded by both). Review CLAUDE.md updates for accuracy and completeness.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-13T08:11:06.639Z"
      },
      {
        "id": "21",
        "title": "Port retry logic from PostingScheduler to parallel orchestrator/worker system",
        "description": "Add automatic retry capabilities to the parallel posting system by porting the existing retry patterns from PostingScheduler, including attempts tracking, RETRYING status, retry delay configuration, and periodic retry job reclamation.",
        "details": "## Overview\nThe existing `posting_scheduler.py` has a robust auto-retry mechanism (lines 294-300, 809-924) that needs to be ported to the parallel orchestrator/worker architecture. Currently, jobs that fail in the parallel system remain permanently failed.\n\n## Implementation Details\n\n### 1. Extend progress_tracker.py with retry fields and status\n\n**Add new status constant (around line 83):**\n```python\nSTATUS_RETRYING = 'retrying'\n```\n\n**Extend COLUMNS list (line 73-76) to include retry tracking:**\n```python\nCOLUMNS = [\n    'job_id', 'account', 'video_path', 'caption', 'status',\n    'worker_id', 'claimed_at', 'completed_at', 'error',\n    'attempts', 'max_attempts', 'last_attempt', 'error_type'  # NEW\n]\n```\n\n**Add new method `get_retry_jobs()` to ProgressTracker class:**\n```python\ndef get_retry_jobs(self, retry_delay_minutes: float = 0.25) -> List[Dict[str, Any]]:\n    \"\"\"Get jobs that are in RETRYING status and ready for retry.\n    \n    A job is ready for retry if:\n    1. status == STATUS_RETRYING\n    2. (now - last_attempt) >= retry_delay_minutes\n    \n    Args:\n        retry_delay_minutes: Minimum time since last attempt before retry\n        \n    Returns:\n        List of jobs ready to be retried\n    \"\"\"\n    jobs = self._read_all_jobs()\n    ready_jobs = []\n    now = datetime.now()\n    \n    for job in jobs:\n        if job.get('status') == self.STATUS_RETRYING:\n            last_attempt = job.get('last_attempt', '')\n            if last_attempt:\n                try:\n                    attempt_time = datetime.fromisoformat(last_attempt)\n                    elapsed_minutes = (now - attempt_time).total_seconds() / 60\n                    if elapsed_minutes >= retry_delay_minutes:\n                        ready_jobs.append(job)\n                except:\n                    ready_jobs.append(job)  # If can't parse, allow retry\n            else:\n                ready_jobs.append(job)\n    \n    return ready_jobs\n```\n\n**Update seed_from_scheduler_state() to initialize retry fields in new jobs (around line 307-317):**\n```python\nnew_jobs.append({\n    'job_id': job.get('id', ''),\n    'account': assigned_account,\n    'video_path': job.get('video_path', ''),\n    'caption': job.get('caption', ''),\n    'status': self.STATUS_PENDING,\n    'worker_id': '',\n    'claimed_at': '',\n    'completed_at': '',\n    'error': '',\n    'attempts': '0',           # NEW\n    'max_attempts': '3',       # NEW - default from posting_scheduler.py\n    'last_attempt': '',        # NEW\n    'error_type': ''           # NEW\n})\n```\n\n### 2. Extend parallel_config.py with retry settings\n\nAdd these fields to ParallelConfig dataclass:\n```python\n@dataclass\nclass ParallelConfig:\n    # ... existing fields ...\n    retry_delay_minutes: float = 0.25  # 15 seconds, same as PostingScheduler\n    max_attempts: int = 3  # Same as PostJob.max_attempts default\n    non_retryable_errors: tuple = ('suspended', 'captcha', 'logged_out', 'action_blocked')\n```\n\n### 3. Update parallel_worker.py to handle retries\n\n**Modify the main job processing loop (around lines 279-340) to:**\n\na) **Increment attempts when claiming a job:**\n   - After claiming, increment the `attempts` field\n   - Update `last_attempt` timestamp\n\nb) **Check claim_next_job AND get_retry_jobs:**\n```python\n# First try to claim a pending job\njob = tracker.claim_next_job(worker_id, max_posts_per_account_per_day=config.max_posts_per_account_per_day)\n\n# If no pending jobs, check for retry jobs that are ready\nif job is None:\n    retry_jobs = tracker.get_retry_jobs(retry_delay_minutes=config.retry_delay_minutes)\n    if retry_jobs:\n        # Claim the first ready retry job\n        job = tracker.claim_retry_job(retry_jobs[0]['job_id'], worker_id)\n```\n\nc) **After job failure, decide retry vs permanent fail (port logic from posting_scheduler.py lines 905-915):**\n```python\ndef should_retry(job: dict, error_type: str, config: ParallelConfig) -> bool:\n    \"\"\"Determine if a failed job should be retried.\"\"\"\n    # Don't retry account-level errors\n    if error_type in config.non_retryable_errors:\n        return False\n    \n    attempts = int(job.get('attempts', 0))\n    max_attempts = int(job.get('max_attempts', config.max_attempts))\n    \n    return attempts < max_attempts\n```\n\nd) **Update job status based on retry decision:**\n```python\nif success:\n    tracker.update_job_status(job_id, 'success', worker_id)\nelse:\n    error_type = extract_error_type(error)  # Parse error message\n    if should_retry(job, error_type, config):\n        tracker.update_job_status(\n            job_id, \n            tracker.STATUS_RETRYING,  # Move to RETRYING instead of FAILED\n            worker_id, \n            error=error,\n            attempts=int(job.get('attempts', 0)) + 1,\n            error_type=error_type\n        )\n        logger.info(f\"Job {job_id} will retry (attempt {attempts}/{max_attempts})\")\n    else:\n        tracker.update_job_status(job_id, 'failed', worker_id, error=error)\n        logger.info(f\"Job {job_id} permanently failed: {error_type}\")\n```\n\n### 4. Add claim_retry_job method to ProgressTracker\n\n```python\ndef claim_retry_job(self, job_id: str, worker_id: int, max_posts_per_account_per_day: int = 1) -> Optional[Dict[str, Any]]:\n    \"\"\"Claim a specific job that is in RETRYING status.\n    \n    This is similar to claim_next_job but for a specific retry job.\n    Still enforces account-in-use and daily limit checks.\n    \"\"\"\n    def _claim_retry_operation(jobs):\n        # Build accounts in use and success counts (same as claim_next_job)\n        accounts_in_use = set()\n        success_counts = {}\n        for job in jobs:\n            if job.get('status') == self.STATUS_CLAIMED:\n                if job.get('account'):\n                    accounts_in_use.add(job.get('account'))\n            elif job.get('status') == self.STATUS_SUCCESS:\n                acc = job.get('account', '')\n                if acc:\n                    success_counts[acc] = success_counts.get(acc, 0) + 1\n        \n        # Find and claim the target job\n        for job in jobs:\n            if job.get('job_id') == job_id and job.get('status') == self.STATUS_RETRYING:\n                account = job.get('account', '')\n                \n                # Safety checks\n                if not account:\n                    return jobs, None\n                if account in accounts_in_use:\n                    return jobs, None\n                if success_counts.get(account, 0) >= max_posts_per_account_per_day:\n                    return jobs, None\n                \n                # Claim the job\n                job['status'] = self.STATUS_CLAIMED\n                job['worker_id'] = str(worker_id)\n                job['claimed_at'] = datetime.now().isoformat()\n                return jobs, dict(job)\n        \n        return jobs, None\n    \n    return self._locked_operation(_claim_retry_operation)\n```\n\n### 5. Update update_job_status to handle retry fields\n\nModify `update_job_status` method signature and implementation:\n```python\ndef update_job_status(\n    self,\n    job_id: str,\n    status: str,\n    worker_id: int,\n    error: str = '',\n    attempts: int = None,\n    error_type: str = ''\n) -> bool:\n    \"\"\"Update job status with optional retry tracking fields.\"\"\"\n    def _update_operation(jobs):\n        for job in jobs:\n            if job.get('job_id') == job_id:\n                job['status'] = status\n                job['worker_id'] = str(worker_id)\n                job['completed_at'] = datetime.now().isoformat()\n                job['error'] = error[:500] if error else ''\n                job['last_attempt'] = datetime.now().isoformat()  # Always update\n                \n                if attempts is not None:\n                    job['attempts'] = str(attempts)\n                if error_type:\n                    job['error_type'] = error_type\n                    \n                return jobs, True\n        return jobs, False\n    \n    return self._locked_operation(_update_operation)\n```\n\n### 6. Update get_stats to include retrying count\n\n```python\ndef get_stats(self) -> Dict[str, int]:\n    \"\"\"Get job status statistics.\"\"\"\n    jobs = self._read_all_jobs()\n    stats = {\n        'total': len(jobs),\n        'pending': 0,\n        'claimed': 0,\n        'success': 0,\n        'failed': 0,\n        'skipped': 0,\n        'retrying': 0  # NEW\n    }\n    for job in jobs:\n        status = job.get('status', '')\n        if status in stats:\n            stats[status] += 1\n    return stats\n```\n\n### Reference Files\n- Source patterns: `posting_scheduler.py` lines 294-300 (PostStatus enum), 809-924 (execute_job retry logic), 793-795 (get_retry_jobs)\n- Target files: `progress_tracker.py`, `parallel_worker.py`, `parallel_config.py`",
        "testStrategy": "## Testing Strategy\n\n### 1. Unit Tests for ProgressTracker\n\n**Test new STATUS_RETRYING constant:**\n```python\ndef test_status_retrying_exists():\n    assert ProgressTracker.STATUS_RETRYING == 'retrying'\n```\n\n**Test get_retry_jobs() method:**\n- Create progress file with jobs in various states (pending, claimed, success, failed, retrying)\n- Set `last_attempt` timestamps at different times\n- Verify only RETRYING jobs with elapsed delay are returned\n- Test edge cases: missing last_attempt, unparseable timestamps\n\n**Test claim_retry_job() method:**\n- Verify it claims only RETRYING jobs (not pending/claimed/failed)\n- Verify account-in-use check prevents claiming\n- Verify daily limit check prevents claiming\n- Verify job transitions to CLAIMED status after successful claim\n\n**Test updated update_job_status():**\n- Verify attempts field is updated correctly\n- Verify error_type field is stored\n- Verify last_attempt is updated\n\n### 2. Integration Tests for Parallel Worker\n\n**Test retry flow end-to-end:**\n1. Seed progress file with test jobs\n2. Manually claim a job\n3. Call update_job_status with RETRYING status\n4. Verify job appears in get_retry_jobs() after delay\n5. Verify worker can claim the retry job\n6. Complete job, verify it reaches success or permanent fail\n\n**Test non-retryable errors:**\n1. Simulate failure with error_type='suspended'\n2. Verify job goes directly to FAILED (not RETRYING)\n3. Verify job does NOT appear in get_retry_jobs()\n\n**Test max_attempts exhaustion:**\n1. Create job with attempts=2, max_attempts=3\n2. Fail the job\n3. Verify it moves to RETRYING (attempt 3)\n4. Fail again\n5. Verify it moves to FAILED (exhausted retries)\n\n### 3. Live Tests (Per CLAUDE.md Instructions)\n\n**Run with actual orchestrator:**\n```bash\n# Seed with a few test accounts\npython parallel_orchestrator.py --seed-only\n\n# Manually edit one job in parallel_progress.csv to have a bad video path (will fail)\n# Run with 1 worker to observe retry behavior\npython parallel_orchestrator.py --run --workers 1\n```\n\n**Verify in logs:**\n- Check worker log for \"will retry\" messages\n- Check progress CSV for RETRYING status entries\n- Verify retrying jobs get re-claimed after delay\n- Verify jobs eventually succeed or reach permanent failure\n\n### 4. Regression Tests\n\n**Verify backward compatibility:**\n- Progress files without new columns should still work\n- Workers should handle missing attempts/max_attempts gracefully (use defaults)\n- Existing pending/claimed/success/failed flows unchanged\n\n### 5. Stress Test\n\n**Run with 5 workers as specified in review1.txt:**\n```bash\npython parallel_orchestrator.py --run --workers 5\n```\nVerify:\n- Multiple workers can claim retry jobs without conflicts\n- No duplicate posts occur during retry handling\n- Stats correctly show retrying count",
        "status": "done",
        "dependencies": [
          "19"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-13T05:21:54.104Z"
      },
      {
        "id": "22",
        "title": "Fix per-account daily cap enforcement with robust progress file handling",
        "description": "Strengthen the per-account daily posting limits by implementing claim-time enforcement that considers both success and claimed job counts, preventing same-account reuse during reseeding, hardening progress file validation to never delete non-empty files, and requiring --reset-day when using --force-reseed.",
        "details": "## Overview\n\nThis task addresses 4 specific violations identified in the code review (review1.txt Section 1.1-1.4) that can cause accounts to exceed daily posting limits or lose posting history.\n\n## Implementation Details\n\n### 1.1 Modify `claim_next_job()` to check success+claimed counts at claim time\n\n**File:** `progress_tracker.py`\n**Function:** `claim_next_job()` (lines 374-451)\n\nThe current implementation checks success counts and accounts_in_use separately. Modify to combine both for total count comparison:\n\n```python\ndef claim_next_job(self, worker_id: int, max_posts_per_account_per_day: int = 1) -> Optional[Dict[str, Any]]:\n    def _claim_operation(jobs):\n        # Build combined counts: success + currently claimed\n        total_assigned_by_account = {}\n        accounts_in_use = set()\n        \n        for job in jobs:\n            account = job.get('account', '')\n            if not account:\n                continue\n            status = job.get('status', '')\n            \n            if status == self.STATUS_SUCCESS:\n                total_assigned_by_account[account] = total_assigned_by_account.get(account, 0) + 1\n            elif status == self.STATUS_CLAIMED:\n                total_assigned_by_account[account] = total_assigned_by_account.get(account, 0) + 1\n                accounts_in_use.add(account)\n        \n        # Find accounts at daily limit (success + claimed >= max)\n        accounts_at_limit = {\n            acc for acc, cnt in total_assigned_by_account.items() \n            if cnt >= max_posts_per_account_per_day\n        }\n        \n        # Find pending job where:\n        # 1. Has assigned account\n        # 2. Account not currently claimed by another worker  \n        # 3. Account total (success+claimed) < daily limit\n        for job in jobs:\n            if job.get('status') != self.STATUS_PENDING:\n                continue\n            account = job.get('account', '')\n            if not account:\n                continue\n            if account in accounts_in_use:\n                logger.debug(f\"Skipping job {job['job_id']} - account {account} in use\")\n                continue\n            if account in accounts_at_limit:\n                logger.warning(f\"Skipping job {job['job_id']} - account {account} at daily limit\")\n                continue\n            \n            # Claim the job\n            job['status'] = self.STATUS_CLAIMED\n            job['worker_id'] = str(worker_id)\n            job['claimed_at'] = datetime.now().isoformat()\n            logger.info(f\"Worker {worker_id} claimed job {job['job_id']} (account: {account})\")\n            return jobs, dict(job)\n        \n        return jobs, None\n    \n    return self._locked_operation(_claim_operation)\n```\n\n### 1.2 In `seed_from_scheduler_state()`, consider existing pending/claimed jobs\n\n**File:** `progress_tracker.py`\n**Function:** `seed_from_scheduler_state()` (lines 212-330)\n\nCurrently only checks success counts. Modify to also exclude accounts with existing pending/claimed jobs for the day:\n\n```python\ndef seed_from_scheduler_state(self, state_file: str, ...):\n    # ... existing code to load state ...\n    \n    # CRITICAL: Build success_count AND assigned_accounts from existing progress\n    success_count_by_account = self._load_success_counts()\n    existing_job_ids = set()\n    existing_jobs = []\n    assigned_accounts_today = set()  # NEW: Track accounts with any job status\n    \n    if os.path.exists(self.progress_file):\n        existing_jobs = self._read_all_jobs()\n        for job in existing_jobs:\n            existing_job_ids.add(job.get('job_id', ''))\n            # NEW: Track accounts that already have ANY job (pending/claimed/success)\n            acc = job.get('account', '')\n            status = job.get('status', '')\n            if acc and status in (self.STATUS_PENDING, self.STATUS_CLAIMED, self.STATUS_SUCCESS):\n                assigned_accounts_today.add(acc)\n    \n    # Filter accounts - exclude those at success limit OR already assigned\n    available_accounts = [\n        acc for acc in accounts\n        if (success_count_by_account.get(acc, 0) < max_posts_per_account_per_day \n            and acc not in assigned_accounts_today)  # NEW condition\n    ]\n    \n    logger.info(f\"Available accounts: {len(available_accounts)} \"\n                f\"(excluded {len(assigned_accounts_today)} with existing jobs)\")\n    \n    # ... rest of seeding logic ...\n```\n\n### 1.3 Tighten `validate_progress_file()` to NEVER delete non-empty files\n\n**File:** `parallel_orchestrator.py`\n**Function:** `validate_progress_file()` (lines 491-522)\n\nReplace the current aggressive deletion behavior with error logging and abort:\n\n```python\ndef validate_progress_file(progress_file: str) -> bool:\n    \"\"\"\n    Check if progress file is valid.\n    \n    CRITICAL: This function NEVER deletes files. It only validates and reports.\n    If the file is empty or corrupt, it logs an error and returns False.\n    The operator must manually resolve using --reset-day.\n    \n    Returns:\n        True if file is valid or doesn't exist\n        False if file exists but is empty/corrupt (requires manual intervention)\n    \"\"\"\n    if not os.path.exists(progress_file):\n        return True\n    \n    try:\n        import csv\n        with open(progress_file, 'r', encoding='utf-8') as f:\n            reader = csv.DictReader(f)\n            rows = list(reader)\n            \n            if len(rows) == 0:\n                # CHANGED: Log error and return False instead of deleting\n                logger.error(\n                    f\"VALIDATION FAILED: Progress file {progress_file} is empty (header only).\\n\"\n                    f\"  This may indicate a crash during write.\\n\"\n                    f\"  ACTION REQUIRED: Run with --reset-day to archive and start fresh,\\n\"\n                    f\"  or manually inspect the file before proceeding.\"\n                )\n                return False\n        return True\n        \n    except Exception as e:\n        # CHANGED: Log error and return False instead of deleting\n        logger.error(\n            f\"VALIDATION FAILED: Progress file {progress_file} appears corrupt: {e}\\n\"\n            f\"  ACTION REQUIRED: Run with --reset-day to archive and start fresh,\\n\"\n            f\"  or manually inspect/repair the file.\"\n        )\n        return False\n```\n\nAlso update `full_cleanup()` (line 557) to check the return value:\n\n```python\n# In full_cleanup():\n# 4. Validate progress file - but DO NOT delete it\nif not validate_progress_file(config.progress_file):\n    logger.warning(\"Progress file validation failed - manual intervention may be required\")\n```\n\n### 1.4 Make `--force-reseed` require `--reset-day`\n\n**File:** `parallel_orchestrator.py`\n**Functions:** `main()` (lines 921-1016) and `run_parallel_posting()` (lines 820-918)\n\nAdd validation in `main()` before executing:\n\n```python\ndef main():\n    # ... argparse setup ...\n    args = parser.parse_args()\n    \n    # NEW: Validate --force-reseed requires --reset-day\n    if args.force_reseed and not args.reset_day:\n        logger.error(\"=\"*60)\n        logger.error(\"SAFETY CHECK FAILED: --force-reseed requires --reset-day\")\n        logger.error(\"=\"*60)\n        logger.error(\"\")\n        logger.error(\"Using --force-reseed without --reset-day would wipe posting history\")\n        logger.error(\"for the current day, allowing duplicate posts to accounts.\")\n        logger.error(\"\")\n        logger.error(\"If you intend to start a new day, run:\")\n        logger.error(\"  python parallel_orchestrator.py --reset-day --force-reseed --run\")\n        logger.error(\"\")\n        logger.error(\"If you need to reseed mid-day (DANGEROUS), manually archive the\")\n        logger.error(\"progress file first, then run with both flags.\")\n        logger.error(\"=\"*60)\n        sys.exit(1)\n    \n    # ... rest of main() ...\n```\n\nAlso update `run_parallel_posting()` to validate similarly when called programmatically.\n\n## Files to Modify\n\n1. **`progress_tracker.py`**:\n   - `claim_next_job()` (lines 374-451): Add success+claimed counting\n   - `seed_from_scheduler_state()` (lines 212-330): Consider pending/claimed jobs in seeding\n\n2. **`parallel_orchestrator.py`**:\n   - `validate_progress_file()` (lines 491-522): Remove deletion, only log errors\n   - `full_cleanup()` (line 557): Handle validation failure gracefully\n   - `main()` (lines 921-1016): Add --force-reseed + --reset-day requirement\n   - `run_parallel_posting()` (lines 820-918): Validate force_reseed parameter\n\n## Constants/Config Changes\n\nNo new configuration needed - these changes enforce existing `max_posts_per_account_per_day` more strictly.",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Tests for `claim_next_job()` Daily Limit Enforcement\n\n**Test: Claimed jobs count toward daily limit**\n```python\n# Setup: Create progress CSV with account_a having 1 success\n# Add pending job for account_a\n# Call claim_next_job with max_posts_per_account_per_day=1\n# Expected: Job should NOT be claimed (account at limit)\n\n# Verify with max_posts_per_account_per_day=2\n# Expected: Job SHOULD be claimed (1 success < 2 limit)\n```\n\n**Test: Account with claimed job cannot get second claim**\n```python\n# Setup: Create progress CSV with account_a having status=claimed\n# Add another pending job for account_a\n# Call claim_next_job\n# Expected: Second job NOT claimed (account already has 1 claimed)\n```\n\n### 2. Unit Tests for `seed_from_scheduler_state()` Reuse Prevention\n\n**Test: Existing pending jobs block reseeding**\n```python\n# Setup: Create progress CSV with pending job for account_a\n# Run seed_from_scheduler_state with jobs for account_a\n# Expected: No new jobs added for account_a (already has pending)\n```\n\n**Test: Only success jobs were previously counted**\n```python\n# Regression test: Verify failed jobs don't block seeding\n# Setup: Progress CSV with failed job for account_a\n# Expected: New job CAN be seeded for account_a\n```\n\n### 3. Integration Test for `validate_progress_file()` Safety\n\n**Test: Empty file is NOT deleted**\n```bash\n# Create empty progress file (header only)\necho \"job_id,account,video_path,caption,status,worker_id,claimed_at,completed_at,error\" > parallel_progress.csv\n\n# Run orchestrator\npython parallel_orchestrator.py --status\n\n# Expected: Error logged, file still exists, orchestrator does NOT proceed\ntest -f parallel_progress.csv && echo \"PASS: File preserved\"\n```\n\n**Test: Corrupt file is NOT deleted**\n```bash\n# Create corrupt progress file\necho \"garbage data\" > parallel_progress.csv\n\n# Run validation\npython -c \"from parallel_orchestrator import validate_progress_file; print(validate_progress_file('parallel_progress.csv'))\"\n\n# Expected: Returns False, file still exists\n```\n\n### 4. CLI Validation Test for --force-reseed Safety\n\n**Test: --force-reseed alone is rejected**\n```bash\npython parallel_orchestrator.py --force-reseed --run 2>&1 | grep -q \"requires --reset-day\"\n# Expected: Exit code 1, error message shown\n```\n\n**Test: --force-reseed with --reset-day is accepted**\n```bash\n# Create dummy progress file\ntouch parallel_progress.csv\n\n# Run with both flags (won't actually post without accounts)\npython parallel_orchestrator.py --force-reseed --reset-day --status\n# Expected: No error about --force-reseed\n```\n\n### 5. End-to-End Scenario Tests\n\n**Scenario A: Mid-day reseed attempt blocked**\n1. Run orchestrator, let some jobs complete\n2. Attempt `--force-reseed --run` without `--reset-day`\n3. Verify: Rejected with clear error message\n4. Verify: Progress file unchanged, history preserved\n\n**Scenario B: Same account cannot get 2 posts in one day**\n1. Seed with account_a having 1 pending job\n2. Worker claims and completes job (status=success)\n3. Run `--force-reseed --reset-day` to start new batch\n4. Seed new jobs\n5. Verify: account_a gets NO new job (already at limit=1 success)\n\n**Scenario C: Crash recovery preserves limits**\n1. Seed jobs, worker claims job for account_a\n2. Simulate crash (kill worker mid-job, claim status remains)\n3. Restart orchestrator\n4. Verify: account_a job NOT re-claimed until stale claim released\n5. Verify: After release, account_a can be claimed again (was only claimed, not success)\n\n### 6. Logging Verification\n\nFor each test, verify appropriate log messages:\n- `claim_next_job`: \"Skipping job X - account Y at daily limit of N\"\n- `seed_from_scheduler_state`: \"excluded N with existing jobs\"\n- `validate_progress_file`: \"VALIDATION FAILED\" with action instructions\n- `main()`: \"SAFETY CHECK FAILED: --force-reseed requires --reset-day\"",
        "status": "done",
        "dependencies": [
          "19",
          "20"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-13T05:56:33.379Z"
      },
      {
        "id": "23",
        "title": "Add ADB/Appium Lifecycle State Machine with Device Readiness Checks and Recovery",
        "description": "Implement robust ADB device lifecycle management by adding a wait_for_adb() helper that polls until device is present, an ensure_device_alive() function for mid-run ADB loss detection with recovery, and a formal state machine in parallel_worker.py governing phone/Appium transitions.",
        "details": "## Overview\n\nThis task addresses Section 2.1-2.3 from reviews/review1.txt, implementing robust ADB/Appium lifecycle management to handle device readiness and mid-run failures. The current implementation in `parallel_worker.py` lacks explicit ADB readiness gates and recovery mechanisms for device loss during job execution.\n\n## Current State Analysis\n\n- `parallel_worker.py` (lines 268-341): Main job loop relies on `appium_manager.ensure_healthy()` for Appium checks but has no explicit ADB device readiness verification\n- `post_reel_smart.py` has `verify_adb_connection()` (line 819) and `reconnect_adb()` (line 838) but these are not integrated into the parallel worker flow\n- `parallel_config.py` already exposes `adb_path` (line 81): `r\"C:\\Users\\asus\\Downloads\\android-sdk\\platform-tools\\adb.exe\"`\n- `appium_server_manager.py` manages Appium lifecycle but is unaware of underlying ADB device state\n\n## Implementation Details\n\n### 2.1 Add `wait_for_adb(device_id, timeout)` helper\n\n**File:** Create new `adb_utils.py` or add to `parallel_worker.py` (recommend separate module for reusability)\n\n```python\nimport subprocess\nimport time\nimport logging\nfrom typing import Optional\n\nlogger = logging.getLogger(__name__)\n\ndef wait_for_adb(\n    device_id: str,\n    adb_path: str,\n    timeout: int = 90,\n    poll_interval: float = 2.0\n) -> bool:\n    \"\"\"\n    Poll adb devices until the specified device is present and ready.\n    \n    Args:\n        device_id: The device UDID/serial (e.g., \"192.168.1.100:5555\")\n        adb_path: Full path to adb executable\n        timeout: Maximum seconds to wait (default 90)\n        poll_interval: Seconds between polls (default 2)\n        \n    Returns:\n        True if device became ready, False if timeout\n    \"\"\"\n    deadline = time.time() + timeout\n    attempts = 0\n    \n    while time.time() < deadline:\n        attempts += 1\n        try:\n            result = subprocess.run(\n                [adb_path, \"devices\"],\n                capture_output=True,\n                encoding='utf-8',\n                timeout=10\n            )\n            \n            for line in result.stdout.splitlines():\n                # Line format: \"192.168.1.100:5555\\tdevice\"\n                if device_id in line and '\\tdevice' in line:\n                    logger.info(f\"ADB device {device_id} ready after {attempts} attempts\")\n                    return True\n                    \n            # Device exists but wrong status (offline, unauthorized)?\n            for line in result.stdout.splitlines():\n                if device_id in line:\n                    status = line.split('\\t')[-1] if '\\t' in line else 'unknown'\n                    logger.debug(f\"Device {device_id} found but status is '{status}', waiting...\")\n                    break\n                    \n        except subprocess.TimeoutExpired:\n            logger.warning(f\"ADB command timed out on attempt {attempts}\")\n        except Exception as e:\n            logger.warning(f\"ADB check error on attempt {attempts}: {e}\")\n            \n        time.sleep(poll_interval)\n    \n    logger.error(f\"Device {device_id} did not become ready within {timeout}s ({attempts} attempts)\")\n    return False\n```\n\n### 2.2 Add `ensure_device_alive()` for mid-run ADB loss detection\n\n**File:** Add to `adb_utils.py` or `parallel_worker.py`\n\n```python\ndef ensure_device_alive(device_id: str, adb_path: str, timeout: float = 5.0) -> bool:\n    \"\"\"\n    Quick check if device is still connected and responsive.\n    \n    Unlike wait_for_adb(), this is a single-shot check intended for\n    periodic verification during job execution.\n    \n    Args:\n        device_id: The device UDID/serial\n        adb_path: Full path to adb executable\n        timeout: Command timeout in seconds\n        \n    Returns:\n        True if device is alive, False otherwise\n    \"\"\"\n    try:\n        result = subprocess.run(\n            [adb_path, \"devices\"],\n            capture_output=True,\n            encoding='utf-8',\n            timeout=timeout\n        )\n        \n        for line in result.stdout.splitlines():\n            if device_id in line and '\\tdevice' in line:\n                return True\n                \n        return False\n        \n    except Exception as e:\n        logger.debug(f\"ensure_device_alive failed: {e}\")\n        return False\n\n\ndef recover_device(\n    device_id: str,\n    phone_name: str,\n    adb_path: str,\n    worker_config: 'WorkerConfig',\n    config: 'ParallelConfig',\n    appium_manager: 'AppiumServerManager',\n    logger: logging.Logger\n) -> bool:\n    \"\"\"\n    Full recovery sequence when device is lost mid-run.\n    \n    Sequence:\n    1. Stop Appium server\n    2. Stop Geelark phone\n    3. Wait for cleanup\n    4. Restart Geelark phone\n    5. Wait for ADB readiness\n    6. Restart Appium server\n    \n    Args:\n        device_id: The device UDID/serial\n        phone_name: Geelark phone serial name\n        adb_path: Full path to adb\n        worker_config: Worker's configuration\n        config: Parallel configuration\n        appium_manager: The worker's AppiumServerManager\n        logger: Worker logger\n        \n    Returns:\n        True if recovery successful, False otherwise\n    \"\"\"\n    from geelark_client import GeelarkClient\n    \n    logger.warning(f\"[RECOVERY] Device {device_id} lost, initiating recovery sequence...\")\n    \n    # 1. Stop Appium\n    logger.info(\"[RECOVERY] Step 1/6: Stopping Appium server...\")\n    try:\n        appium_manager.stop()\n    except Exception as e:\n        logger.warning(f\"[RECOVERY] Appium stop error (non-fatal): {e}\")\n    \n    # 2. Disconnect ADB\n    logger.info(\"[RECOVERY] Step 2/6: Disconnecting ADB...\")\n    try:\n        subprocess.run([adb_path, \"disconnect\", device_id], capture_output=True, timeout=10)\n    except Exception as e:\n        logger.debug(f\"[RECOVERY] ADB disconnect error: {e}\")\n    \n    time.sleep(2)\n    \n    # 3. Stop Geelark phone\n    logger.info(\"[RECOVERY] Step 3/6: Stopping Geelark phone...\")\n    try:\n        client = GeelarkClient()\n        phones = client.list_phones(page_size=100)\n        for phone in phones.get('items', []):\n            if phone.get('serialName') == phone_name:\n                if phone.get('status') == 1:  # Running\n                    client.stop_phone(phone['id'])\n                    logger.info(f\"[RECOVERY] Stopped phone {phone_name}\")\n                break\n    except Exception as e:\n        logger.warning(f\"[RECOVERY] Phone stop error: {e}\")\n    \n    time.sleep(5)  # Let phone fully stop\n    \n    # 4. Restart phone (handled by the caller via SmartInstagramPoster.connect())\n    # The state machine will transition back to PHONE_STARTING\n    logger.info(\"[RECOVERY] Step 4/6: Phone stop complete, ready for restart\")\n    \n    # 5. Wait for ADB (will be done in state machine's ADB_PENDING state)\n    logger.info(\"[RECOVERY] Step 5/6: Recovery cleanup complete\")\n    \n    # 6. Appium restart (will be done in state machine's ADB_READY state)\n    logger.info(\"[RECOVERY] Step 6/6: Ready for state machine restart sequence\")\n    \n    return True\n```\n\n### 2.3 Implement State Machine in `parallel_worker.py`\n\n**File:** `parallel_worker.py` - Major refactor of `run_worker()` function\n\n```python\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple\n\nclass WorkerState(Enum):\n    \"\"\"State machine states for worker phone/Appium lifecycle.\"\"\"\n    IDLE = auto()              # Initial state, no phone assigned\n    PHONE_STARTING = auto()    # Geelark phone being started\n    ADB_PENDING = auto()       # Waiting for ADB device to appear\n    ADB_READY = auto()         # ADB device connected, starting Appium\n    APPIUM_READY = auto()      # Appium healthy, ready to process jobs\n    JOB_RUNNING = auto()       # Currently executing a posting job\n    ERROR_RECOVERY = auto()    # Recovery in progress after failure\n    SHUTDOWN = auto()          # Clean shutdown requested\n\n\n@dataclass\nclass WorkerContext:\n    \"\"\"Context carried through state transitions.\"\"\"\n    device_id: Optional[str] = None\n    phone_name: Optional[str] = None\n    phone_id: Optional[str] = None\n    recovery_attempts: int = 0\n    max_recovery_attempts: int = 3\n    last_error: Optional[str] = None\n\n\ndef run_worker_state_machine(\n    worker_id: int,\n    config: ParallelConfig,\n    progress_file: str,\n    delay_between_jobs: int,\n    logger: logging.Logger\n) -> dict:\n    \"\"\"\n    State machine-based worker loop.\n    \n    State transitions:\n    IDLE -> PHONE_STARTING: When claiming a job\n    PHONE_STARTING -> ADB_PENDING: After Geelark start_phone() called\n    ADB_PENDING -> ADB_READY: After wait_for_adb() returns True\n    ADB_PENDING -> ERROR_RECOVERY: After wait_for_adb() timeout\n    ADB_READY -> APPIUM_READY: After Appium starts successfully\n    ADB_READY -> ERROR_RECOVERY: Appium start failure\n    APPIUM_READY -> JOB_RUNNING: When executing a job\n    JOB_RUNNING -> APPIUM_READY: Job complete (success or fail)\n    JOB_RUNNING -> ERROR_RECOVERY: Device lost mid-job\n    ERROR_RECOVERY -> PHONE_STARTING: After cleanup, retry\n    ERROR_RECOVERY -> SHUTDOWN: Max retries exceeded\n    Any -> SHUTDOWN: Shutdown signal received\n    \"\"\"\n    global _shutdown_requested\n    \n    worker_config = config.get_worker(worker_id)\n    tracker = ProgressTracker(progress_file)\n    appium_manager = AppiumServerManager(worker_config, config)\n    \n    state = WorkerState.IDLE\n    ctx = WorkerContext()\n    stats = {\n        'worker_id': worker_id,\n        'jobs_completed': 0,\n        'jobs_failed': 0,\n        'recovery_cycles': 0,\n        'start_time': datetime.now().isoformat(),\n        'end_time': None,\n        'exit_reason': None\n    }\n    \n    current_job = None\n    \n    while state != WorkerState.SHUTDOWN:\n        if _shutdown_requested:\n            state = WorkerState.SHUTDOWN\n            continue\n            \n        logger.debug(f\"State: {state.name}, Context: recovery_attempts={ctx.recovery_attempts}\")\n        \n        # --- IDLE: Wait for a job to claim ---\n        if state == WorkerState.IDLE:\n            progress_stats = tracker.get_stats()\n            if progress_stats['pending'] == 0 and progress_stats['claimed'] == 0:\n                logger.info(\"No more jobs to process\")\n                stats['exit_reason'] = 'all_jobs_complete'\n                state = WorkerState.SHUTDOWN\n                continue\n            \n            current_job = tracker.claim_next_job(\n                worker_id, \n                max_posts_per_account_per_day=config.max_posts_per_account_per_day\n            )\n            \n            if current_job is None:\n                if progress_stats['claimed'] > 0:\n                    time.sleep(5)  # Other workers processing\n                    continue\n                else:\n                    stats['exit_reason'] = 'all_jobs_complete'\n                    state = WorkerState.SHUTDOWN\n                    continue\n            \n            ctx.phone_name = current_job['account']\n            ctx.recovery_attempts = 0\n            state = WorkerState.PHONE_STARTING\n            \n        # --- PHONE_STARTING: Start Geelark phone ---\n        elif state == WorkerState.PHONE_STARTING:\n            logger.info(f\"Starting phone for account: {ctx.phone_name}\")\n            # Phone startup is handled by SmartInstagramPoster.connect()\n            # which calls Geelark API, enables ADB, and gets device_id\n            # For now, we transition to ADB_PENDING and let execute_posting_job handle it\n            # In future: explicit Geelark phone start here\n            state = WorkerState.ADB_PENDING\n            \n        # --- ADB_PENDING: Wait for device to appear in adb devices ---\n        elif state == WorkerState.ADB_PENDING:\n            # Note: device_id is obtained during SmartInstagramPoster.connect()\n            # For explicit ADB waiting, we'd need device_id earlier\n            # This state confirms the pattern; actual waiting happens in execute_posting_job\n            logger.info(f\"ADB pending for {ctx.phone_name}, proceeding to start Appium...\")\n            state = WorkerState.ADB_READY\n            \n        # --- ADB_READY: Start Appium server ---\n        elif state == WorkerState.ADB_READY:\n            try:\n                appium_manager.ensure_healthy()\n                logger.info(f\"Appium ready at {worker_config.appium_url}\")\n                state = WorkerState.APPIUM_READY\n            except AppiumServerError as e:\n                ctx.last_error = str(e)\n                logger.error(f\"Appium start failed: {e}\")\n                state = WorkerState.ERROR_RECOVERY\n                \n        # --- APPIUM_READY: Ready to execute jobs ---\n        elif state == WorkerState.APPIUM_READY:\n            if current_job is None:\n                state = WorkerState.IDLE\n                continue\n            state = WorkerState.JOB_RUNNING\n            \n        # --- JOB_RUNNING: Execute the posting job ---\n        elif state == WorkerState.JOB_RUNNING:\n            job_id = current_job['job_id']\n            \n            try:\n                success, error = execute_posting_job(\n                    current_job, worker_config, config, logger,\n                    tracker=tracker, worker_id=worker_id\n                )\n                \n                if success:\n                    tracker.update_job_status(job_id, 'success', worker_id)\n                    stats['jobs_completed'] += 1\n                else:\n                    tracker.update_job_status(job_id, 'failed', worker_id, error=error)\n                    stats['jobs_failed'] += 1\n                    \n                    # Check if error indicates device loss\n                    device_loss_errors = [\n                        'device offline', 'not found', 'connection reset',\n                        'adb', 'device not ready', 'UiAutomator'\n                    ]\n                    if any(e in error.lower() for e in device_loss_errors):\n                        state = WorkerState.ERROR_RECOVERY\n                        continue\n                \n                current_job = None\n                ctx.recovery_attempts = 0  # Reset on successful cycle\n                \n                # Delay between jobs\n                if delay_between_jobs > 0:\n                    logger.info(f\"Waiting {delay_between_jobs}s before next job...\")\n                    time.sleep(delay_between_jobs)\n                    \n                state = WorkerState.IDLE\n                \n            except Exception as e:\n                error_msg = f\"{type(e).__name__}: {str(e)}\"\n                logger.error(f\"Job {job_id} exception: {error_msg}\")\n                tracker.update_job_status(job_id, 'failed', worker_id, error=error_msg)\n                stats['jobs_failed'] += 1\n                ctx.last_error = error_msg\n                state = WorkerState.ERROR_RECOVERY\n                \n        # --- ERROR_RECOVERY: Clean up and retry ---\n        elif state == WorkerState.ERROR_RECOVERY:\n            ctx.recovery_attempts += 1\n            stats['recovery_cycles'] += 1\n            \n            logger.warning(\n                f\"[RECOVERY] Attempt {ctx.recovery_attempts}/{ctx.max_recovery_attempts}, \"\n                f\"last error: {ctx.last_error}\"\n            )\n            \n            if ctx.recovery_attempts > ctx.max_recovery_attempts:\n                logger.error(\"[RECOVERY] Max attempts exceeded, shutting down worker\")\n                stats['exit_reason'] = 'max_recovery_attempts'\n                state = WorkerState.SHUTDOWN\n                continue\n            \n            # Full cleanup\n            try:\n                appium_manager.stop()\n            except:\n                pass\n                \n            if ctx.phone_name:\n                stop_phone_by_name(ctx.phone_name, logger)\n            \n            # Backoff before retry\n            backoff = min(30, 5 * ctx.recovery_attempts)\n            logger.info(f\"[RECOVERY] Backing off {backoff}s before retry...\")\n            time.sleep(backoff)\n            \n            # Return to PHONE_STARTING to try again\n            state = WorkerState.PHONE_STARTING\n            \n    # --- SHUTDOWN: Clean exit ---\n    logger.info(\"Worker shutting down...\")\n    \n    try:\n        appium_manager.stop()\n    except:\n        pass\n        \n    if ctx.phone_name:\n        stop_phone_by_name(ctx.phone_name, logger)\n    \n    stats['end_time'] = datetime.now().isoformat()\n    if stats['exit_reason'] is None:\n        stats['exit_reason'] = 'shutdown_requested'\n    \n    return stats\n```\n\n## File Changes Summary\n\n1. **New file: `adb_utils.py`** (recommended)\n   - `wait_for_adb(device_id, adb_path, timeout)` - Polls until device present\n   - `ensure_device_alive(device_id, adb_path)` - Quick liveness check\n   - `recover_device(...)` - Full recovery sequence\n\n2. **Modified: `parallel_worker.py`**\n   - Add `WorkerState` enum with states: IDLE, PHONE_STARTING, ADB_PENDING, ADB_READY, APPIUM_READY, JOB_RUNNING, ERROR_RECOVERY, SHUTDOWN\n   - Add `WorkerContext` dataclass for state machine context\n   - Refactor `run_worker()` to use `run_worker_state_machine()`\n   - Import and use `wait_for_adb`, `ensure_device_alive` from adb_utils\n   - Add periodic `ensure_device_alive()` checks during JOB_RUNNING state\n\n3. **Optional: `parallel_config.py`**\n   - Add `adb_timeout: int = 90` for configurable ADB wait timeout\n   - Add `max_recovery_attempts: int = 3` for worker resilience config\n\n## Integration Points\n\n- `execute_posting_job()` should call `ensure_device_alive()` before Appium operations\n- State machine replaces the flat while loop in current `run_worker()`\n- Recovery state properly cleans up Appium, disconnects ADB, stops phone, then retries\n- Stats now track `recovery_cycles` for observability",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Tests for ADB Helper Functions\n\n**Test `wait_for_adb()` timeout behavior:**\n```bash\n# Simulate by running with a non-existent device\npython -c \"\nfrom adb_utils import wait_for_adb\n# Should return False after timeout\nresult = wait_for_adb('192.168.99.99:5555', \n    r'C:\\Users\\asus\\Downloads\\android-sdk\\platform-tools\\adb.exe',\n    timeout=10)\nassert result == False, 'Should timeout for non-existent device'\nprint('PASS: wait_for_adb timeout test')\n\"\n```\n\n**Test `wait_for_adb()` success case:**\n```bash\n# With a real running phone\npython -c \"\nfrom adb_utils import wait_for_adb\nfrom geelark_client import GeelarkClient\nimport time\n\nclient = GeelarkClient()\n# Find a phone and start it\nphones = client.list_phones(page_size=1)\nif phones['items']:\n    phone = phones['items'][0]\n    # Get ADB info\n    adb_info = client.enable_adb(phone['id'])\n    device_id = f\\\"{adb_info['ip']}:{adb_info['port']}\\\"\n    \n    # Test wait_for_adb\n    result = wait_for_adb(device_id, \n        r'C:\\\\Users\\\\asus\\\\Downloads\\\\android-sdk\\\\platform-tools\\\\adb.exe',\n        timeout=60)\n    print(f'wait_for_adb result: {result}')\n    assert result == True, 'Should find started device'\n    \n    client.stop_phone(phone['id'])\n    print('PASS: wait_for_adb success test')\n\"\n```\n\n**Test `ensure_device_alive()` quick check:**\n```bash\npython -c \"\nfrom adb_utils import ensure_device_alive\n# Quick check should return fast\nimport time\nstart = time.time()\nresult = ensure_device_alive('192.168.99.99:5555', \n    r'C:\\Users\\asus\\Downloads\\android-sdk\\platform-tools\\adb.exe')\nelapsed = time.time() - start\nassert elapsed < 10, f'Should be quick, took {elapsed}s'\nassert result == False, 'Non-existent device should return False'\nprint(f'PASS: ensure_device_alive quick check ({elapsed:.1f}s)')\n\"\n```\n\n### 2. State Machine Integration Tests\n\n**Test state transitions logging:**\n```bash\n# Run worker with verbose logging to verify state transitions\npython parallel_worker.py --worker-id 0 --num-workers 1 --progress-file test_progress.csv --delay 5 2>&1 | grep -E \"State:|RECOVERY|transition\"\n```\n\n**Expected state flow for successful job:**\n```\nState: IDLE\nState: PHONE_STARTING\nState: ADB_PENDING  \nState: ADB_READY\nState: APPIUM_READY\nState: JOB_RUNNING\nState: IDLE (back to claim next job)\n```\n\n**Test recovery flow by simulating ADB loss:**\n```bash\n# Start worker, then during JOB_RUNNING, manually disconnect ADB\n# Worker should transition: JOB_RUNNING -> ERROR_RECOVERY -> PHONE_STARTING -> ...\nadb disconnect <device_id>\n# Watch logs for \"[RECOVERY]\" messages\n```\n\n### 3. End-to-End Recovery Test\n\n**Simulate device loss and recovery:**\n```bash\n# 1. Seed a test job\npython -c \"\nimport csv\nwith open('test_recovery.csv', 'w', newline='') as f:\n    w = csv.writer(f)\n    w.writerow(['job_id','account','video_path','caption','status','worker_id','claimed_at','completed_at','error'])\n    w.writerow(['test1','testaccount1','chunk_01c/video.mp4','Test caption','pending','','','',''])\n\"\n\n# 2. Start worker\npython parallel_worker.py --worker-id 0 --num-workers 1 --progress-file test_recovery.csv &\n\n# 3. While job is running, kill ADB connection\nsleep 30\nadb disconnect all\n\n# 4. Verify worker enters ERROR_RECOVERY and attempts restart\n# Check logs/worker_0.log for:\n#   [RECOVERY] Device ... lost, initiating recovery sequence...\n#   [RECOVERY] Step 1/6: Stopping Appium server...\n#   ...\n#   State: PHONE_STARTING\n```\n\n### 4. Stress Test with Multiple Workers\n\n**Run 3 workers and verify independent recovery:**\n```bash\n# Seed jobs for 3 workers\npython parallel_orchestrator.py --seed-only --accounts acc1 acc2 acc3\n\n# Start orchestrator\npython parallel_orchestrator.py --run --workers 3\n\n# During execution, manually stop one phone via Geelark dashboard\n# Verify:\n# - Only affected worker enters ERROR_RECOVERY\n# - Other workers continue normally\n# - Affected worker recovers and continues\n```\n\n### 5. Max Recovery Attempts Test\n\n**Verify worker exits after max retries:**\n```bash\n# Configure impossibly short ADB timeout to force failures\n# Edit test to set ctx.max_recovery_attempts = 2\n\n# Watch for log:\n#   [RECOVERY] Max attempts exceeded, shutting down worker\n#   exit_reason: max_recovery_attempts\n```\n\n### 6. Metrics Validation\n\n**Verify stats include recovery_cycles:**\n```bash\n# After worker completes/exits, check returned stats\npython -c \"\n# Mock test\nstats = {'recovery_cycles': 2, 'jobs_completed': 5, 'jobs_failed': 1}\nassert 'recovery_cycles' in stats\nprint(f'Recovery cycles tracked: {stats[\\\"recovery_cycles\\\"]}')\n\"\n```\n\n### 7. Live Production Test (5 Workers)\n\n**Final validation per CLAUDE.md instructions:**\n```bash\n# Use live accounts and real videos\npython parallel_orchestrator.py --run --workers 5 --accounts $(head -5 accounts.txt | tr '\\n' ' ')\n\n# Monitor for:\n# - No duplicate posts to same account\n# - Clean recovery from any ADB flakiness\n# - All phones stopped after completion\n```\n\n### 8. Post-Test Phone Cleanup Verification\n\n```bash\n# CRITICAL: Verify all phones stopped\npython -c \"\nfrom geelark_client import GeelarkClient\nclient = GeelarkClient()\nrunning = []\nfor page in range(1, 20):\n    result = client.list_phones(page=page, page_size=100)\n    for phone in result['items']:\n        if phone['status'] == 1:\n            running.append(phone['serialName'])\n    if len(result['items']) < 100:\n        break\nif running:\n    print(f'WARNING: {len(running)} phones still running: {running}')\nelse:\n    print('PASS: All phones stopped')\n\"\n```",
        "status": "done",
        "dependencies": [
          "13",
          "16"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-13T05:58:09.305Z"
      },
      {
        "id": "24",
        "title": "Enforce strict worker-phone-Appium bindings with explicit phone assignment",
        "description": "Add phone_id field to WorkerConfig, implement --phones CLI argument for the orchestrator to map each worker to a specific phone, update get_config() to assign phones to workers, pass --phone-id to parallel_worker.py, and validate phone assignments are unique and exist in Geelark before starting.",
        "details": "## Overview\n\nThis task addresses Section 3.1-3.2 from reviews/review1.txt, implementing strict worker-phone-Appium bindings to prevent phone-level collisions. Currently, workers can implicitly \"grab phones\" rather than being bound to a pre-assigned phone identity, making phone-level collisions possible when multiple workers run concurrently.\n\n## Current State Analysis\n\n- `parallel_config.py` (lines 26-53): `WorkerConfig` dataclass has no `phone_id` field\n- `parallel_config.py` (lines 172-174): `get_config()` only accepts `num_workers` parameter, no phone assignment\n- `parallel_orchestrator.py` (lines 626-657): `start_worker_process()` passes `--worker-id`, `--num-workers`, `--progress-file`, `--delay` but no `--phone-id`\n- `parallel_worker.py` (lines 368-399): `main()` parser accepts `--worker-id`, `--num-workers`, `--progress-file`, `--delay` but no `--phone-id`\n- `parallel_worker.py` (lines 131-216): `execute_posting_job()` uses `account` from job dict as phone_name, not a worker-bound phone_id\n- Workers currently select phones dynamically based on the account name from jobs, not from a pre-assigned binding\n\n## Implementation Details\n\n### 3.1 Add phone_id field to WorkerConfig and orchestrator CLI\n\n**File:** `parallel_config.py`\n\n1. Extend `WorkerConfig` dataclass (around line 26):\n```python\n@dataclass\nclass WorkerConfig:\n    \"\"\"Configuration for a single worker process.\"\"\"\n    worker_id: int\n    appium_port: int\n    system_port_start: int\n    system_port_end: int\n    log_file: str\n    appium_log_file: str\n    phone_id: Optional[str] = None  # NEW: Geelark phone ID or serialName\n```\n\n2. Modify `get_config()` function (around line 172) to accept phones parameter:\n```python\ndef get_config(num_workers: int = 3, phones: Optional[List[str]] = None) -> ParallelConfig:\n    \"\"\"\n    Get a parallel configuration with the specified number of workers.\n    \n    Args:\n        num_workers: Number of parallel workers\n        phones: Optional list of phone IDs/names to assign to workers (must match num_workers)\n    \n    Returns:\n        ParallelConfig with phone assignments if provided\n    \"\"\"\n    config = ParallelConfig(num_workers=num_workers)\n    if phones:\n        if len(phones) != num_workers:\n            raise ValueError(f\"Number of phones ({len(phones)}) must match number of workers ({num_workers})\")\n        for worker, phone in zip(config.workers, phones):\n            worker.phone_id = phone\n    return config\n```\n\n**File:** `parallel_orchestrator.py`\n\n3. Add `--phones` CLI argument (around line 959):\n```python\nparser.add_argument('--phones', '-p',\n                    help='Comma-separated list of phone IDs/names (must match --workers count)')\n```\n\n4. Parse phones list in `main()` (around line 967):\n```python\n# Parse phones list if provided\nphones_list = None\nif args.phones:\n    phones_list = [p.strip() for p in args.phones.split(',') if p.strip()]\n```\n\n5. Update `get_config()` calls throughout orchestrator to pass phones:\n```python\nconfig = get_config(num_workers=args.workers, phones=phones_list)\n```\n\n6. Add phone validation function before starting workers:\n```python\ndef validate_phone_assignments(config: ParallelConfig) -> Tuple[bool, List[str]]:\n    \"\"\"\n    Validate that all phone_id values are unique and exist in Geelark.\n    \n    Returns:\n        (valid: bool, list of error messages)\n    \"\"\"\n    errors = []\n    \n    # Check for phone assignments\n    phone_ids = [w.phone_id for w in config.workers if w.phone_id]\n    if not phone_ids:\n        errors.append(\"No phones assigned to workers. Use --phones phone1,phone2,...\")\n        return False, errors\n    \n    if len(phone_ids) != config.num_workers:\n        errors.append(f\"Only {len(phone_ids)} phones assigned but {config.num_workers} workers configured\")\n        return False, errors\n    \n    # Check for duplicates\n    if len(phone_ids) != len(set(phone_ids)):\n        duplicates = [p for p in phone_ids if phone_ids.count(p) > 1]\n        errors.append(f\"Duplicate phone assignments: {set(duplicates)}\")\n        return False, errors\n    \n    # Validate phones exist in Geelark\n    try:\n        client = GeelarkClient()\n        all_phones = {}\n        for page in range(1, 20):\n            result = client.list_phones(page=page, page_size=100)\n            for phone in result.get('items', []):\n                all_phones[phone['id']] = phone['serialName']\n                all_phones[phone['serialName']] = phone['id']\n            if len(result.get('items', [])) < 100:\n                break\n        \n        for phone_id in phone_ids:\n            if phone_id not in all_phones:\n                errors.append(f\"Phone '{phone_id}' not found in Geelark\")\n    except Exception as e:\n        errors.append(f\"Failed to validate phones with Geelark: {e}\")\n    \n    return len(errors) == 0, errors\n```\n\n7. Call validation before `start_all_workers()` in `run_parallel_posting()`:\n```python\n# Validate phone assignments\nlogger.info(\"Validating phone assignments...\")\nvalid, errors = validate_phone_assignments(config)\nif not valid:\n    for err in errors:\n        logger.error(f\"  - {err}\")\n    return {'error': 'invalid_phone_assignments', 'details': errors}\nlogger.info(\"Phone assignments validated successfully\")\n```\n\n### 3.2 Pass --phone-id to parallel_worker.py and enforce exclusive use\n\n**File:** `parallel_orchestrator.py`\n\n8. Update `start_worker_process()` (around line 626) to pass phone_id:\n```python\ndef start_worker_process(worker_id: int, config: ParallelConfig) -> subprocess.Popen:\n    \"\"\"Start a single worker subprocess.\"\"\"\n    worker_config = config.get_worker(worker_id)\n    \n    cmd = [\n        sys.executable,\n        'parallel_worker.py',\n        '--worker-id', str(worker_id),\n        '--num-workers', str(config.num_workers),\n        '--progress-file', config.progress_file,\n        '--delay', str(config.delay_between_jobs),\n        '--phone-id', worker_config.phone_id,  # NEW: Pass assigned phone\n    ]\n    # ... rest of function\n```\n\n**File:** `parallel_worker.py`\n\n9. Add `--phone-id` argument to parser (around line 371):\n```python\nparser.add_argument('--phone-id', required=True,\n                    help='Geelark phone ID or serialName assigned to this worker')\n```\n\n10. Store phone_id in worker state and pass to job execution (around line 385):\n```python\n# Run worker with assigned phone\nstats = run_worker(\n    worker_id=args.worker_id,\n    config=config,\n    progress_file=args.progress_file,\n    delay_between_jobs=args.delay,\n    phone_id=args.phone_id  # NEW\n)\n```\n\n11. Update `run_worker()` signature and enforce phone binding (around line 218):\n```python\ndef run_worker(\n    worker_id: int,\n    config: ParallelConfig,\n    progress_file: str = None,\n    delay_between_jobs: int = None,\n    phone_id: str = None  # NEW: Required phone assignment\n) -> dict:\n    \"\"\"\n    Main worker loop.\n    \n    Args:\n        worker_id: This worker's ID\n        config: Parallel configuration\n        progress_file: Override progress file path\n        delay_between_jobs: Override delay between jobs\n        phone_id: Assigned Geelark phone (required - worker uses ONLY this phone)\n    \"\"\"\n    if not phone_id:\n        raise ValueError(\"phone_id is required - worker must have an assigned phone\")\n```\n\n12. Update `execute_posting_job()` to use worker's assigned phone instead of job account (around line 131):\n```python\ndef execute_posting_job(\n    job: dict,\n    worker_config: WorkerConfig,\n    config: ParallelConfig,\n    logger: logging.Logger,\n    tracker=None,\n    worker_id: int = None,\n    phone_id: str = None  # NEW: Worker's assigned phone\n) -> tuple:\n    \"\"\"\n    Execute a single posting job using the worker's assigned phone.\n    \n    IMPORTANT: The worker uses its assigned phone_id, NOT the account from the job.\n    The 'account' in the job refers to the Instagram account to post to,\n    while phone_id is the Geelark cloud phone this worker exclusively controls.\n    \"\"\"\n```\n\n13. Pass phone_id when calling execute_posting_job in run_worker():\n```python\nsuccess, error = execute_posting_job(\n    job, worker_config, config, logger,\n    tracker=tracker, worker_id=worker_id,\n    phone_id=phone_id  # Pass worker's assigned phone\n)\n```\n\n## File-level Change Summary\n\n| File | Changes |\n|------|---------|\n| `parallel_config.py` | Add `phone_id: Optional[str] = None` to WorkerConfig, update `get_config()` to accept phones list |\n| `parallel_orchestrator.py` | Add `--phones` CLI arg, add `validate_phone_assignments()`, update `start_worker_process()` to pass `--phone-id`, call validation before starting |\n| `parallel_worker.py` | Add `--phone-id` arg (required), update `run_worker()` and `execute_posting_job()` to use assigned phone exclusively |\n\n## Key Invariants Enforced\n\n1. **One worker ↔ one phone**: Each worker is bound to exactly one Geelark phone at startup\n2. **No dynamic phone selection**: Workers do not scan for \"any available phone\"\n3. **Uniqueness**: No two workers can be assigned the same phone\n4. **Existence validation**: All assigned phones must exist in Geelark before orchestrator starts\n5. **Explicit binding**: Phone assignment is explicit via CLI, not implicit",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Tests for WorkerConfig phone_id Field\n\n**Test phone_id field exists and is optional:**\n```python\ndef test_worker_config_phone_id_optional():\n    config = WorkerConfig(\n        worker_id=0, appium_port=4723,\n        system_port_start=8200, system_port_end=8209,\n        log_file=\"test.log\", appium_log_file=\"appium.log\"\n    )\n    assert config.phone_id is None\n\ndef test_worker_config_phone_id_set():\n    config = WorkerConfig(\n        worker_id=0, appium_port=4723,\n        system_port_start=8200, system_port_end=8209,\n        log_file=\"test.log\", appium_log_file=\"appium.log\",\n        phone_id=\"test_phone_123\"\n    )\n    assert config.phone_id == \"test_phone_123\"\n```\n\n### 2. Unit Tests for get_config() with phones\n\n**Test get_config with matching phones:**\n```bash\npython -c \"\nfrom parallel_config import get_config\nconfig = get_config(num_workers=3, phones=['phone1', 'phone2', 'phone3'])\nassert len(config.workers) == 3\nassert config.workers[0].phone_id == 'phone1'\nassert config.workers[1].phone_id == 'phone2'\nassert config.workers[2].phone_id == 'phone3'\nprint('PASS: get_config with phones')\n\"\n```\n\n**Test get_config with mismatched phones count (should raise):**\n```bash\npython -c \"\nfrom parallel_config import get_config\ntry:\n    config = get_config(num_workers=3, phones=['phone1', 'phone2'])\n    print('FAIL: Should have raised ValueError')\nexcept ValueError as e:\n    print(f'PASS: Raised ValueError: {e}')\n\"\n```\n\n### 3. Integration Tests for Phone Validation\n\n**Test validate_phone_assignments with duplicates:**\n```bash\npython -c \"\nfrom parallel_orchestrator import validate_phone_assignments\nfrom parallel_config import get_config\n\n# Create config with duplicate phones\nconfig = get_config(num_workers=2)\nconfig.workers[0].phone_id = 'same_phone'\nconfig.workers[1].phone_id = 'same_phone'\n\nvalid, errors = validate_phone_assignments(config)\nassert not valid, 'Should be invalid'\nassert any('Duplicate' in e for e in errors)\nprint(f'PASS: Duplicate detection - {errors}')\n\"\n```\n\n**Test validate_phone_assignments with non-existent phone:**\n```bash\npython -c \"\nfrom parallel_orchestrator import validate_phone_assignments\nfrom parallel_config import get_config\n\nconfig = get_config(num_workers=1, phones=['nonexistent_phone_xyz123'])\nvalid, errors = validate_phone_assignments(config)\nassert not valid, 'Should be invalid'\nassert any('not found' in e for e in errors)\nprint(f'PASS: Non-existent phone detection - {errors}')\n\"\n```\n\n### 4. CLI Argument Tests\n\n**Test --phones argument parsing:**\n```bash\n# Test with matching phones\npython parallel_orchestrator.py --workers 2 --phones phone1,phone2 --status\n\n# Test with mismatched count (should error)\npython parallel_orchestrator.py --workers 3 --phones phone1,phone2 --run 2>&1 | grep -i \"must match\"\n```\n\n### 5. Worker phone_id Enforcement Tests\n\n**Test worker refuses to start without phone_id:**\n```bash\npython -c \"\nfrom parallel_worker import run_worker\nfrom parallel_config import get_config\n\nconfig = get_config(num_workers=1)\ntry:\n    run_worker(worker_id=0, config=config, phone_id=None)\n    print('FAIL: Should have raised ValueError')\nexcept ValueError as e:\n    print(f'PASS: Worker requires phone_id - {e}')\n\"\n```\n\n### 6. End-to-End Test with Real Phones\n\n**Prerequisites:** Have at least 2 Geelark phones available (e.g., from accounts.txt)\n\n```bash\n# Step 1: Get two available phone names\npython -c \"\nfrom geelark_client import GeelarkClient\nclient = GeelarkClient()\nresult = client.list_phones(page_size=5)\nphones = [p['serialName'] for p in result['items'][:2]]\nprint(f'Available phones: {phones}')\nprint(f'Use: --phones {phones[0]},{phones[1]}')\n\"\n\n# Step 2: Test orchestrator with phone binding (dry run)\npython parallel_orchestrator.py --workers 2 --phones <phone1>,<phone2> --status\n\n# Step 3: Full test with 2 workers on 2 phones\npython parallel_orchestrator.py --workers 2 --phones <phone1>,<phone2> --seed-only --accounts acc1,acc2\n\n# Verify logs show phone binding\ngrep \"phone_id\" logs/worker_0.log\ngrep \"phone_id\" logs/worker_1.log\n```\n\n### 7. Collision Prevention Test\n\n**Test that workers use only assigned phones:**\n```bash\n# Start orchestrator with explicit phone bindings\n# Monitor that worker 0 only uses phone1, worker 1 only uses phone2\n# Check logs for any attempts to use non-assigned phones\npython parallel_orchestrator.py --workers 2 --phones phone1,phone2 --run &\n\n# In another terminal, monitor:\ntail -f logs/worker_0.log | grep -i phone\ntail -f logs/worker_1.log | grep -i phone\n\n# Verify no cross-phone operations\n```\n\n### 8. Regression Tests\n\n**Ensure backwards compatibility when --phones not provided:**\n```bash\n# Without --phones should show error or warning, not crash\npython parallel_orchestrator.py --workers 2 --status  # Should work (no phone validation for status)\npython parallel_orchestrator.py --workers 2 --run  # Should error gracefully asking for --phones\n```",
        "status": "deferred",
        "dependencies": [
          "19"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-13T08:59:43.228Z"
      },
      {
        "id": "25",
        "title": "Create centralized config.py with all paths and settings",
        "description": "Consolidate ADB_PATH, ANDROID_HOME, and other configuration values into a single config.py module that all other modules import from, eliminating scattered hardcoded paths and providing a single source of truth.",
        "details": "## Current State Analysis\n\nA `config.py` already exists (lines 1-186) with a well-structured `Config` class containing:\n- `ANDROID_SDK_PATH`, `ADB_PATH`, `PROJECT_ROOT`\n- Appium settings (`APPIUM_BASE_PORT`, `DEFAULT_APPIUM_URL`)\n- Parallel execution settings (`DEFAULT_NUM_WORKERS`, `MAX_WORKERS`, `SYSTEM_PORT_BASE`)\n- Job execution settings (`MAX_POSTS_PER_ACCOUNT_PER_DAY`, `DELAY_BETWEEN_JOBS`, `JOB_TIMEOUT`)\n- Retry settings (`MAX_RETRY_ATTEMPTS`, `RETRY_DELAY_MINUTES`, `NON_RETRYABLE_ERRORS`)\n- File paths (`PROGRESS_FILE`, `STATE_FILE`, `LOGS_DIR`, `ACCOUNTS_FILE`)\n- Timeout constants (`ADB_TIMEOUT`, `ADB_READY_TIMEOUT`, `APPIUM_CONNECT_TIMEOUT`, `PHONE_BOOT_TIMEOUT`)\n- Helper functions: `setup_environment()`, `get_adb_env()`, `_validate_config()`\n\n**Files already using config.py correctly:**\n- `post_reel_smart.py` (lines 18-38): imports `Config, setup_environment`\n- `parallel_config.py` (lines 25-87): imports `Config` and uses its values as defaults\n- `parallel_worker.py` (line 47): uses `ADB_PATH = Config.ADB_PATH`\n- `parallel_orchestrator.py` (line 412): uses `Config.ADB_PATH`\n\n**Files with hardcoded paths that need migration:**\n1. `adb_controller.py` (line 9): `ADB_PATH = r\"C:\\Users\\...\\adb.exe\"` (different path!)\n2. `diagnose_adbkeyboard.py` (line 10): `ADB_PATH = r\"C:\\Users\\...\\adb.exe\"`\n3. `setup_adbkeyboard.py` (line 17): `ADB_PATH = r\"C:\\Users\\...\\adb.exe\"`\n4. `setup_clipboard_helper.py` (line 17): `ADB_PATH = r\"C:\\Users\\...\\adb.exe\"`\n5. `fix_adbkeyboard.py` (line 18): `ADB_PATH = r\"C:\\Users\\...\\adb.exe\"`\n6. `reprovision_phone.py` (line 21): `ADB_PATH = r\"C:\\Users\\...\\adb.exe\"`\n7. `test_typing.py` (line 17): `ADB_PATH = r\"C:\\Users\\...\\adb.exe\"`\n8. `posting_scheduler.py` (lines 17-19): directly sets `os.environ['ANDROID_HOME']`\n9. `debug_page_source.py` (line 6): sets `os.environ['ANDROID_HOME']`\n10. `test_appium.py` (line 15): sets `os.environ['ANDROID_HOME']`\n11. `test_appium_typing.py` (line 10): sets `os.environ['ANDROID_HOME']`\n12. `test_dump_ui_fix.py` (line 6): sets `os.environ['ANDROID_HOME']`\n13. `test_full_flow_android15.py` (line 6): sets `os.environ['ANDROID_HOME']`\n\n## Implementation Steps\n\n### 1. Expand config.py if needed\nThe existing `config.py` is well-structured. Verify it contains all needed settings. Add if missing:\n- `APK_DIR` for APK file locations (ADBKeyboard.apk, ClipboardHelper.apk)\n\n```python\n# Add to Config class:\nAPK_DIR: str = os.path.dirname(os.path.abspath(__file__))\nADBKEYBOARD_APK: str = os.path.join(APK_DIR, \"ADBKeyboard.apk\")\nCLIPBOARD_HELPER_APK: str = os.path.join(APK_DIR, \"ClipboardHelper.apk\")\n```\n\n### 2. Migrate adb_controller.py\n```python\n# Replace line 9\n# OLD: ADB_PATH = r\"C:\\Users\\asus\\Downloads\\platform-tools-latest-windows\\platform-tools\\adb.exe\"\n# NEW:\nfrom config import Config\nADB_PATH = Config.ADB_PATH\n```\n\n### 3. Migrate utility scripts\nFor each of these files, add at the top:\n```python\nfrom config import Config, setup_environment\nsetup_environment()  # Only if they use Appium\n\nADB_PATH = Config.ADB_PATH\nAPK_PATH = Config.ADBKEYBOARD_APK  # or CLIPBOARD_HELPER_APK as appropriate\n```\n\nFiles: `diagnose_adbkeyboard.py`, `setup_adbkeyboard.py`, `setup_clipboard_helper.py`, `fix_adbkeyboard.py`, `reprovision_phone.py`, `test_typing.py`\n\n### 4. Migrate posting_scheduler.py\n```python\n# Replace lines 17-19\n# OLD:\n# os.environ['ANDROID_HOME'] = r'C:\\Users\\asus\\Downloads\\android-sdk'\n# os.environ['ANDROID_SDK_ROOT'] = r'C:\\Users\\asus\\Downloads\\android-sdk'\n# NEW:\nfrom config import Config, setup_environment\nsetup_environment()\n```\n\nAlso remove duplicate `get_appium_env()` function (lines 186-199) and use `get_adb_env()` from config.py instead.\n\n### 5. Migrate test files\nFor `debug_page_source.py`, `test_appium.py`, `test_appium_typing.py`, `test_dump_ui_fix.py`, `test_full_flow_android15.py`:\n```python\n# Replace direct os.environ calls\nfrom config import Config, setup_environment\nsetup_environment()\nADB_PATH = Config.ADB_PATH\n```\n\n### 6. Address the ADB_PATH discrepancy\nNote: Some files use `C:\\Users\\asus\\Downloads\\platform-tools-latest-windows\\platform-tools\\adb.exe` while config.py uses `C:\\Users\\asus\\Downloads\\android-sdk\\platform-tools\\adb.exe`. Verify which is correct and update config.py if needed:\n```python\n# If the standalone platform-tools is preferred:\nADB_PATH: str = r\"C:\\Users\\asus\\Downloads\\platform-tools-latest-windows\\platform-tools\\adb.exe\"\n# OR keep deriving from ANDROID_SDK_PATH if SDK path is correct\n```\n\n### 7. Update CLAUDE.md documentation\nUpdate the Key Files table to emphasize config.py as the single source of truth:\n```markdown\n| File | Purpose |\n|------|---------|\n| `config.py` | **SINGLE SOURCE OF TRUTH** - All paths, settings, timeouts |\n```\n\n## Important Considerations\n\n1. **Import order matters**: `setup_environment()` must be called BEFORE any Appium imports\n2. **Backward compatibility**: Keep the module-level `ADB_PATH` variable for files that use it\n3. **Validation**: `_validate_config()` runs on import and warns about missing paths\n4. **Environment propagation**: Use `get_adb_env()` when spawning subprocesses",
        "testStrategy": "## Test Strategy\n\n### 1. Verify config.py loads without errors\n```bash\npython -c \"from config import Config, setup_environment; setup_environment(); print('OK')\"\n```\n\n### 2. Verify all migrated files import successfully\n```bash\n# Test each migrated file\npython -c \"import adb_controller; print('adb_controller OK')\"\npython -c \"import diagnose_adbkeyboard; print('diagnose_adbkeyboard OK')\"\npython -c \"import setup_adbkeyboard; print('setup_adbkeyboard OK')\"\npython -c \"import setup_clipboard_helper; print('setup_clipboard_helper OK')\"\npython -c \"import fix_adbkeyboard; print('fix_adbkeyboard OK')\"\npython -c \"import reprovision_phone; print('reprovision_phone OK')\"\npython -c \"import posting_scheduler; print('posting_scheduler OK')\"\n```\n\n### 3. Verify ADB_PATH is consistent across all modules\n```bash\npython -c \"\nfrom config import Config\nimport adb_controller\nimport post_reel_smart\nimport parallel_worker\n\npaths = [\n    ('config', Config.ADB_PATH),\n    ('adb_controller', adb_controller.ADB_PATH),\n    ('post_reel_smart', post_reel_smart.ADB_PATH),\n    ('parallel_worker', parallel_worker.ADB_PATH),\n]\nfor name, path in paths:\n    print(f'{name}: {path}')\n\n# All should be identical\nunique = set(p for _, p in paths)\nassert len(unique) == 1, f'ADB_PATH mismatch: {unique}'\nprint('All ADB_PATH values match!')\n\"\n```\n\n### 4. Verify environment is set up correctly\n```bash\npython -c \"\nimport os\nfrom config import setup_environment\nsetup_environment()\nprint(f\\\"ANDROID_HOME={os.environ.get('ANDROID_HOME')}\\\")\nprint(f\\\"ANDROID_SDK_ROOT={os.environ.get('ANDROID_SDK_ROOT')}\\\")\nassert 'ANDROID_HOME' in os.environ\nassert 'ANDROID_SDK_ROOT' in os.environ\nprint('Environment setup OK')\n\"\n```\n\n### 5. Grep verification - no hardcoded paths remain\n```bash\n# Search for hardcoded ADB paths (should only find config.py)\ngrep -r \"platform-tools-latest-windows\" *.py --include=\"*.py\" | grep -v \"archived/\" | grep -v \"config.py\"\n# Expected: no output (empty)\n\n# Search for direct ANDROID_HOME assignments (should only find config.py)\ngrep -rn \"os.environ\\['ANDROID_HOME'\\]\" *.py --include=\"*.py\" | grep -v \"archived/\" | grep -v \"config.py\"\n# Expected: no output (empty)\n```\n\n### 6. Integration test - run actual posting workflow\n```bash\n# Test parallel orchestrator starts correctly\npython parallel_orchestrator.py --status\n\n# Test posting scheduler loads state\npython posting_scheduler.py --status\n```\n\n### 7. Verify subprocess environment propagation\n```bash\npython -c \"\nfrom config import get_adb_env\nenv = get_adb_env()\nassert 'ANDROID_HOME' in env\nassert 'ANDROID_SDK_ROOT' in env\nassert 'platform-tools' in env.get('PATH', '')\nprint('Subprocess environment OK')\n\"\n```",
        "status": "done",
        "dependencies": [
          "16"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T08:10:58.126Z"
      },
      {
        "id": "26",
        "title": "Archive deprecated files to archived/ folder",
        "description": "Move all .backup.py files, batch_post_ARCHIVED.py, batch_post_concurrent_ARCHIVED.py, post_to_instagram.py, and post_reel.py to an archived/ directory to clean up the main codebase while preserving historical code.",
        "details": "## Current State Analysis\n\nBased on codebase analysis, the deprecated files have already been moved to `archived/`:\n- `archived/batch_post_ARCHIVED.py` (8,988 bytes)\n- `archived/batch_post_concurrent_ARCHIVED.py` (7,743 bytes)\n- `archived/post_reel.py` (14,372 bytes)\n- `archived/post_reel_smart.backup.py` (29,764 bytes)\n- `archived/post_to_instagram.py` (7,420 bytes)\n- `archived/setup_adbkeyboard.backup.py` (4,993 bytes)\n- `archived/setup_clipboard_helper.backup.py` (5,071 bytes)\n\nGit status shows these files as deleted from the main directory (marked with `D`), indicating they've been moved but not yet committed.\n\n## Implementation Details\n\n### 1. Verify No Active Imports\n\nGrep analysis confirms no active Python files import from the deprecated modules:\n- No `from batch_post` imports found\n- No `from post_reel` (excluding `post_reel_smart`) imports found  \n- No `from post_to_instagram` imports found\n\n### 2. Update .gitignore (Optional)\n\nConsider whether to track `archived/` in git:\n- **Option A (Recommended)**: Keep `archived/` tracked in git for historical reference\n- **Option B**: Add `archived/` to `.gitignore` if disk space is a concern\n\nCurrent `.gitignore` does not exclude `archived/`, which is the correct default.\n\n### 3. Update Documentation References\n\nThe following documentation files reference deprecated files and may need updates:\n- `reviews/coupling_cohesion_analysis.md` (lines 163, 300, 308, 516) - References `post_to_instagram.py` in historical context\n- These references are acceptable as they document the evolution of the codebase\n\n### 4. Add README to archived/ Folder\n\nCreate `archived/README.md` to document why these files were archived:\n\n```markdown\n# Archived Files\n\nThis directory contains deprecated scripts that are no longer in active use.\nThese files are preserved for historical reference only.\n\n## Why Archived\n\n- **batch_post_ARCHIVED.py** - Replaced by `posting_scheduler.py` with better state management\n- **batch_post_concurrent_ARCHIVED.py** - Replaced by `parallel_orchestrator.py` \n- **post_reel.py** - Original posting script, replaced by `post_reel_smart.py` with Appium support\n- **post_to_instagram.py** - Early implementation using ADBController, deprecated in favor of Appium-based `post_reel_smart.py`\n- **setup_adbkeyboard.backup.py** - Backup of ADB keyboard setup before Android 15 migration\n- **setup_clipboard_helper.backup.py** - Backup of clipboard helper setup\n- **post_reel_smart.backup.py** - Pre-Appium version of smart posting script\n\n## DO NOT USE\n\nThese scripts are NOT maintained and should NOT be used for any purpose other than historical reference.\nThe current production scripts are:\n- `parallel_orchestrator.py` - Main batch posting entry point\n- `posting_scheduler.py` - Single-threaded alternative\n- `post_reel_smart.py` - Core posting logic (used by both)\n```\n\n### 5. Stage and Commit Changes\n\nThe files have been moved but the git changes need to be staged and committed:\n\n```bash\ngit add archived/\ngit add -A  # Stage deletions from root\ngit status  # Verify changes\ngit commit -m \"chore: archive deprecated posting scripts to archived/ folder\"\n```\n\n### 6. Verify No Broken References\n\nAfter archiving, verify the main scripts still work:\n- `python -c \"import posting_scheduler\"` should succeed\n- `python -c \"import parallel_orchestrator\"` should succeed\n- `python -c \"import post_reel_smart\"` should succeed",
        "testStrategy": "## Test Strategy\n\n### 1. Verify Archive Directory Contents\n\n```bash\n# List all files in archived/\nls -la archived/\n\n# Expected: 7 Python files + optional README.md\n# - batch_post_ARCHIVED.py\n# - batch_post_concurrent_ARCHIVED.py\n# - post_reel.py\n# - post_reel_smart.backup.py\n# - post_to_instagram.py\n# - setup_adbkeyboard.backup.py\n# - setup_clipboard_helper.backup.py\n```\n\n### 2. Verify Main Directory is Clean\n\n```bash\n# Check no .backup.py files remain in root\nls *.backup.py 2>/dev/null && echo \"ERROR: backup files still in root\" || echo \"OK: no backup files in root\"\n\n# Check deprecated scripts are gone from root\nls batch_post_ARCHIVED.py batch_post_concurrent_ARCHIVED.py post_reel.py post_to_instagram.py 2>/dev/null && echo \"ERROR: deprecated files in root\" || echo \"OK: deprecated files moved\"\n```\n\n### 3. Verify No Broken Imports\n\n```bash\n# Test all main scripts can be imported\npython -c \"import posting_scheduler; print('posting_scheduler: OK')\"\npython -c \"import parallel_orchestrator; print('parallel_orchestrator: OK')\"\npython -c \"import post_reel_smart; print('post_reel_smart: OK')\"\npython -c \"import parallel_worker; print('parallel_worker: OK')\"\npython -c \"import progress_tracker; print('progress_tracker: OK')\"\npython -c \"import geelark_client; print('geelark_client: OK')\"\n```\n\n### 4. Verify Git Status\n\n```bash\n# Check git status shows clean working directory after commit\ngit status\n\n# Expected output after commit:\n# \"nothing to commit, working tree clean\" (or only untracked files)\n```\n\n### 5. Verify No Import References to Archived Files\n\n```bash\n# Search for any imports of archived modules in active code\ngrep -r \"from batch_post\" --include=\"*.py\" --exclude-dir=archived || echo \"No batch_post imports found\"\ngrep -r \"from post_reel\\b\" --include=\"*.py\" --exclude-dir=archived || echo \"No post_reel imports found\"\ngrep -r \"from post_to_instagram\" --include=\"*.py\" --exclude-dir=archived || echo \"No post_to_instagram imports found\"\ngrep -r \"import batch_post\" --include=\"*.py\" --exclude-dir=archived || echo \"No batch_post imports found\"\ngrep -r \"import post_reel\\b\" --include=\"*.py\" --exclude-dir=archived || echo \"No post_reel imports found\"\n```\n\n### 6. Run Smoke Test of Main Entry Points\n\n```bash\n# Verify main scripts can show their help/usage\npython posting_scheduler.py --status\npython parallel_orchestrator.py --status\n```\n\n### 7. Verify README Exists in archived/ (if created)\n\n```bash\ntest -f archived/README.md && echo \"README exists\" || echo \"README missing (optional)\"\n```",
        "status": "done",
        "dependencies": [
          "25"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T08:13:20.950Z"
      },
      {
        "id": "27",
        "title": "Add retry_all_failed() convenience methods to ProgressTracker",
        "description": "Implement retry_failed_job(job_id) and retry_all_failed() methods that reset failed jobs back to RETRYING status for another attempt, with automatic invocation when the orchestrator starts with --run.",
        "details": "## NOTE: This task is ALREADY IMPLEMENTED\n\nAfter analyzing the codebase, I found that both methods and the orchestrator integration already exist:\n\n### Existing Implementation in progress_tracker.py\n\n**1. retry_failed_job(job_id: str) -> bool (lines 753-786):**\n- Resets a single failed job back to RETRYING status\n- Clears attempts counter to '0', error_type, retry_at, worker_id, and completed_at\n- Returns True if job was found and reset, False otherwise\n- Only operates on jobs with STATUS_FAILED status\n\n**2. retry_all_failed(include_non_retryable: bool = False) -> int (lines 788-833):**\n- Bulk-resets ALL failed jobs back to RETRYING status\n- Uses NON_RETRYABLE_ERRORS set: {'suspended', 'captcha', 'loggedout', 'actionblocked', 'banned'}\n- When include_non_retryable=False (default), skips jobs with non-retryable error types\n- When include_non_retryable=True, also retries suspended/captcha/loggedout jobs\n- Returns count of jobs reset\n\n### Existing Integration in parallel_orchestrator.py\n\n**Automatic invocation on --run (lines 886-899):**\n```python\nif retry_all_failed and tracker.exists():\n    stats_before = tracker.get_stats()\n    if stats_before['failed'] > 0:\n        logger.info(\"RETRYING FAILED JOBS FROM PREVIOUS RUNS\")\n        count = tracker.retry_all_failed(include_non_retryable=retry_include_non_retryable)\n```\n\n**CLI flag support (lines 984-987):**\n- `--retry-all-failed`: Standalone command to reset failed jobs\n- `--retry-include-non-retryable`: Include non-retryable errors when retrying\n\n**run_parallel_posting() parameters (lines 832-834):**\n- `retry_all_failed: bool = True` - Always enabled by default on --run\n- `retry_include_non_retryable: bool = False` - Respects non-retryable classification by default\n\n### If Task Requires Changes, Consider:\n1. **No changes needed** - Mark this task as already done\n2. **Enhancement requests**: Add additional retry options like retry delay, max retry count override, or selective retry by error type",
        "testStrategy": "## Verification of Existing Implementation\n\n### 1. Unit Test retry_failed_job()\n```bash\npython -c \"\nfrom progress_tracker import ProgressTracker\nimport tempfile, os\n\n# Create temp progress file with failed job\ntracker = ProgressTracker(tempfile.mktemp(suffix='.csv'))\ntracker.seed_from_jobs([{'job_id': 'test1', 'account': 'acc1', 'video_path': '/v1.mp4', 'caption': 'test'}])\ntracker.claim_next_job(worker_id=0)\ntracker.update_job_status('test1', 'failed', worker_id=0, error='Test error')\n\n# Verify job is failed\nstats = tracker.get_stats()\nassert stats['failed'] == 1, f'Expected 1 failed, got {stats}'\n\n# Retry the failed job\nresult = tracker.retry_failed_job('test1')\nassert result == True, 'retry_failed_job should return True'\n\n# Verify job is now retrying\nstats = tracker.get_stats()\nassert stats['retrying'] == 1, f'Expected 1 retrying, got {stats}'\nassert stats['failed'] == 0, f'Expected 0 failed, got {stats}'\n\nprint('PASS: retry_failed_job() works correctly')\n\"\n```\n\n### 2. Unit Test retry_all_failed() with non-retryable filtering\n```bash\npython -c \"\nfrom progress_tracker import ProgressTracker\nimport tempfile\n\ntracker = ProgressTracker(tempfile.mktemp(suffix='.csv'))\ntracker.seed_from_jobs([\n    {'job_id': 'j1', 'account': 'a1', 'video_path': '/v1.mp4', 'caption': 't1'},\n    {'job_id': 'j2', 'account': 'a2', 'video_path': '/v2.mp4', 'caption': 't2'},\n    {'job_id': 'j3', 'account': 'a3', 'video_path': '/v3.mp4', 'caption': 't3'},\n])\n\n# Claim and fail with different error types\nfor jid, err in [('j1', 'timeout'), ('j2', 'account suspended'), ('j3', 'network error')]:\n    tracker.claim_next_job(worker_id=0)\n    tracker.update_job_status(jid, 'failed', worker_id=0, error=err)\n\n# Without include_non_retryable: should retry j1, j3 but NOT j2 (suspended)\ncount = tracker.retry_all_failed(include_non_retryable=False)\nstats = tracker.get_stats()\nassert count == 2, f'Expected 2 retried, got {count}'\nassert stats['failed'] == 1, f'Expected 1 still failed (suspended), got {stats}'\n\n# With include_non_retryable: should retry j2 too\ncount2 = tracker.retry_all_failed(include_non_retryable=True)\nassert count2 == 1, f'Expected 1 more retried, got {count2}'\n\nprint('PASS: retry_all_failed() respects non-retryable errors')\n\"\n```\n\n### 3. Integration Test with Orchestrator --run\n```bash\n# Create a test scenario with failed jobs\npython -c \"\nfrom progress_tracker import ProgressTracker\ntracker = ProgressTracker('parallel_progress.csv')\nif tracker.exists():\n    stats = tracker.get_stats()\n    print(f'Before: {stats[\\\"failed\\\"]} failed, {stats[\\\"retrying\\\"]} retrying')\n\"\n\n# Run orchestrator - verify it auto-retries failed jobs\npython parallel_orchestrator.py --status\n\n# If failed > 0, run with --run (dry) to see retry logic trigger:\n# python parallel_orchestrator.py --workers 1 --run\n# Look for log: \"RETRYING FAILED JOBS FROM PREVIOUS RUNS\"\n```\n\n### 4. Manual CLI Test\n```bash\n# Test standalone retry command\npython parallel_orchestrator.py --retry-all-failed\npython parallel_orchestrator.py --retry-all-failed --retry-include-non-retryable\n```",
        "status": "done",
        "dependencies": [
          "19",
          "20"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T08:14:43.997Z"
      },
      {
        "id": "28",
        "title": "Update CLAUDE.md documentation with parallel orchestrator architecture",
        "description": "Document the parallel_orchestrator.py as the primary entry point for batch posting, explain the worker architecture (separate processes with dedicated Appium servers), document progress tracking via parallel_progress.csv, and update the Key Files table with new modules.",
        "details": "## Current State Analysis\n\nThe CLAUDE.md file (399 lines) has basic parallel orchestrator documentation at lines 199-239, but it is outdated and incomplete. The following areas need updating:\n\n### 1. Key Files Table Update (Lines 263-274)\n\n**Current Table (missing new modules):**\n```markdown\n| File | Purpose |\n|------|---------|\n| `posting_scheduler.py` | **MAIN SCRIPT** - scheduler with tracking, retry, state persistence |\n| `post_reel_smart.py` | Core posting logic for single phone (Appium timeout: 60s) |\n| `geelark_client.py` | Geelark API wrapper (upload timeout: 60s) |\n| `dashboard.py` | Real-time web dashboard (http://localhost:5000) |\n| `scheduler_state.json` | Persistent state (auto-generated) |\n| `geelark_batch.log` | Execution log with phase info |\n| `geelark_api.log` | API response log (for Geelark support) |\n```\n\n**Add these new modules to the table:**\n- `parallel_orchestrator.py` - **PRIMARY ENTRY POINT** for parallel batch posting\n- `parallel_worker.py` - Individual worker process (one per Appium server)\n- `parallel_config.py` - Worker configuration (ports, systemPorts, log files)\n- `progress_tracker.py` - File-locked CSV progress tracking with retry support\n- `config.py` - Centralized configuration (all paths, timeouts, constants)\n- `parallel_progress.csv` - Daily job ledger (file-locked, NEVER delete manually)\n\n### 2. Parallel Orchestrator Architecture Section Update (Lines 199-239)\n\nExpand the architecture documentation to include:\n\n**Worker Architecture Details:**\n```\nparallel_orchestrator.py (main process)\n    │\n    ├── Worker 0 ──► Appium:4723 ──► systemPort:8200-8209 ──► Phone\n    ├── Worker 1 ──► Appium:4725 ──► systemPort:8210-8219 ──► Phone\n    ├── Worker 2 ──► Appium:4727 ──► systemPort:8220-8229 ──► Phone\n    └── Worker N ──► Appium:472X ──► systemPort:82X0-82X9 ──► Phone\n\n    Coordination: Workers communicate via file-locked parallel_progress.csv\n    Logs: logs/worker_N.log, logs/appium_N.log per worker\n```\n\n**Key Implementation Details to Document:**\n- Each worker is a SEPARATE PROCESS (subprocess.Popen), not a thread\n- Port allocation: Appium on odd ports (4723, 4725, ...), systemPorts in 10-port ranges\n- Workers coordinate via ProgressTracker using portalocker file locking\n- Config from config.py: MAX_POSTS_PER_ACCOUNT_PER_DAY=1, MAX_RETRY_ATTEMPTS=3\n- State machine states in parallel_worker.py: STARTING → ADB_PENDING → ADB_READY → APPIUM_READY → JOB_RUNNING\n\n### 3. Progress Tracking Documentation (New Section)\n\nAdd detailed documentation about the progress tracking system:\n\n```markdown\n## Progress Tracking (parallel_progress.csv)\n\nThe daily job ledger that tracks ALL posting jobs. Uses file locking (portalocker)\nto ensure only one worker can claim a job at a time.\n\n### CSV Columns:\n- job_id, account, video_path, caption\n- status: pending/claimed/success/failed/skipped/retrying\n- worker_id, claimed_at, completed_at, error\n- attempts, max_attempts, retry_at, error_type\n\n### Status Transitions:\npending → claimed (worker claims job)\nclaimed → success (post succeeded)\nclaimed → retrying (post failed, will retry)\nretrying → claimed (worker claims retry job)\nretrying → failed (max attempts reached or non-retryable error)\n\n### Non-Retryable Errors:\nsuspended, captcha, loggedout, actionblocked, banned\n```\n\n### 4. Config.py Documentation (New Section)\n\nDocument the centralized configuration:\n\n```markdown\n## Centralized Configuration (config.py)\n\nAll paths and settings are defined in config.py. NEVER hardcode paths elsewhere.\n\nKey Settings:\n- ANDROID_SDK_PATH: C:\\Users\\asus\\Downloads\\android-sdk\n- ADB_PATH: {SDK}\\platform-tools\\adb.exe\n- MAX_POSTS_PER_ACCOUNT_PER_DAY: 1\n- MAX_RETRY_ATTEMPTS: 3\n- RETRY_DELAY_MINUTES: 5\n- JOB_TIMEOUT: 300s\n```\n\n### 5. Update MAIN ENTRY POINTS Section (Lines 185-195)\n\nEmphasize parallel_orchestrator.py as the PRIMARY method:\n\n```markdown\n## MAIN ENTRY POINTS\n\n### For Parallel Posting (PRIMARY - RECOMMENDED):\n```bash\npython parallel_orchestrator.py --workers 5 --run\n```\n\n### For Single-Threaded Posting (Legacy):\n```bash\npython posting_scheduler.py --add-folder chunk_01c --run\n```\n```\n\n### Implementation Notes\n\n1. Update the Key Files table to include all new modules with accurate descriptions\n2. Expand the worker architecture diagram with systemPort allocations\n3. Add a new \"Progress Tracking\" section explaining the CSV ledger\n4. Add a new \"Centralized Configuration\" section documenting config.py\n5. Update the \"MAIN ENTRY POINTS\" section to emphasize parallel_orchestrator\n6. Ensure worker stagger timing (60s between starts) is documented\n7. Document the retry system (attempts, delay, non-retryable errors)",
        "testStrategy": "## Test Strategy\n\n### 1. Documentation Accuracy Verification\n\n**Verify Key Files table matches actual files:**\n```bash\n# Check all documented files exist\nls -la parallel_orchestrator.py parallel_worker.py parallel_config.py progress_tracker.py config.py\n\n# Verify CSV columns match ProgressTracker.COLUMNS\npython -c \"from progress_tracker import ProgressTracker; print(ProgressTracker.COLUMNS)\"\n```\n\n**Verify port allocations match code:**\n```bash\npython -c \"\nfrom config import Config\nprint(f'Base Appium: {Config.APPIUM_BASE_PORT}')\nprint(f'System port base: {Config.SYSTEM_PORT_BASE}')\nfor i in range(3):\n    print(f'Worker {i}: Appium {Config.get_worker_appium_port(i)}, systemPort {Config.get_worker_system_port_range(i)}')\n\"\n```\n\n### 2. Documentation Completeness Check\n\n**Ensure all CLI flags are documented:**\n```bash\npython parallel_orchestrator.py --help\n```\n\nCompare output against CLAUDE.md documentation for completeness.\n\n**Verify config values match documentation:**\n```bash\npython -c \"\nfrom config import Config\nprint(f'MAX_POSTS_PER_ACCOUNT_PER_DAY: {Config.MAX_POSTS_PER_ACCOUNT_PER_DAY}')\nprint(f'MAX_RETRY_ATTEMPTS: {Config.MAX_RETRY_ATTEMPTS}')\nprint(f'RETRY_DELAY_MINUTES: {Config.RETRY_DELAY_MINUTES}')\nprint(f'JOB_TIMEOUT: {Config.JOB_TIMEOUT}')\n\"\n```\n\n### 3. Code-Documentation Consistency\n\n**Verify worker architecture matches implementation:**\n```bash\n# Check worker startup in orchestrator\ngrep -n \"start_worker_process\\|Popen\\|stagger\" parallel_orchestrator.py\n\n# Check state machine states in worker\ngrep -n \"WorkerState\\|STARTING\\|ADB_PENDING\\|ADB_READY\" parallel_worker.py\n```\n\n**Verify status values match code:**\n```bash\npython -c \"\nfrom progress_tracker import ProgressTracker\nprint('Statuses:', [s for s in dir(ProgressTracker) if s.startswith('STATUS_')])\nprint('Non-retryable:', ProgressTracker.NON_RETRYABLE_ERRORS)\n\"\n```\n\n### 4. Functional Verification\n\n**Test that documented commands work:**\n```bash\n# Test status command\npython parallel_orchestrator.py --status\n\n# Test reset-day (dry run - just verify it parses)\npython parallel_orchestrator.py --help | grep reset-day\n```\n\n### 5. Cross-Reference Check\n\nVerify all CRITICAL sections in CLAUDE.md reference the correct files:\n- \"PROGRESS FILE MANAGEMENT\" → parallel_progress.csv\n- \"ACCOUNT MANAGEMENT\" → accounts.txt, scheduler_state.json\n- \"STOP PHONES\" → stop script uses GeelarkClient correctly\n\n### 6. Markdown Rendering Test\n\nOpen CLAUDE.md in a markdown viewer or VS Code preview to ensure:\n- Tables render correctly\n- Code blocks have proper syntax highlighting\n- Architecture diagrams are properly formatted\n- All links (if any) are valid",
        "status": "done",
        "dependencies": [
          "25",
          "26"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T08:16:55.800Z"
      },
      {
        "id": "29",
        "title": "Add HTTP timeout to GeelarkClient._request()",
        "description": "Add a configurable timeout parameter to all HTTP requests in geelark_client.py to prevent indefinite hangs on network issues, ensuring posting operations fail fast and can be retried rather than blocking workers indefinitely.",
        "details": "## Problem Statement\n\nThe `GeelarkClient._request()` method at line 49 of `geelark_client.py` uses `requests.post()` without a timeout parameter:\n```python\nresp = requests.post(url, json=data or {}, headers=headers)\n```\n\nSimilarly, `upload_file_to_geelark()` at line 161 uses `requests.put()` without a timeout:\n```python\nresp = requests.put(upload_url, data=f)\n```\n\nWithout timeouts, these calls can hang indefinitely on network issues, causing workers to become stuck and reducing system throughput.\n\n## Implementation Steps\n\n### 1. Add HTTP_TIMEOUT constant to config.py\n\nAdd to the TIMEOUTS section (around line 106-118):\n```python\n# HTTP request timeout for Geelark API calls (seconds)\nHTTP_API_TIMEOUT: int = 30\n\n# HTTP timeout for file uploads (larger files need more time)\nHTTP_UPLOAD_TIMEOUT: int = 120\n```\n\n### 2. Update GeelarkClient._request() to use timeout\n\nIn `geelark_client.py`, import Config and add timeout to the POST request:\n\n```python\nfrom config import Config\n\n# In _request() method (line 49):\nresp = requests.post(url, json=data or {}, headers=headers, timeout=Config.HTTP_API_TIMEOUT)\n```\n\n### 3. Update upload_file_to_geelark() to use timeout\n\n```python\n# In upload_file_to_geelark() method (line 161):\nresp = requests.put(upload_url, data=f, timeout=Config.HTTP_UPLOAD_TIMEOUT)\n```\n\n### 4. Add proper exception handling for timeout errors\n\nWrap the requests calls in try-except to handle `requests.exceptions.Timeout` and `requests.exceptions.ConnectionError`:\n\n```python\ndef _request(self, endpoint, data=None):\n    \"\"\"Make API request with full response logging\"\"\"\n    url = f\"{API_BASE}{endpoint}\"\n    headers = self._get_headers()\n    \n    start_time = time.time()\n    api_logger.debug(f\"REQUEST: {endpoint} data={data}\")\n    \n    try:\n        resp = requests.post(\n            url, \n            json=data or {}, \n            headers=headers, \n            timeout=Config.HTTP_API_TIMEOUT\n        )\n    except requests.exceptions.Timeout:\n        api_logger.error(f\"TIMEOUT: {endpoint} after {Config.HTTP_API_TIMEOUT}s\")\n        raise Exception(f\"API timeout: {endpoint} did not respond within {Config.HTTP_API_TIMEOUT}s\")\n    except requests.exceptions.ConnectionError as e:\n        api_logger.error(f\"CONNECTION ERROR: {endpoint} - {e}\")\n        raise Exception(f\"API connection error: {endpoint} - {e}\")\n    \n    elapsed = time.time() - start_time\n    # ... rest of method unchanged\n```\n\n### 5. Similar handling for upload_file_to_geelark()\n\n```python\ndef upload_file_to_geelark(self, local_path):\n    \"\"\"Upload a local file to Geelark's temp storage, return resource URL\"\"\"\n    # ... existing code to get upload_url and resource_url ...\n    \n    try:\n        with open(local_path, \"rb\") as f:\n            resp = requests.put(upload_url, data=f, timeout=Config.HTTP_UPLOAD_TIMEOUT)\n    except requests.exceptions.Timeout:\n        raise Exception(f\"Upload timeout: file upload did not complete within {Config.HTTP_UPLOAD_TIMEOUT}s\")\n    except requests.exceptions.ConnectionError as e:\n        raise Exception(f\"Upload connection error: {e}\")\n    \n    # ... rest of method unchanged\n```\n\n## Rationale for Timeout Values\n\n- **HTTP_API_TIMEOUT = 30s**: Most Geelark API calls are simple JSON exchanges. 30 seconds is generous for normal operations while preventing indefinite hangs.\n- **HTTP_UPLOAD_TIMEOUT = 120s**: File uploads (videos) can be several MB, requiring more time. 120 seconds accommodates larger files over slower connections.\n\n## Files to Modify\n\n1. `config.py` - Add HTTP_API_TIMEOUT and HTTP_UPLOAD_TIMEOUT constants\n2. `geelark_client.py` - Add timeout parameter and exception handling to _request() and upload_file_to_geelark()",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Test - Verify timeout parameter is passed\n\n```bash\n# Quick verification that requests.post is called with timeout\npython -c \"\nimport geelark_client\nimport requests\nfrom unittest.mock import patch, MagicMock\n\n# Mock successful response\nmock_resp = MagicMock()\nmock_resp.status_code = 200\nmock_resp.json.return_value = {'code': 0, 'data': {'items': []}}\nmock_resp.text = '{}'\nmock_resp.headers = {}\n\nwith patch.object(requests, 'post', return_value=mock_resp) as mock_post:\n    client = geelark_client.GeelarkClient()\n    client._request('/test/endpoint', {'test': 'data'})\n    \n    # Verify timeout was passed\n    call_kwargs = mock_post.call_args.kwargs\n    assert 'timeout' in call_kwargs, 'timeout parameter not passed to requests.post'\n    assert call_kwargs['timeout'] == 30, f'Expected timeout=30, got {call_kwargs[\\\"timeout\\\"]}'\n    print('✓ requests.post called with timeout=30')\n\"\n```\n\n### 2. Unit Test - Verify timeout exception handling\n\n```bash\npython -c \"\nimport geelark_client\nimport requests\nfrom unittest.mock import patch\n\n# Test Timeout exception is caught and re-raised with descriptive message\nwith patch.object(requests, 'post', side_effect=requests.exceptions.Timeout()):\n    client = geelark_client.GeelarkClient()\n    try:\n        client._request('/test/endpoint', {})\n        print('✗ Expected exception was not raised')\n    except Exception as e:\n        assert 'timeout' in str(e).lower(), f'Exception message should mention timeout: {e}'\n        print(f'✓ Timeout properly caught and re-raised: {e}')\n\"\n```\n\n### 3. Unit Test - Verify connection error handling\n\n```bash\npython -c \"\nimport geelark_client\nimport requests\nfrom unittest.mock import patch\n\n# Test ConnectionError exception is caught and re-raised\nwith patch.object(requests, 'post', side_effect=requests.exceptions.ConnectionError('Network unreachable')):\n    client = geelark_client.GeelarkClient()\n    try:\n        client._request('/test/endpoint', {})\n        print('✗ Expected exception was not raised')\n    except Exception as e:\n        assert 'connection' in str(e).lower(), f'Exception message should mention connection: {e}'\n        print(f'✓ ConnectionError properly caught and re-raised: {e}')\n\"\n```\n\n### 4. Verify config.py has timeout constants\n\n```bash\npython -c \"\nfrom config import Config\nassert hasattr(Config, 'HTTP_API_TIMEOUT'), 'Missing HTTP_API_TIMEOUT'\nassert hasattr(Config, 'HTTP_UPLOAD_TIMEOUT'), 'Missing HTTP_UPLOAD_TIMEOUT'\nassert Config.HTTP_API_TIMEOUT == 30, f'Expected HTTP_API_TIMEOUT=30, got {Config.HTTP_API_TIMEOUT}'\nassert Config.HTTP_UPLOAD_TIMEOUT == 120, f'Expected HTTP_UPLOAD_TIMEOUT=120, got {Config.HTTP_UPLOAD_TIMEOUT}'\nprint(f'✓ Config.HTTP_API_TIMEOUT = {Config.HTTP_API_TIMEOUT}')\nprint(f'✓ Config.HTTP_UPLOAD_TIMEOUT = {Config.HTTP_UPLOAD_TIMEOUT}')\n\"\n```\n\n### 5. Integration Test - Live API call with timeout\n\n```bash\n# Test that actual API calls work with the timeout\npython -c \"\nfrom geelark_client import GeelarkClient\nclient = GeelarkClient()\ntry:\n    result = client.list_phones(page_size=1)\n    print(f'✓ API call succeeded with timeout: {len(result.get(\\\"items\\\", []))} phones')\nexcept Exception as e:\n    if 'timeout' in str(e).lower():\n        print(f'⚠ API timed out (may indicate slow network): {e}')\n    else:\n        print(f'✗ API call failed: {e}')\n\"\n```\n\n### 6. Verify upload timeout on upload_file_to_geelark()\n\n```bash\npython -c \"\nimport geelark_client\nimport requests\nfrom unittest.mock import patch, MagicMock\n\n# Mock get_upload_url response\nmock_get_url = MagicMock()\nmock_get_url.return_value = {'uploadUrl': 'https://test.com/upload', 'resourceUrl': 'https://test.com/resource'}\n\n# Mock successful PUT response\nmock_resp = MagicMock()\nmock_resp.status_code = 200\n\nwith patch.object(requests, 'put', return_value=mock_resp) as mock_put:\n    with patch.object(geelark_client.GeelarkClient, 'get_upload_url', mock_get_url):\n        client = geelark_client.GeelarkClient()\n        # Create a small temp file for testing\n        import tempfile\n        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as f:\n            f.write(b'test data')\n            temp_path = f.name\n        \n        try:\n            client.upload_file_to_geelark(temp_path)\n            call_kwargs = mock_put.call_args.kwargs\n            assert 'timeout' in call_kwargs, 'timeout parameter not passed to requests.put'\n            assert call_kwargs['timeout'] == 120, f'Expected timeout=120, got {call_kwargs[\\\"timeout\\\"]}'\n            print('✓ requests.put called with timeout=120')\n        finally:\n            import os\n            os.unlink(temp_path)\n\"\n```",
        "status": "done",
        "dependencies": [
          "25"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T08:55:34.203Z"
      },
      {
        "id": "30",
        "title": "Add credential validation to GeelarkClient.__init__()",
        "description": "Add fail-fast validation to GeelarkClient.__init__() that checks for required GEELARK_TOKEN at initialization time and raises a clear, actionable error immediately if missing, rather than failing later in the posting flow with cryptic authentication errors.",
        "details": "## Problem Statement\n\nCurrently, `GeelarkClient.__init__()` (lines 26-29 of `geelark_client.py`) simply assigns environment variables without validation:\n```python\ndef __init__(self):\n    self.app_id = os.getenv(\"GEELARK_APP_ID\")\n    self.api_key = os.getenv(\"GEELARK_API_KEY\")\n    self.token = os.getenv(\"GEELARK_TOKEN\")\n```\n\nWhen `GEELARK_TOKEN` is missing, the first API call fails deep in the posting flow at `_get_headers()` (line 37) with `Authorization: Bearer None`, resulting in a confusing HTTP 401 error that doesn't clearly indicate the root cause.\n\n## Implementation Requirements\n\n### 1. Add credential validation in `__init__()`\n\n```python\ndef __init__(self):\n    self.app_id = os.getenv(\"GEELARK_APP_ID\")\n    self.api_key = os.getenv(\"GEELARK_API_KEY\")\n    self.token = os.getenv(\"GEELARK_TOKEN\")\n    \n    # Fail-fast validation\n    self._validate_credentials()\n\ndef _validate_credentials(self):\n    \"\"\"Validate required credentials are present. Raises ValueError if missing.\"\"\"\n    missing = []\n    \n    if not self.token:\n        missing.append(\"GEELARK_TOKEN\")\n    \n    # Optional: validate legacy credentials if used\n    # if not self.app_id:\n    #     missing.append(\"GEELARK_APP_ID\")\n    # if not self.api_key:\n    #     missing.append(\"GEELARK_API_KEY\")\n    \n    if missing:\n        raise ValueError(\n            f\"Missing required Geelark credentials: {', '.join(missing)}. \"\n            f\"Set these in your .env file or environment variables.\"\n        )\n```\n\n### 2. Error message requirements\n\nThe error message should:\n- Clearly state which credentials are missing\n- Mention `.env` file as the expected location\n- Be actionable (tell user what to do)\n\n### 3. Follow existing patterns\n\nThe implementation mirrors the `_validate_config()` pattern in `config.py` (lines 174-185) which validates paths at import time. However, use `ValueError` instead of print warnings since missing credentials make the client non-functional.\n\n### 4. Backward compatibility considerations\n\n- The `app_id` and `api_key` are legacy credentials that may still be used in some code paths\n- Focus validation on `GEELARK_TOKEN` which is the primary auth mechanism (used in `_get_headers()`)\n- Consider making `app_id`/`api_key` validation optional or behind a flag\n\n### 5. Update import error handling in consuming modules\n\nModules like `parallel_worker.py` (line 220), `parallel_orchestrator.py` (lines 379, 812), and `post_reel_smart.py` (line 43) instantiate `GeelarkClient()`. These should catch the `ValueError` if graceful startup failure is needed, though the default behavior of letting it propagate is often correct for fail-fast.",
        "testStrategy": "## Test Strategy\n\n### 1. Manual validation - missing token\n```bash\n# Temporarily rename .env to test missing credentials\nmv .env .env.backup\n\n# Attempt to instantiate client\npython -c \"from geelark_client import GeelarkClient; c = GeelarkClient()\"\n\n# Expected: ValueError with message about missing GEELARK_TOKEN\n# Restore .env\nmv .env.backup .env\n```\n\n### 2. Manual validation - valid credentials\n```bash\n# With valid .env in place\npython -c \"from geelark_client import GeelarkClient; c = GeelarkClient(); print('OK')\"\n\n# Expected: prints 'OK' without error\n```\n\n### 3. Integration test - orchestrator startup\n```bash\n# Test that orchestrator fails fast with clear error\nmv .env .env.backup\npython parallel_orchestrator.py --status 2>&1 | grep -i \"GEELARK_TOKEN\"\n\n# Expected: Error message mentions GEELARK_TOKEN\nmv .env.backup .env\n```\n\n### 4. Verify error message clarity\n```bash\n# Create .env without GEELARK_TOKEN\necho \"ANTHROPIC_API_KEY=test\" > .env.test\nenv -i python -c \"\nimport os\nos.chdir('.')\n# Load empty env\nfrom geelark_client import GeelarkClient\ntry:\n    c = GeelarkClient()\nexcept ValueError as e:\n    print(f'Good: {e}')\n    assert 'GEELARK_TOKEN' in str(e)\n    assert '.env' in str(e)\n\"\nrm .env.test\n```\n\n### 5. Verify existing functionality still works\n```bash\n# Run the client's __main__ test (lists phones)\npython geelark_client.py\n\n# Expected: Should list phones if credentials valid, or clear error if not\n```",
        "status": "done",
        "dependencies": [
          "25"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T08:55:40.574Z"
      },
      {
        "id": "31",
        "title": "Add HTTP connection pooling to GeelarkClient",
        "description": "Create a requests.Session() with HTTPAdapter connection pooling in GeelarkClient.__init__() and migrate all HTTP calls from requests.post()/requests.put() to self.session.post()/self.session.put() to prevent connection exhaustion under parallel worker load.",
        "details": "## Problem Statement\n\nThe current `GeelarkClient` (geelark_client.py lines 40-68) creates a new HTTP connection for every API call via `requests.post()` and `requests.put()`. Under parallel worker load (5+ workers making concurrent API calls), this can lead to:\n- Connection exhaustion (too many simultaneous connections)\n- TCP TIME_WAIT accumulation\n- Increased latency (no connection reuse)\n- Resource leaks under high load\n\n## Current Implementation Analysis\n\n**geelark_client.py line 49:**\n```python\nresp = requests.post(url, json=data or {}, headers=headers)\n```\n\n**geelark_client.py line 161:**\n```python\nresp = requests.put(upload_url, data=f)\n```\n\nEach `GeelarkClient()` instance (created in parallel_worker.py:220, parallel_orchestrator.py:379, post_reel_smart.py:43, etc.) opens fresh connections per request.\n\n## Implementation Steps\n\n### 1. Add requests.Session with HTTPAdapter in __init__()\n\n```python\n# geelark_client.py - imports (add at top)\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n\n# In __init__() after line 29:\ndef __init__(self):\n    self.app_id = os.getenv(\"GEELARK_APP_ID\")\n    self.api_key = os.getenv(\"GEELARK_API_KEY\")\n    self.token = os.getenv(\"GEELARK_TOKEN\")\n    \n    # Create session with connection pooling\n    self.session = requests.Session()\n    \n    # Configure HTTPAdapter with connection pooling\n    adapter = HTTPAdapter(\n        pool_connections=10,  # Number of connection pools to cache\n        pool_maxsize=10,      # Max connections per pool\n        max_retries=Retry(\n            total=3,\n            backoff_factor=0.5,\n            status_forcelist=[502, 503, 504]\n        )\n    )\n    self.session.mount('http://', adapter)\n    self.session.mount('https://', adapter)\n```\n\n### 2. Update _request() to use self.session.post()\n\n```python\n# geelark_client.py line 49 - change:\n# FROM:\nresp = requests.post(url, json=data or {}, headers=headers)\n# TO:\nresp = self.session.post(url, json=data or {}, headers=headers)\n```\n\n### 3. Update upload_file_to_geelark() to use self.session.put()\n\n```python\n# geelark_client.py line 161 - change:\n# FROM:\nresp = requests.put(upload_url, data=f)\n# TO:\nresp = self.session.put(upload_url, data=f)\n```\n\n### 4. Add optional close() method for cleanup\n\n```python\ndef close(self):\n    \"\"\"Close the session and release connections.\"\"\"\n    if hasattr(self, 'session') and self.session:\n        self.session.close()\n\ndef __enter__(self):\n    return self\n\ndef __exit__(self, exc_type, exc_val, exc_tb):\n    self.close()\n```\n\n## Configuration Recommendations\n\nThe `pool_connections=10` and `pool_maxsize=10` values are appropriate because:\n- `Config.MAX_WORKERS` is 10 (config.py line 57)\n- Geelark API is a single host (API_BASE = \"https://openapi.geelark.com\")\n- Each worker may have 1-2 concurrent requests at most\n\n## Integration with Existing Tasks\n\nThis task complements:\n- **Task 29**: HTTP timeout parameter (can be added to session.post/put calls)\n- **Task 30**: Credential validation (should run before session creation)\n\n## Edge Cases to Handle\n\n1. **Session reuse across methods**: All methods inherit the pooled session\n2. **File uploads**: Large file PUT requests should still benefit from pooling\n3. **Concurrent access**: requests.Session is thread-safe for most operations\n4. **Error recovery**: Built-in retry via Retry adapter handles transient failures",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Test - Session Creation\n```bash\npython -c \"\nfrom geelark_client import GeelarkClient\nclient = GeelarkClient()\n\n# Verify session exists\nassert hasattr(client, 'session'), 'Session not created'\nassert client.session is not None, 'Session is None'\n\n# Verify adapters mounted\nadapters = client.session.adapters\nassert 'https://' in adapters, 'HTTPS adapter not mounted'\nassert 'http://' in adapters, 'HTTP adapter not mounted'\n\nprint('Session creation: PASS')\n\"\n```\n\n### 2. Unit Test - Connection Pooling Configuration\n```bash\npython -c \"\nfrom geelark_client import GeelarkClient\nfrom requests.adapters import HTTPAdapter\n\nclient = GeelarkClient()\nadapter = client.session.get_adapter('https://')\n\n# Verify it's an HTTPAdapter (not default)\nassert isinstance(adapter, HTTPAdapter), f'Wrong adapter type: {type(adapter)}'\n\n# Verify pool settings (introspect the adapter)\nconfig = adapter.config\nassert config.get('pool_connections', 0) >= 10, 'pool_connections too low'\nassert config.get('pool_maxsize', 0) >= 10, 'pool_maxsize too low'\n\nprint('Connection pooling config: PASS')\n\"\n```\n\n### 3. Functional Test - API Calls Use Session\n```bash\npython -c \"\nfrom geelark_client import GeelarkClient\nfrom unittest.mock import patch, MagicMock\n\nclient = GeelarkClient()\n\n# Mock the session.post method\nwith patch.object(client.session, 'post') as mock_post:\n    mock_response = MagicMock()\n    mock_response.status_code = 200\n    mock_response.json.return_value = {'code': 0, 'data': {'items': [], 'total': 0}}\n    mock_response.text = '{}'\n    mock_response.headers = {}\n    mock_post.return_value = mock_response\n    \n    # Call list_phones which uses _request()\n    client.list_phones(page_size=1)\n    \n    # Verify session.post was called (not requests.post)\n    assert mock_post.called, 'session.post was not called'\n    print('API calls use session: PASS')\n\"\n```\n\n### 4. Live Test - Parallel Workers\n```bash\n# Start orchestrator with 5 workers briefly\npython parallel_orchestrator.py --workers 5 --status\n\n# Check geelark_api.log for connection patterns\n# Should NOT see TCP connection errors or exhaustion warnings\ntail -20 geelark_api.log\n```\n\n### 5. Load Test - Multiple Concurrent Clients\n```bash\npython -c \"\nimport threading\nimport time\nfrom geelark_client import GeelarkClient\n\nresults = []\nerrors = []\n\ndef make_requests(client_id):\n    try:\n        client = GeelarkClient()\n        # Make 5 requests in quick succession\n        for i in range(5):\n            result = client.list_phones(page_size=1)\n            results.append((client_id, i, 'success'))\n            time.sleep(0.1)\n    except Exception as e:\n        errors.append((client_id, str(e)))\n\n# Spawn 5 threads (simulating 5 workers)\nthreads = []\nfor i in range(5):\n    t = threading.Thread(target=make_requests, args=(i,))\n    threads.append(t)\n    t.start()\n\nfor t in threads:\n    t.join()\n\nprint(f'Successful requests: {len(results)}')\nprint(f'Errors: {len(errors)}')\nif errors:\n    for e in errors:\n        print(f'  Client {e[0]}: {e[1]}')\nelse:\n    print('All concurrent requests succeeded: PASS')\n\"\n```\n\n### 6. Context Manager Test\n```bash\npython -c \"\nfrom geelark_client import GeelarkClient\n\n# Test context manager usage\nwith GeelarkClient() as client:\n    result = client.list_phones(page_size=1)\n    print(f'Got {result.get(\\\"total\\\", 0)} phones')\n\n# Session should be closed after exiting context\nprint('Context manager: PASS')\n\"\n```",
        "status": "done",
        "dependencies": [
          "25",
          "29",
          "30"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T08:55:47.649Z"
      },
      {
        "id": "32",
        "title": "Fix hardcoded ADB path in adb_controller.py",
        "description": "Replace the hardcoded ADB_PATH constant in adb_controller.py with an import from config.py, using Config.ADB_PATH to ensure consistent ADB path usage across the entire codebase.",
        "details": "## Problem Statement\n\nThe `adb_controller.py` module (line 9) has a hardcoded ADB path that differs from the centralized configuration:\n\n**Current hardcoded path in adb_controller.py:**\n```python\nADB_PATH = r\"C:\\Users\\asus\\Downloads\\platform-tools-latest-windows\\platform-tools\\adb.exe\"\n```\n\n**Centralized config.py path (lines 35-38):**\n```python\nANDROID_SDK_PATH: str = r\"C:\\Users\\asus\\Downloads\\android-sdk\"\nADB_PATH: str = os.path.join(ANDROID_SDK_PATH, \"platform-tools\", \"adb.exe\")\n```\n\nThis inconsistency means `adb_controller.py` uses a different ADB executable than the rest of the codebase (parallel_worker.py, post_reel_smart.py, parallel_config.py, parallel_orchestrator.py), which all correctly import from Config.\n\n## Implementation Steps\n\n### Step 1: Add import statement\nAt the top of `adb_controller.py`, add the import for Config:\n\n```python\n\"\"\"\nADB Controller - connects to Geelark devices and runs commands\n\"\"\"\nimport subprocess\nimport time\nimport os\nfrom config import Config\n```\n\n### Step 2: Replace hardcoded ADB_PATH\nRemove line 9 which defines:\n```python\nADB_PATH = r\"C:\\Users\\asus\\Downloads\\platform-tools-latest-windows\\platform-tools\\adb.exe\"\n```\n\nReplace with:\n```python\n# ADB executable path - use centralized config\nADB_PATH = Config.ADB_PATH\n```\n\n### Step 3: Verify no other hardcoded paths\nConfirm the module has no other hardcoded paths that should be centralized.\n\n## Code Changes Summary\n\n**File: adb_controller.py**\n\nBefore (lines 1-10):\n```python\n\"\"\"\nADB Controller - connects to Geelark devices and runs commands\n\"\"\"\nimport subprocess\nimport time\nimport os\n\n# ADB executable path\nADB_PATH = r\"C:\\Users\\asus\\Downloads\\platform-tools-latest-windows\\platform-tools\\adb.exe\"\n```\n\nAfter (lines 1-11):\n```python\n\"\"\"\nADB Controller - connects to Geelark devices and runs commands\n\"\"\"\nimport subprocess\nimport time\nimport os\nfrom config import Config\n\n# ADB executable path - use centralized config\nADB_PATH = Config.ADB_PATH\n```\n\n## Impact Analysis\n\n- **ADBController class**: All methods (connect, disconnect, shell, tap, swipe, type_text, key_event, screenshot_to_file, push_file, launch_app) will use the centralized ADB path\n- **Consistency**: The module will now use the same ADB executable as parallel_worker.py, post_reel_smart.py, and parallel_orchestrator.py\n- **Maintainability**: Future ADB path changes only need to be made in config.py\n\n## Files Modified\n- `adb_controller.py` - Single file modification",
        "testStrategy": "## Test Strategy\n\n### 1. Import verification\n```bash\npython -c \"from adb_controller import ADB_PATH; from config import Config; assert ADB_PATH == Config.ADB_PATH, f'Mismatch: {ADB_PATH} != {Config.ADB_PATH}'; print(f'SUCCESS: ADB_PATH = {ADB_PATH}')\"\n```\n\nExpected output: `SUCCESS: ADB_PATH = C:\\Users\\asus\\Downloads\\android-sdk\\platform-tools\\adb.exe`\n\n### 2. Module import test\n```bash\npython -c \"from adb_controller import ADBController; print('ADBController imported successfully')\"\n```\n\n### 3. Path consistency verification\n```bash\npython -c \"\nfrom adb_controller import ADB_PATH as adb_ctrl_path\nfrom parallel_worker import ADB_PATH as worker_path\nfrom post_reel_smart import ADB_PATH as smart_path\nfrom config import Config\n\nprint(f'adb_controller.py: {adb_ctrl_path}')\nprint(f'parallel_worker.py: {worker_path}')\nprint(f'post_reel_smart.py: {smart_path}')\nprint(f'config.py: {Config.ADB_PATH}')\n\n# All should match\nassert adb_ctrl_path == Config.ADB_PATH, 'adb_controller mismatch'\nassert worker_path == Config.ADB_PATH, 'parallel_worker mismatch'\nassert smart_path == Config.ADB_PATH, 'post_reel_smart mismatch'\nprint('SUCCESS: All ADB paths are consistent')\n\"\n```\n\n### 4. ADB executable existence check\n```bash\npython -c \"\nimport os\nfrom adb_controller import ADB_PATH\nexists = os.path.exists(ADB_PATH)\nprint(f'ADB_PATH exists: {exists} ({ADB_PATH})')\nassert exists, f'ADB not found at {ADB_PATH}'\n\"\n```\n\n### 5. Functional test (if device available)\n```bash\npython -c \"\nfrom adb_controller import ADBController, ADB_PATH\nimport subprocess\n\n# Quick test that ADB can run\nresult = subprocess.run([ADB_PATH, 'version'], capture_output=True, text=True, timeout=10)\nprint(f'ADB version check: {result.stdout.strip()}')\nprint('Functional test PASSED')\n\"\n```\n\n### 6. No regression in existing code\nRun the parallel orchestrator status check to ensure the system still works:\n```bash\npython parallel_orchestrator.py --status\n```",
        "status": "done",
        "dependencies": [
          "25",
          "16"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T08:58:46.461Z"
      },
      {
        "id": "33",
        "title": "Add JSON error handling to vision.py analyze functions",
        "description": "Add try/except JSONDecodeError handling to analyze_screen() and analyze_for_instagram_post() functions in vision.py to prevent crashes when Claude returns malformed JSON responses, using the same error handling pattern already established in post_reel_smart.py.",
        "details": "## Problem Statement\n\nThe `vision.py` module has two functions that call `json.loads()` without error handling:\n- `analyze_screen()` at line 90: `return json.loads(text)`\n- `analyze_for_instagram_post()` at line 174: `return json.loads(text)`\n\nWhen Claude returns malformed JSON (due to truncation, formatting issues, or model errors), these calls raise `json.JSONDecodeError` and crash without graceful error handling.\n\n## Implementation Pattern\n\nFollow the existing error handling pattern from `post_reel_smart.py` (lines 646-655):\n\n```python\ntry:\n    return json.loads(text)\nexcept json.JSONDecodeError as e:\n    # Log full raw response for debugging JSON issues\n    print(f\"  [JSON PARSE ERROR] attempt {attempt+1}: {e}\")\n    print(f\"  Raw response (full): {text}\")\n    if attempt < 2:\n        time.sleep(1)\n        continue\n    raise ValueError(f\"JSON parse failed after 3 attempts: {e}. Response: {text[:100]}\")\n```\n\n## Changes Required\n\n### 1. Update imports at top of vision.py (line 6-7)\n\nAdd `json` to explicit imports (currently imported inline at lines 80, 165) and add `time` for retry delays:\n\n```python\nimport anthropic\nimport base64\nimport json\nimport os\nimport time\n```\n\n### 2. Refactor analyze_screen() (lines 57-90)\n\nWrap the Claude API call and JSON parsing in a retry loop with proper error handling:\n\n```python\n# Replace lines 57-90 with:\nfor attempt in range(3):\n    try:\n        response = client.messages.create(\n            model=\"claude-sonnet-4-20250514\",\n            max_tokens=500,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image\",\n                        \"source\": {\n                            \"type\": \"base64\",\n                            \"media_type\": \"image/png\",\n                            \"data\": image_data\n                        }\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": prompt\n                    }\n                ]\n            }]\n        )\n\n        # Check for empty response\n        if not response.content:\n            if attempt < 2:\n                time.sleep(1)\n                continue\n            return {\"action\": \"error\", \"message\": \"Claude returned empty response after 3 attempts\"}\n\n        text = response.content[0].text.strip()\n\n        # Check for empty text\n        if not text:\n            if attempt < 2:\n                time.sleep(1)\n                continue\n            return {\"action\": \"error\", \"message\": \"Claude returned empty text after 3 attempts\"}\n\n        # Handle markdown code blocks\n        if text.startswith(\"```\"):\n            text = text.split(\"```\")[1]\n            if text.startswith(\"json\"):\n                text = text[4:]\n            text = text.strip()\n\n        try:\n            return json.loads(text)\n        except json.JSONDecodeError as e:\n            print(f\"  [JSON PARSE ERROR in analyze_screen] attempt {attempt+1}: {e}\")\n            print(f\"  Raw response: {text}\")\n            if attempt < 2:\n                time.sleep(1)\n                continue\n            return {\"action\": \"error\", \"message\": f\"JSON parse failed: {e}. Response: {text[:200]}\"}\n\n    except anthropic.APIError as e:\n        print(f\"  [API ERROR in analyze_screen] attempt {attempt+1}: {e}\")\n        if attempt < 2:\n            time.sleep(1)\n            continue\n        return {\"action\": \"error\", \"message\": f\"Claude API error after 3 attempts: {e}\"}\n\nreturn {\"action\": \"error\", \"message\": \"Failed to get valid response from Claude after 3 attempts\"}\n```\n\n### 3. Refactor analyze_for_instagram_post() (lines 143-174)\n\nApply the same retry and error handling pattern:\n\n```python\n# Replace lines 143-174 with similar retry loop structure\n# Return dict with action=\"error\" and message field on failure\n# Include video_selected=False in error response for API consistency\n```\n\n### 4. Error Return Format\n\nBoth functions should return a consistent error structure that calling code can handle:\n\n```python\n# analyze_screen error return:\n{\"action\": \"error\", \"message\": \"descriptive error message\"}\n\n# analyze_for_instagram_post error return:\n{\"action\": \"error\", \"message\": \"descriptive error message\", \"video_selected\": False}\n```\n\n### 5. Remove inline imports\n\nRemove the inline `import json` statements at lines 80 and 165 since json will be imported at module level.\n\n## Key Considerations\n\n1. **Graceful degradation**: Return error dict instead of raising exceptions, allowing callers to handle gracefully\n2. **Debugging support**: Log raw responses on parse failure for troubleshooting\n3. **Retry logic**: 3 attempts with 1-second delays matches existing pattern\n4. **API error handling**: Also catch anthropic.APIError for network/rate limit issues\n5. **Consistent API**: Error responses include all expected fields to prevent KeyError in callers",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Test - Malformed JSON Handling\n\n```bash\n# Create test script to verify error handling\npython -c \"\nimport json\nfrom unittest.mock import patch, MagicMock\n\n# Mock anthropic client to return malformed JSON\nmock_response = MagicMock()\nmock_response.content = [MagicMock(text='not valid json {{{')]\n\nwith patch('anthropic.Anthropic') as mock_client:\n    mock_client.return_value.messages.create.return_value = mock_response\n    \n    from vision import analyze_screen\n    result = analyze_screen('test.png', 'test context')\n    \n    # Should return error dict, not raise exception\n    assert result['action'] == 'error', f'Expected error action, got: {result}'\n    assert 'message' in result, 'Error response missing message field'\n    print(f'SUCCESS: Malformed JSON handled gracefully')\n    print(f'Result: {result}')\n\"\n```\n\n### 2. Unit Test - Empty Response Handling\n\n```bash\npython -c \"\nfrom unittest.mock import patch, MagicMock\n\n# Mock empty response\nmock_response = MagicMock()\nmock_response.content = []\n\nwith patch('anthropic.Anthropic') as mock_client:\n    mock_client.return_value.messages.create.return_value = mock_response\n    \n    from vision import analyze_screen\n    result = analyze_screen('test.png', 'test context')\n    \n    assert result['action'] == 'error', f'Expected error action, got: {result}'\n    print('SUCCESS: Empty response handled gracefully')\n\"\n```\n\n### 3. Unit Test - analyze_for_instagram_post Error Fields\n\n```bash\npython -c \"\nfrom unittest.mock import patch, MagicMock\n\nmock_response = MagicMock()\nmock_response.content = [MagicMock(text='invalid')]\n\nwith patch('anthropic.Anthropic') as mock_client:\n    mock_client.return_value.messages.create.return_value = mock_response\n    \n    from vision import analyze_for_instagram_post\n    result = analyze_for_instagram_post('test.png', 'caption')\n    \n    assert result['action'] == 'error', f'Expected error action'\n    assert 'video_selected' in result, 'Missing video_selected field in error response'\n    print('SUCCESS: analyze_for_instagram_post error includes video_selected field')\n\"\n```\n\n### 4. Integration Test - Valid JSON Still Works\n\n```bash\npython -c \"\nfrom unittest.mock import patch, MagicMock\nimport json\n\n# Mock valid JSON response\nvalid_response = {'action': 'tap', 'x': 100, 'y': 200, 'message': 'Tap button'}\nmock_response = MagicMock()\nmock_response.content = [MagicMock(text=json.dumps(valid_response))]\n\nwith patch('anthropic.Anthropic') as mock_client:\n    mock_client.return_value.messages.create.return_value = mock_response\n    \n    from vision import analyze_screen\n    result = analyze_screen('test.png', 'test context')\n    \n    assert result == valid_response, f'Expected {valid_response}, got {result}'\n    print('SUCCESS: Valid JSON parsing still works correctly')\n\"\n```\n\n### 5. Manual Verification - Code Review\n\n```bash\n# Verify imports are at module level\nhead -10 vision.py | grep -E \"^import json|^import time\"\n\n# Verify no inline imports remain\ngrep -n \"import json\" vision.py  # Should only show line ~6\n\n# Verify try/except exists for json.loads\ngrep -A2 \"json.loads\" vision.py | grep -c \"except\"  # Should be 2\n```\n\n### 6. Test Retry Behavior\n\n```bash\npython -c \"\nfrom unittest.mock import patch, MagicMock, call\nimport json\n\n# Track call count\ncall_count = 0\n\ndef failing_create(*args, **kwargs):\n    global call_count\n    call_count += 1\n    mock = MagicMock()\n    mock.content = [MagicMock(text='invalid json')]\n    return mock\n\nwith patch('anthropic.Anthropic') as mock_client:\n    mock_client.return_value.messages.create.side_effect = failing_create\n    \n    from vision import analyze_screen\n    result = analyze_screen('test.png', 'test')\n    \n    # Should have retried 3 times\n    assert call_count == 3, f'Expected 3 attempts, got {call_count}'\n    print(f'SUCCESS: Retry logic executed {call_count} attempts')\n\"\n```",
        "status": "done",
        "dependencies": [
          "6"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-13T09:06:02.607Z"
      },
      {
        "id": "34",
        "title": "Fix hardcoded ANDROID_HOME in posting_scheduler.py",
        "description": "Replace hardcoded ANDROID_HOME paths at lines 17-19 and 193-197 in posting_scheduler.py with centralized config.py imports, using setup_environment() for early initialization and Config.ANDROID_SDK_PATH in get_android_env().",
        "details": "## Problem Statement\n\nThe `posting_scheduler.py` module has hardcoded ANDROID_HOME paths in two locations that should use the centralized `config.py`:\n\n**Location 1: Lines 17-19 (module-level initialization)**\n```python\n# Current hardcoded:\nos.environ['ANDROID_HOME'] = r'C:\\Users\\asus\\Downloads\\android-sdk'\nos.environ['ANDROID_SDK_ROOT'] = r'C:\\Users\\asus\\Downloads\\android-sdk'\n```\n\n**Location 2: Lines 193-197 (get_android_env function)**\n```python\n# Current hardcoded:\nandroid_sdk = r'C:\\Users\\asus\\Downloads\\android-sdk'\nenv['ANDROID_HOME'] = android_sdk\nenv['ANDROID_SDK_ROOT'] = android_sdk\n```\n\n## Implementation Steps\n\n### Step 1: Add import at the top of posting_scheduler.py\n\nAfter `import sys` (line 15), add:\n```python\nfrom config import Config, setup_environment\n```\n\n### Step 2: Replace lines 17-19 with setup_environment() call\n\nRemove:\n```python\nos.environ['ANDROID_HOME'] = r'C:\\Users\\asus\\Downloads\\android-sdk'\nos.environ['ANDROID_SDK_ROOT'] = r'C:\\Users\\asus\\Downloads\\android-sdk'\n```\n\nReplace with:\n```python\n# Set ANDROID_HOME early for Appium - MUST be before any Appium imports\nsetup_environment()\n```\n\n### Step 3: Update get_android_env() function (lines 185-204)\n\nReplace the hardcoded path with Config.ANDROID_SDK_PATH:\n```python\ndef get_android_env() -> dict:\n    \"\"\"Get environment with ANDROID_HOME/ANDROID_SDK_ROOT properly set.\n\n    This ensures Appium can find the Android SDK regardless of how\n    the parent process was started.\n    \"\"\"\n    env = os.environ.copy()\n\n    # Use centralized config for Android SDK path\n    android_sdk = Config.ANDROID_SDK_PATH\n\n    env['ANDROID_HOME'] = android_sdk\n    env['ANDROID_SDK_ROOT'] = android_sdk\n\n    # Add platform-tools to PATH if not already there\n    platform_tools = os.path.join(android_sdk, 'platform-tools')\n    if platform_tools not in env.get('PATH', ''):\n        env['PATH'] = platform_tools + os.pathsep + env.get('PATH', '')\n\n    return env\n```\n\n### Alternative: Use get_adb_env() directly\n\nNote: `config.py` already provides `get_adb_env()` which does the same thing as `get_android_env()`. Consider whether to:\n1. Keep `get_android_env()` but use Config.ANDROID_SDK_PATH (recommended for minimal change)\n2. Replace calls to `get_android_env()` with `get_adb_env()` from config (more DRY but larger change)\n\nOption 1 is recommended for this task to minimize scope and risk.\n\n## Files Modified\n\n- `posting_scheduler.py` - lines 15-19 and 185-204\n\n## Dependencies on This Change\n\nThis aligns with Task 32 (ADB path centralization) and Task 16 (Appium SDK detection), ensuring all Android SDK references flow through config.py.",
        "testStrategy": "## Test Strategy\n\n### 1. Import verification\n```bash\npython -c \"from posting_scheduler import get_android_env; from config import Config; env = get_android_env(); assert env['ANDROID_HOME'] == Config.ANDROID_SDK_PATH, f'Mismatch: {env[\\\"ANDROID_HOME\\\"]} != {Config.ANDROID_SDK_PATH}'; print(f'SUCCESS: ANDROID_HOME = {env[\\\"ANDROID_HOME\\\"]}')\"\n```\n\nExpected output: `SUCCESS: ANDROID_HOME = C:\\Users\\asus\\Downloads\\android-sdk`\n\n### 2. Environment variable test\n```bash\npython -c \"\nimport os\n# Clear any existing values\nos.environ.pop('ANDROID_HOME', None)\nos.environ.pop('ANDROID_SDK_ROOT', None)\n\n# Import should trigger setup_environment()\nimport posting_scheduler\n\n# Verify environment was set\nfrom config import Config\nassert os.environ.get('ANDROID_HOME') == Config.ANDROID_SDK_PATH, 'ANDROID_HOME not set'\nassert os.environ.get('ANDROID_SDK_ROOT') == Config.ANDROID_SDK_PATH, 'ANDROID_SDK_ROOT not set'\nprint('SUCCESS: Environment variables set correctly on import')\n\"\n```\n\n### 3. No hardcoded paths remaining\n```bash\n# Verify no hardcoded android-sdk paths remain in posting_scheduler.py\ngrep -n \"android-sdk\" posting_scheduler.py\n# Expected: No matches or only matches in comments\n```\n\n### 4. Functional test - Appium startup\n```bash\n# Test that Appium can still find Android SDK after the change\npython -c \"\nfrom posting_scheduler import get_android_env, restart_appium\nenv = get_android_env()\nprint(f'ANDROID_HOME: {env.get(\\\"ANDROID_HOME\\\")}')\nprint(f'ANDROID_SDK_ROOT: {env.get(\\\"ANDROID_SDK_ROOT\\\")}')\nprint(f'PATH includes platform-tools: {\\\"platform-tools\\\" in env.get(\\\"PATH\\\", \\\"\\\")}')\n\"\n```\n\n### 5. Full scheduler status test\n```bash\npython posting_scheduler.py --status\n# Should work without errors, showing Appium health status\n```\n\n### 6. Config consistency check\n```bash\npython -c \"\nfrom config import Config, get_adb_env\nfrom posting_scheduler import get_android_env\n\nconfig_env = get_adb_env()\nsched_env = get_android_env()\n\nassert config_env['ANDROID_HOME'] == sched_env['ANDROID_HOME'], 'ANDROID_HOME mismatch'\nassert config_env['ANDROID_SDK_ROOT'] == sched_env['ANDROID_SDK_ROOT'], 'ANDROID_SDK_ROOT mismatch'\nprint('SUCCESS: Both modules use consistent Android SDK path')\n\"\n```",
        "status": "done",
        "dependencies": [
          "16",
          "32"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-13T09:06:54.435Z"
      },
      {
        "id": "35",
        "title": "Move timedelta import to top of progress_tracker.py per PEP8",
        "description": "Move the `from datetime import timedelta` import statement from inside the `update_job_status()` method (line 632) to the module-level imports at the top of progress_tracker.py, alongside the existing `from datetime import datetime` import on line 40.",
        "details": "## Problem Statement\n\nThe `progress_tracker.py` module has an import statement inside a function, violating PEP8 style guidelines:\n\n**Location: Line 632 (inside `update_job_status()` method)**\n```python\n# Lines 629-633 in update_job_status():\nelse:\n    # Retryable - set to retrying with delay\n    job['status'] = self.STATUS_RETRYING\n    from datetime import timedelta  # <-- THIS SHOULD BE AT TOP\n    retry_at = datetime.now() + timedelta(minutes=retry_delay_minutes)\n```\n\n## Existing Import (Line 40)\n```python\nfrom datetime import datetime\n```\n\n## Implementation Steps\n\n### Step 1: Modify the existing datetime import at line 40\nChange:\n```python\nfrom datetime import datetime\n```\nTo:\n```python\nfrom datetime import datetime, timedelta\n```\n\n### Step 2: Remove the inline import at line 632\nDelete the entire line:\n```python\nfrom datetime import timedelta\n```\n\nThe surrounding code (lines 629-635) should become:\n```python\nelse:\n    # Retryable - set to retrying with delay\n    job['status'] = self.STATUS_RETRYING\n    retry_at = datetime.now() + timedelta(minutes=retry_delay_minutes)\n    job['retry_at'] = retry_at.isoformat()\n```\n\n## Why This Matters\n\n1. **PEP8 Compliance**: All imports should be at the top of the module\n2. **Performance**: While Python caches imports, having them at module level makes the import cost explicit at load time rather than hidden in function execution\n3. **Readability**: Developers can see all dependencies at the top of the file\n4. **Consistency**: The module already imports `datetime` from the `datetime` module - adding `timedelta` to the same import follows a clean pattern",
        "testStrategy": "## Test Strategy\n\n### 1. Syntax and Import Verification\n```bash\n# Verify the file has no syntax errors and imports correctly\npython -c \"import progress_tracker; print('Import successful')\"\n```\n\n### 2. Verify timedelta is in module-level imports\n```bash\n# Check that timedelta is imported at module level\npython -c \"from progress_tracker import ProgressTracker; import progress_tracker; print('timedelta' in dir(progress_tracker))\"\n```\n\n### 3. Verify no inline import remains\n```bash\n# Search for any remaining inline imports of timedelta\ngrep -n \"from datetime import timedelta\" progress_tracker.py\n# Should return NO results after the fix\n```\n\n### 4. Functional Test - Retry Logic\n```bash\n# Test that the retry functionality still works correctly\npython -c \"\nfrom progress_tracker import ProgressTracker\nimport os\nimport tempfile\n\n# Create a temp progress file\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n    test_file = f.name\n\ntracker = ProgressTracker(test_file)\n\n# Seed a test job\ntracker.seed_from_jobs([{\n    'job_id': 'test123',\n    'account': 'test_account',\n    'video_path': '/fake/path.mp4',\n    'caption': 'Test caption'\n}])\n\n# Claim the job\njob = tracker.claim_next_job(worker_id=0)\nassert job is not None, 'Failed to claim job'\nassert job['job_id'] == 'test123', 'Wrong job claimed'\n\n# Fail the job - this triggers the timedelta usage for retry scheduling\ntracker.update_job_status('test123', 'failed', worker_id=0, error='Test error', retry_delay_minutes=5)\n\n# Verify job is in retrying status with retry_at set\njobs = tracker._read_all_jobs()\nassert len(jobs) == 1, f'Expected 1 job, got {len(jobs)}'\nassert jobs[0]['status'] == 'retrying', f'Expected retrying status, got {jobs[0][\\\"status\\\"]}'\nassert jobs[0]['retry_at'], 'retry_at should be set'\n\nprint('SUCCESS: Retry logic works correctly with moved import')\n\n# Cleanup\nos.remove(test_file)\nif os.path.exists(test_file + '.lock'):\n    os.remove(test_file + '.lock')\n\"\n```\n\n### 5. Verify PEP8 compliance\n```bash\n# Run flake8 or pycodestyle on the imports section\npython -m py_compile progress_tracker.py && echo \"Compilation successful\"\n```\n\n### 6. Line 40 structure verification\n```bash\n# Confirm the new import structure at line 40\npython -c \"\nwith open('progress_tracker.py', 'r') as f:\n    lines = f.readlines()\n    # Check line 40 (0-indexed: line 39)\n    import_line = lines[39]\n    assert 'from datetime import datetime, timedelta' in import_line, f'Expected combined import, got: {import_line}'\n    print(f'Line 40: {import_line.strip()}')\n    print('SUCCESS: Import structure correct')\n\"\n```",
        "status": "done",
        "dependencies": [],
        "priority": "low",
        "subtasks": [],
        "updatedAt": "2025-12-13T10:45:17.394Z"
      },
      {
        "id": "36",
        "title": "Fix hardcoded ADB paths in utility scripts",
        "description": "Replace hardcoded ADB_PATH constants in 8 utility scripts with imports from config.py, using Config.ADB_PATH for the ADB executable and setup_environment() for ANDROID_HOME initialization where needed.",
        "details": "## Problem Statement\n\nEight utility scripts have hardcoded ADB paths that differ from the centralized `config.py`:\n\n**Hardcoded path in utility scripts:**\n```python\nADB_PATH = r\"C:\\Users\\asus\\Downloads\\platform-tools-latest-windows\\platform-tools\\adb.exe\"\n```\n\n**Centralized config.py path (lines 35-38):**\n```python\nANDROID_SDK_PATH: str = r\"C:\\Users\\asus\\Downloads\\android-sdk\"\nADB_PATH: str = os.path.join(ANDROID_SDK_PATH, \"platform-tools\", \"adb.exe\")\n```\n\nThe hardcoded path points to a different location than the centralized config, which could cause issues if the ADB location changes.\n\n## Files to Update\n\n| File | ADB_PATH Line | ANDROID_HOME Line | Changes Needed |\n|------|---------------|-------------------|----------------|\n| debug_page_source.py | 12 | 6 | Replace both |\n| fix_adbkeyboard.py | 18 | N/A | Replace ADB_PATH only |\n| diagnose_adbkeyboard.py | 10 | N/A | Replace ADB_PATH only |\n| setup_adbkeyboard.py | 17 | N/A | Replace ADB_PATH only |\n| reprovision_phone.py | 21 | N/A | Replace ADB_PATH only |\n| setup_clipboard_helper.py | 17 | N/A | Replace ADB_PATH only |\n| test_full_flow_android15.py | 70 | 6 | Replace both |\n| test_typing.py | 17 | N/A | Replace ADB_PATH only |\n\n## Implementation Steps\n\n### 1. Files with both ANDROID_HOME and ADB_PATH (2 files)\n\n**debug_page_source.py:**\n```python\n# BEFORE (lines 5-12):\nimport os\nos.environ['ANDROID_HOME'] = r'C:\\Users\\asus\\Downloads\\android-sdk'\n...\nADB_PATH = r\"C:\\Users\\asus\\Downloads\\platform-tools-latest-windows\\platform-tools\\adb.exe\"\n\n# AFTER:\nimport os\nfrom config import Config, setup_environment\nsetup_environment()\n...\n# Remove ADB_PATH constant, use Config.ADB_PATH directly in subprocess calls\n```\n\n**test_full_flow_android15.py:**\n```python\n# BEFORE (lines 5-6, 70):\nimport os\nos.environ['ANDROID_HOME'] = r'C:\\Users\\asus\\Downloads\\android-sdk'\n...\nADB = r\"C:\\Users\\asus\\Downloads\\platform-tools-latest-windows\\platform-tools\\adb.exe\"\n\n# AFTER:\nimport os\nfrom config import Config, setup_environment\nsetup_environment()\n...\n# Replace ADB variable with Config.ADB_PATH\n```\n\n### 2. Files with ADB_PATH only (6 files)\n\nFor each file, add the import and replace the constant:\n\n```python\n# BEFORE:\nADB_PATH = r\"C:\\Users\\asus\\Downloads\\platform-tools-latest-windows\\platform-tools\\adb.exe\"\n\n# AFTER:\nfrom config import Config\nADB_PATH = Config.ADB_PATH  # Or use Config.ADB_PATH directly\n```\n\nThe files and their specific changes:\n\n**fix_adbkeyboard.py (line 18):**\n- Add `from config import Config` after line 16 (after geelark_client import)\n- Replace line 18 with `ADB_PATH = Config.ADB_PATH`\n\n**diagnose_adbkeyboard.py (line 10):**\n- Add `from config import Config` after line 8 (after geelark_client import)\n- Replace line 10 with `ADB_PATH = Config.ADB_PATH`\n\n**setup_adbkeyboard.py (line 17):**\n- Add `from config import Config` after line 15 (after geelark_client import)\n- Replace line 17 with `ADB_PATH = Config.ADB_PATH`\n\n**reprovision_phone.py (line 21):**\n- Add `from config import Config` after line 19 (after geelark_client import)\n- Replace line 21 with `ADB_PATH = Config.ADB_PATH`\n\n**setup_clipboard_helper.py (line 17):**\n- Add `from config import Config` after line 15 (after geelark_client import)\n- Replace line 17 with `ADB_PATH = Config.ADB_PATH`\n\n**test_typing.py (line 17):**\n- Add `from config import Config` after line 15 (after geelark_client import)\n- Replace line 17 with `ADB_PATH = Config.ADB_PATH`\n\n## Alternative Approach: Direct Config.ADB_PATH Usage\n\nInstead of aliasing `ADB_PATH = Config.ADB_PATH`, you could use `Config.ADB_PATH` directly in all subprocess calls. This is more explicit but requires more changes:\n\n```python\n# Instead of:\nsubprocess.run([ADB_PATH, \"-s\", device, \"shell\", cmd], ...)\n\n# Use:\nsubprocess.run([Config.ADB_PATH, \"-s\", device, \"shell\", cmd], ...)\n```\n\nThe alias approach (`ADB_PATH = Config.ADB_PATH`) minimizes code changes while still achieving centralization.\n\n## Notes\n\n- The hardcoded path (`platform-tools-latest-windows`) differs from config.py's path (`android-sdk/platform-tools`), indicating these files may have been using a different ADB installation\n- After this change, all ADB operations will use the same ADB binary as the rest of the codebase\n- The `setup_environment()` function should be called early (before Appium imports) in files that need ANDROID_HOME set",
        "testStrategy": "## Test Strategy\n\n### 1. Import Verification for All Files\n```bash\n# Verify each file imports successfully after changes\npython -c \"import debug_page_source; print('debug_page_source OK')\"\npython -c \"import fix_adbkeyboard; print('fix_adbkeyboard OK')\"\npython -c \"import diagnose_adbkeyboard; print('diagnose_adbkeyboard OK')\"\npython -c \"import setup_adbkeyboard; print('setup_adbkeyboard OK')\"\npython -c \"import reprovision_phone; print('reprovision_phone OK')\"\npython -c \"import setup_clipboard_helper; print('setup_clipboard_helper OK')\"\npython -c \"import test_full_flow_android15; print('test_full_flow_android15 OK')\"\npython -c \"import test_typing; print('test_typing OK')\"\n```\n\n### 2. Verify ADB_PATH Resolution\n```bash\n# For files using ADB_PATH alias\npython -c \"\nfrom config import Config\nfrom fix_adbkeyboard import ADB_PATH\nassert ADB_PATH == Config.ADB_PATH, f'Mismatch: {ADB_PATH} != {Config.ADB_PATH}'\nprint(f'SUCCESS: ADB_PATH = {ADB_PATH}')\n\"\n\n# Repeat for other files with ADB_PATH\npython -c \"from diagnose_adbkeyboard import ADB_PATH; from config import Config; assert ADB_PATH == Config.ADB_PATH; print('diagnose_adbkeyboard OK')\"\npython -c \"from setup_adbkeyboard import ADB_PATH; from config import Config; assert ADB_PATH == Config.ADB_PATH; print('setup_adbkeyboard OK')\"\npython -c \"from reprovision_phone import ADB_PATH; from config import Config; assert ADB_PATH == Config.ADB_PATH; print('reprovision_phone OK')\"\npython -c \"from setup_clipboard_helper import ADB_PATH; from config import Config; assert ADB_PATH == Config.ADB_PATH; print('setup_clipboard_helper OK')\"\npython -c \"from test_typing import ADB_PATH; from config import Config; assert ADB_PATH == Config.ADB_PATH; print('test_typing OK')\"\n```\n\n### 3. Verify ANDROID_HOME Environment Variable\n```bash\n# For files that call setup_environment()\npython -c \"\nimport os\n# Clear any existing value\nif 'ANDROID_HOME' in os.environ:\n    del os.environ['ANDROID_HOME']\n\nfrom config import Config, setup_environment\nsetup_environment()\n\nassert os.environ.get('ANDROID_HOME') == Config.ANDROID_SDK_PATH, \\\n    f\\\"ANDROID_HOME mismatch: {os.environ.get('ANDROID_HOME')} != {Config.ANDROID_SDK_PATH}\\\"\nprint(f'SUCCESS: ANDROID_HOME = {os.environ[\\\"ANDROID_HOME\\\"]}')\n\"\n```\n\n### 4. Grep Verification - No Hardcoded Paths Remain\n```bash\n# Verify no hardcoded ADB paths remain in utility scripts\ngrep -l \"platform-tools-latest-windows\" debug_page_source.py fix_adbkeyboard.py diagnose_adbkeyboard.py setup_adbkeyboard.py reprovision_phone.py setup_clipboard_helper.py test_full_flow_android15.py test_typing.py\n\n# Expected: No output (no files contain the hardcoded path)\n```\n\n### 5. Functional Smoke Test\n```bash\n# Test that ADB commands still work (requires a connected device)\npython -c \"\nimport subprocess\nfrom config import Config\n\nresult = subprocess.run([Config.ADB_PATH, 'devices'], capture_output=True, text=True)\nprint('ADB devices output:')\nprint(result.stdout)\nassert result.returncode == 0, 'ADB command failed'\nprint('SUCCESS: ADB command executed successfully')\n\"\n```\n\n### 6. Optional: Run Utility Script Help/Usage\n```bash\n# Verify scripts don't crash on startup\npython fix_adbkeyboard.py --help 2>/dev/null || python fix_adbkeyboard.py 2>&1 | head -5\npython diagnose_adbkeyboard.py --help 2>/dev/null || python diagnose_adbkeyboard.py 2>&1 | head -5\npython setup_adbkeyboard.py --help 2>/dev/null || python setup_adbkeyboard.py 2>&1 | head -5\npython reprovision_phone.py --help 2>/dev/null || python reprovision_phone.py 2>&1 | head -5\npython setup_clipboard_helper.py --help 2>/dev/null || python setup_clipboard_helper.py 2>&1 | head -5\npython test_typing.py --help 2>/dev/null || python test_typing.py 2>&1 | head -5\n```",
        "status": "done",
        "dependencies": [
          "25",
          "32"
        ],
        "priority": "low",
        "subtasks": [],
        "updatedAt": "2025-12-13T10:47:24.337Z"
      },
      {
        "id": "37",
        "title": "Extract DeviceConnectionManager from SmartInstagramPoster",
        "description": "Create device_connection.py with a DeviceConnectionManager class that handles the device connection lifecycle, extracting the connect() method logic (~150 lines) from post_reel_smart.py that currently mixes Geelark API calls, ADB subprocess commands, and Appium connection.",
        "details": "## Current State Analysis\n\nThe `SmartInstagramPoster.connect()` method in `post_reel_smart.py` (lines 665-819, ~155 lines) currently handles:\n\n1. **Geelark API calls** (via GeelarkClient):\n   - `list_phones()` to find phone by name\n   - `start_phone()` to boot the phone if not running\n   - `get_phone_status()` to poll for boot completion\n   - `enable_adb()` with retry loop for API failures\n   - `get_adb_info()` to get IP/port/password\n\n2. **ADB subprocess commands** (via subprocess.run):\n   - `adb disconnect` to clean stale connections\n   - `adb connect` to establish connection\n   - `adb devices` polling to wait for device readiness\n   - `adb shell glogin` for Geelark authentication\n\n3. **Appium connection** (calls `connect_appium()` at the end)\n\n## Implementation Plan\n\n### 1. Create `device_connection.py` with DeviceConnectionManager class\n\n```python\n\"\"\"\nDevice Connection Manager - handles Geelark phone lifecycle and ADB connection.\n\nSeparates device connection concerns from Instagram posting logic.\n\"\"\"\nimport subprocess\nimport time\nimport logging\nfrom typing import Optional, Tuple\nfrom dataclasses import dataclass\n\nfrom config import Config\nfrom geelark_client import GeelarkClient\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass DeviceInfo:\n    \"\"\"Information about a connected device.\"\"\"\n    phone_id: str\n    phone_name: str\n    device_address: str  # ip:port\n    adb_password: str\n\nclass DeviceConnectionError(Exception):\n    \"\"\"Raised when device connection fails.\"\"\"\n    pass\n\nclass DeviceConnectionManager:\n    \"\"\"\n    Manages the lifecycle of connecting to a Geelark cloud phone.\n    \n    Responsibilities:\n    - Find phone by name via Geelark API\n    - Start phone if not running\n    - Enable ADB with retry logic\n    - Establish ADB connection\n    - Authenticate via glogin\n    \n    Usage:\n        manager = DeviceConnectionManager(phone_name)\n        device_info = manager.connect()\n        # ... use device_info.device_address for Appium ...\n        manager.disconnect()\n    \"\"\"\n    \n    ADB_PATH = Config.ADB_PATH\n    \n    def __init__(self, phone_name: str, client: GeelarkClient = None):\n        self.phone_name = phone_name\n        self.client = client or GeelarkClient()\n        self.device_info: Optional[DeviceInfo] = None\n        self._connected = False\n```\n\n### 2. Extract connection logic into methods\n\nThe DeviceConnectionManager should have these methods:\n\n- `connect() -> DeviceInfo`: Main entry point, orchestrates the full connection\n- `_find_phone() -> dict`: Find phone by name across multiple pages\n- `_ensure_phone_running(phone_id: str) -> None`: Start phone and wait for boot\n- `_enable_adb_with_retry(phone_id: str) -> dict`: Enable ADB with retry on API failures\n- `_establish_adb_connection(ip: str, port: int, password: str) -> str`: Connect ADB and run glogin\n- `_wait_for_adb_device(device_address: str, timeout: int) -> bool`: Poll until device appears in adb devices\n- `disconnect() -> None`: Clean up connection\n- `verify_connection() -> bool`: Check if ADB connection is still alive\n- `reconnect() -> bool`: Re-establish dropped connection\n\n### 3. Key implementation details from existing code\n\n**Phone lookup with pagination** (lines 669-678):\n```python\nfor page in range(1, 10):\n    result = self.client.list_phones(page=page, page_size=100)\n    for p in result[\"items\"]:\n        if p[\"serialName\"] == self.phone_name or p[\"id\"] == self.phone_name:\n            # found\n```\n\n**ADB enable retry loop** (lines 703-758):\n- Max 3 retries for enable_adb() API call\n- 30 attempts × 2 seconds for get_adb_info() verification\n- On failure, restart phone and retry the whole process\n\n**ADB connection with device readiness** (lines 773-793):\n- 30 attempts × 2 seconds = 60 seconds max wait\n- Check for `\\tdevice` in adb devices output (not `offline` or `unauthorized`)\n\n**glogin retry** (lines 796-814):\n- 3 attempts for glogin command\n- Check for \"success\" or absence of \"error\"\n\n### 4. Modify SmartInstagramPoster to use composition\n\n```python\nclass SmartInstagramPoster:\n    def __init__(self, phone_name, system_port=8200, appium_url=None):\n        self.connection_manager = DeviceConnectionManager(phone_name)\n        # ... rest of init ...\n        \n    def connect(self):\n        \"\"\"Connect to device using DeviceConnectionManager.\"\"\"\n        device_info = self.connection_manager.connect()\n        self.phone_id = device_info.phone_id\n        self.device = device_info.device_address\n        self.connect_appium()\n        return True\n        \n    def cleanup(self):\n        \"\"\"Cleanup after posting.\"\"\"\n        # ... existing cleanup ...\n        self.connection_manager.disconnect()\n```\n\n### 5. Additional helper methods to extract\n\nAlso extract these related methods from SmartInstagramPoster:\n- `verify_adb_connection()` (lines 821-829) → `DeviceConnectionManager.verify_connection()`\n- `reconnect_adb()` (lines 831-863) → `DeviceConnectionManager.reconnect()`\n\n### 6. Configuration integration\n\nUse `Config.ADB_PATH` from centralized config (already done in post_reel_smart.py).\n\n### 7. Logging\n\nUse the module logger pattern consistent with other modules:\n```python\nlogger = logging.getLogger(__name__)\n```\n\n### 8. Error handling\n\nCreate specific exceptions:\n- `DeviceNotFoundError(DeviceConnectionError)`: Phone not found in Geelark\n- `ADBEnableError(DeviceConnectionError)`: Failed to enable ADB after retries\n- `ADBConnectionError(DeviceConnectionError)`: Failed to establish ADB connection",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Test - Module imports successfully\n```bash\npython -c \"from device_connection import DeviceConnectionManager, DeviceInfo, DeviceConnectionError; print('Import OK')\"\n```\n\n### 2. Unit Test - DeviceConnectionManager instantiation\n```bash\npython -c \"\nfrom device_connection import DeviceConnectionManager\nmanager = DeviceConnectionManager('test_phone')\nassert manager.phone_name == 'test_phone'\nassert manager.device_info is None\nassert manager._connected is False\nprint('Instantiation OK')\n\"\n```\n\n### 3. Integration Test - Full connection flow (requires running phone)\n```bash\n# Use a known test account from accounts.txt\npython -c \"\nfrom device_connection import DeviceConnectionManager\n\nmanager = DeviceConnectionManager('reelwisdompod_')\ntry:\n    device_info = manager.connect()\n    print(f'Connected to {device_info.device_address}')\n    assert manager.verify_connection(), 'Connection verification failed'\nfinally:\n    manager.disconnect()\nprint('Full flow OK')\n\"\n```\n\n### 4. Verify SmartInstagramPoster still works\n```bash\n# Test that the refactored SmartInstagramPoster works with DeviceConnectionManager\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\n\nposter = SmartInstagramPoster('reelwisdompod_')\n# Check composition is set up correctly\nassert hasattr(poster, 'connection_manager'), 'Missing connection_manager'\nprint('SmartInstagramPoster composition OK')\n\"\n```\n\n### 5. Verify parallel_worker.py still works\n```bash\n# Ensure the worker can still import and use SmartInstagramPoster\npython -c \"\nfrom parallel_worker import execute_posting_job\nprint('parallel_worker imports OK')\n\"\n```\n\n### 6. End-to-end posting test (optional - uses real account)\n```bash\n# Only run if willing to make a real post\npython posting_scheduler.py --add-folder chunk_01c --add-accounts reelwisdompod_ --run --limit 1\n```\n\n### 7. Verify error handling\n```bash\n# Test DeviceNotFoundError is raised for non-existent phone\npython -c \"\nfrom device_connection import DeviceConnectionManager, DeviceConnectionError\n\nmanager = DeviceConnectionManager('nonexistent_phone_xyz123')\ntry:\n    manager.connect()\n    print('ERROR: Should have raised exception')\nexcept DeviceConnectionError as e:\n    print(f'Correctly raised DeviceConnectionError: {e}')\nexcept Exception as e:\n    print(f'Wrong exception type: {type(e).__name__}: {e}')\n\"\n```\n\n### 8. ADB path uses centralized config\n```bash\npython -c \"\nfrom device_connection import DeviceConnectionManager\nfrom config import Config\nassert DeviceConnectionManager.ADB_PATH == Config.ADB_PATH, 'ADB_PATH mismatch'\nprint(f'ADB_PATH correctly uses Config: {Config.ADB_PATH}')\n\"\n```",
        "status": "done",
        "dependencies": [
          "25",
          "29",
          "31",
          "32"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-13T11:00:04.365Z"
      },
      {
        "id": "38",
        "title": "Extract ClaudeUIAnalyzer from SmartInstagramPoster",
        "description": "Create claude_analyzer.py with a ClaudeUIAnalyzer class that encapsulates all AI-based UI analysis logic, extracting the analyze_ui() method (~125 lines) from post_reel_smart.py that mixes prompt construction, Claude API calls, and JSON response parsing into a single class with a clean interface: analyze(elements, state) -> action_dict.",
        "details": "## Current State Analysis\n\nThe `SmartInstagramPoster.analyze_ui()` method in `post_reel_smart.py` (lines 539-663, ~125 lines) currently handles:\n\n1. **UI Element Formatting** (lines 543-554):\n   - Iterates through parsed UI elements\n   - Builds a text description with bounds, center coords, text, desc, id, clickable status\n   - Creates `ui_description` string for Claude\n\n2. **Prompt Construction** (lines 556-612):\n   - Builds a large multi-section prompt including:\n     - Current posting state (video_uploaded, caption_entered, share_clicked)\n     - Caption to post\n     - UI element descriptions\n     - Instagram posting flow instructions (8 steps)\n     - JSON response format specification\n     - Critical rules for action handling (~20 rules)\n\n3. **Claude API Calls with Retry** (lines 615-663):\n   - 3-attempt retry loop for transient errors\n   - Uses `anthropic.Anthropic()` client\n   - Model: `claude-sonnet-4-20250514`, max_tokens: 500\n   - Handles empty responses\n   - Parses markdown code blocks (```json)\n   - JSON parsing with error handling\n\n## Implementation Plan\n\n### Step 1: Create claude_analyzer.py module\n\n```python\n\"\"\"\nClaude UI Analyzer - AI-based UI analysis for Instagram posting automation.\n\nExtracts UI analysis logic from SmartInstagramPoster for better separation of concerns.\n\"\"\"\n\nimport json\nimport time\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport anthropic\n\n\n@dataclass\nclass PostingState:\n    \"\"\"Current state of the Instagram posting flow.\"\"\"\n    video_uploaded: bool = False\n    caption_entered: bool = False\n    share_clicked: bool = False\n    caption: str = \"\"\n\n\n@dataclass\nclass UIAction:\n    \"\"\"Parsed action from Claude's analysis.\"\"\"\n    action: str  # tap, tap_and_type, back, scroll_down, scroll_up, home, open_instagram, done\n    element_index: Optional[int] = None\n    text: Optional[str] = None\n    reason: str = \"\"\n    video_selected: bool = False\n    caption_entered: bool = False\n    share_clicked: bool = False\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"UIAction\":\n        \"\"\"Create UIAction from Claude's JSON response.\"\"\"\n        return cls(\n            action=data.get(\"action\", \"\"),\n            element_index=data.get(\"element_index\"),\n            text=data.get(\"text\"),\n            reason=data.get(\"reason\", \"\"),\n            video_selected=data.get(\"video_selected\", False),\n            caption_entered=data.get(\"caption_entered\", False),\n            share_clicked=data.get(\"share_clicked\", False),\n        )\n\n\nclass ClaudeUIAnalyzer:\n    \"\"\"Handles AI-based UI analysis for Instagram posting automation.\"\"\"\n\n    # Default model configuration\n    DEFAULT_MODEL = \"claude-sonnet-4-20250514\"\n    DEFAULT_MAX_TOKENS = 500\n    DEFAULT_RETRIES = 3\n\n    def __init__(\n        self,\n        model: str = DEFAULT_MODEL,\n        max_tokens: int = DEFAULT_MAX_TOKENS,\n        retries: int = DEFAULT_RETRIES,\n    ):\n        self.client = anthropic.Anthropic()\n        self.model = model\n        self.max_tokens = max_tokens\n        self.retries = retries\n\n    def analyze(\n        self,\n        elements: list[dict],\n        state: PostingState,\n    ) -> UIAction:\n        \"\"\"\n        Analyze UI elements and determine next action.\n\n        Args:\n            elements: List of UI element dicts with keys:\n                - text, desc, id, bounds, center, clickable\n            state: Current posting flow state\n\n        Returns:\n            UIAction with the next action to take\n\n        Raises:\n            ValueError: If Claude returns invalid/unparseable response after retries\n        \"\"\"\n        ui_description = self._format_elements(elements)\n        prompt = self._build_prompt(ui_description, state)\n        response_json = self._call_claude(prompt)\n        return UIAction.from_dict(response_json)\n\n    def _format_elements(self, elements: list[dict]) -> str:\n        \"\"\"Format UI elements into a description string for Claude.\"\"\"\n        lines = [\"Current UI elements:\"]\n        for i, elem in enumerate(elements):\n            parts = []\n            if elem.get(\"text\"):\n                parts.append(f'text=\"{elem[\"text\"]}\"')\n            if elem.get(\"desc\"):\n                parts.append(f'desc=\"{elem[\"desc\"]}\"')\n            if elem.get(\"id\"):\n                parts.append(f\"id={elem['id']}\")\n            if elem.get(\"clickable\"):\n                parts.append(\"CLICKABLE\")\n            lines.append(\n                f\"{i}. {elem.get('bounds', '')} center={elem.get('center', '')} | {' | '.join(parts)}\"\n            )\n        return \"\\n\".join(lines)\n\n    def _build_prompt(self, ui_description: str, state: PostingState) -> str:\n        \"\"\"Build the prompt for Claude analysis.\"\"\"\n        # Full prompt extracted from post_reel_smart.py lines 556-612\n        return f\"\"\"You are controlling an Android phone to post a Reel to Instagram.\n\nCurrent state:\n- Video uploaded to phone: {state.video_uploaded}\n- Caption entered: {state.caption_entered}\n- Share button clicked: {state.share_clicked}\n- Caption to post: \"{state.caption}\"\n\n{ui_description}\n\nBased on the UI elements, decide the next action to take.\n\nInstagram posting flow:\n1. Find and tap Create/+ button. IMPORTANT: On different Instagram versions:\n   - Some have \"Create\" in bottom nav bar\n   - Some have \"Create New\" in top left corner (only visible from Profile tab)\n   - If you don't see Create, tap \"Profile\" tab first to find \"Create New\"\n2. Select \"Reel\" option if a menu appears\n3. Select the video from gallery (look for video thumbnails, usually most recent)\n4. Tap \"Next\" to proceed to editing\n5. Tap \"Next\" again to proceed to sharing\n6. When you see the caption field (\"Write a caption\" or similar), return \"type\" action with the caption text\n7. Tap \"Share\" to publish\n8. Done when you see confirmation, \"Sharing to Reels\", or back on feed\n\nRespond with JSON:\n{{\n    \"action\": \"tap\" | \"tap_and_type\" | \"back\" | \"scroll_down\" | \"scroll_up\" | \"home\" | \"open_instagram\" | \"done\",\n    \"element_index\": <index of element to tap>,\n    \"text\": \"<text to type if action is tap_and_type>\",\n    \"reason\": \"<brief explanation>\",\n    \"video_selected\": true/false,\n    \"caption_entered\": true/false,\n    \"share_clicked\": true/false\n}}\n\nCRITICAL RULES - NEVER GIVE UP:\n- NEVER return \"error\". There is no error action. Always try to recover.\n- If you see Play Store, Settings, or any non-Instagram app: return \"home\" to go back to home screen\n- If you see home screen or launcher: return \"open_instagram\" to reopen Instagram\n- If you see a popup, dialog, or unexpected screen: return \"back\" to dismiss it\n- If you're lost or confused: return \"back\" and try again\n- If you don't see Create button, tap Profile tab first\n- Look for \"Create New\" in desc field (top left area, small button)\n- Look for \"Profile\" in desc field (bottom nav, usually id=profile_tab)\n- If you see \"Reel\" or \"Create new reel\" option, tap it\n- If you see gallery thumbnails with video, tap the video\n- If you see \"Next\" button anywhere, tap it\n- IMPORTANT: When you see a caption field (text containing \"Write a caption\", \"Add a caption\", or similar placeholder) AND \"Caption entered\" is False, return action=\"tap_and_type\" with the element_index of the caption field and text set to the caption\n- CRITICAL: If \"Caption entered: True\" is shown above, DO NOT return tap_and_type! The caption is already typed. Just tap the Share button directly.\n- Allow/OK buttons should be tapped for permissions\n- IMPORTANT: Return \"done\" ONLY when Share button clicked is True AND you see \"Sharing to Reels\" confirmation\n- If Share button clicked is False but you see \"Sharing to Reels\", that's from a previous post - ignore it and start the posting flow\n- Set share_clicked=true when you tap the Share button\n- CRITICAL OK BUTTON RULE: After caption has been entered (Caption entered: True), if you see an \"OK\" button visible on screen (text='OK' or desc='OK'), you MUST tap the OK button FIRST before tapping Next or Share. This OK button dismisses the keyboard or a dialog and must be tapped for Next/Share to work properly.\n\nOnly output JSON.\"\"\"\n\n    def _call_claude(self, prompt: str) -> dict:\n        \"\"\"Call Claude API with retry logic and parse JSON response.\"\"\"\n        for attempt in range(self.retries):\n            try:\n                response = self.client.messages.create(\n                    model=self.model,\n                    max_tokens=self.max_tokens,\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                )\n\n                # Check for empty response\n                if not response.content:\n                    if attempt < self.retries - 1:\n                        time.sleep(1)\n                        continue\n                    raise ValueError(\"Claude returned empty response\")\n\n                text = response.content[0].text.strip()\n\n                # Check for empty text\n                if not text:\n                    if attempt < self.retries - 1:\n                        time.sleep(1)\n                        continue\n                    raise ValueError(\"Claude returned empty text\")\n\n                return self._parse_json_response(text, attempt)\n\n            except json.JSONDecodeError as e:\n                if attempt < self.retries - 1:\n                    time.sleep(1)\n                    continue\n                raise ValueError(f\"JSON parse failed after {self.retries} attempts: {e}\")\n\n            except Exception as e:\n                if attempt < self.retries - 1 and \"rate\" not in str(e).lower():\n                    time.sleep(1)\n                    continue\n                raise\n\n        raise ValueError(f\"Failed to get valid response from Claude after {self.retries} attempts\")\n\n    def _parse_json_response(self, text: str, attempt: int) -> dict:\n        \"\"\"Parse JSON from Claude's response, handling markdown code blocks.\"\"\"\n        # Handle markdown code blocks\n        if text.startswith(\"```\"):\n            text = text.split(\"```\")[1]\n            if text.startswith(\"json\"):\n                text = text[4:]\n            text = text.strip()\n\n        try:\n            return json.loads(text)\n        except json.JSONDecodeError as e:\n            print(f\"  [JSON PARSE ERROR] attempt {attempt+1}: {e}\")\n            print(f\"  Raw response (full): {text}\")\n            raise\n```\n\n### Step 2: Update SmartInstagramPoster to use ClaudeUIAnalyzer\n\n```python\n# In post_reel_smart.py\n\nfrom claude_analyzer import ClaudeUIAnalyzer, PostingState\n\nclass SmartInstagramPoster:\n    def __init__(self, phone_name, system_port=8200, appium_url=None):\n        self.client = GeelarkClient()\n        self.ui_analyzer = ClaudeUIAnalyzer()  # Replace self.anthropic\n        # ... rest of __init__\n\n    def analyze_ui(self, elements, caption):\n        \"\"\"Use Claude to analyze UI and decide next action\"\"\"\n        state = PostingState(\n            video_uploaded=self.video_uploaded,\n            caption_entered=self.caption_entered,\n            share_clicked=self.share_clicked,\n            caption=caption,\n        )\n        action = self.ui_analyzer.analyze(elements, state)\n        return {\n            \"action\": action.action,\n            \"element_index\": action.element_index,\n            \"text\": action.text,\n            \"reason\": action.reason,\n            \"video_selected\": action.video_selected,\n            \"caption_entered\": action.caption_entered,\n            \"share_clicked\": action.share_clicked,\n        }\n```\n\n## Key Design Decisions\n\n1. **Dataclasses for State and Actions**: Use `PostingState` and `UIAction` dataclasses for type safety and clear interfaces.\n\n2. **Configurable Model/Tokens**: Allow customization of Claude model and token limits via constructor.\n\n3. **Clean analyze() Interface**: Single entry point that takes elements and state, returns action.\n\n4. **Backwards Compatibility**: The `analyze_ui()` method in SmartInstagramPoster delegates to ClaudeUIAnalyzer but returns the same dict format for minimal changes to calling code.\n\n5. **Separation of Concerns**:\n   - `_format_elements()`: UI element → text conversion\n   - `_build_prompt()`: Prompt construction\n   - `_call_claude()`: API call with retries\n   - `_parse_json_response()`: JSON parsing\n\n## Relationship to Existing vision.py\n\nThe existing `vision.py` module handles screenshot-based (image) analysis, while this new `claude_analyzer.py` handles UI hierarchy (XML dump) analysis. They serve complementary purposes:\n- `vision.py`: Image → action (pixel-coordinate based)\n- `claude_analyzer.py`: UI elements → action (element-index based)",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Test - Module imports successfully\n```bash\npython -c \"from claude_analyzer import ClaudeUIAnalyzer, PostingState, UIAction; print('Import OK')\"\n```\n\n### 2. Unit Test - ClaudeUIAnalyzer instantiation\n```bash\npython -c \"\nfrom claude_analyzer import ClaudeUIAnalyzer\nanalyzer = ClaudeUIAnalyzer()\nassert analyzer.model == 'claude-sonnet-4-20250514'\nassert analyzer.max_tokens == 500\nassert analyzer.retries == 3\nprint('Instantiation OK')\n\"\n```\n\n### 3. Unit Test - PostingState dataclass\n```bash\npython -c \"\nfrom claude_analyzer import PostingState\nstate = PostingState(video_uploaded=True, caption='Test', caption_entered=False, share_clicked=False)\nassert state.video_uploaded == True\nassert state.caption == 'Test'\nprint('PostingState OK')\n\"\n```\n\n### 4. Unit Test - UIAction.from_dict() parsing\n```bash\npython -c \"\nfrom claude_analyzer import UIAction\ndata = {\n    'action': 'tap',\n    'element_index': 5,\n    'reason': 'Tap Create button',\n    'video_selected': False,\n    'caption_entered': False,\n    'share_clicked': False\n}\naction = UIAction.from_dict(data)\nassert action.action == 'tap'\nassert action.element_index == 5\nassert action.reason == 'Tap Create button'\nprint('UIAction.from_dict OK')\n\"\n```\n\n### 5. Unit Test - _format_elements() output format\n```bash\npython -c \"\nfrom claude_analyzer import ClaudeUIAnalyzer\nanalyzer = ClaudeUIAnalyzer()\nelements = [\n    {'text': 'Home', 'desc': '', 'id': 'home_tab', 'bounds': '[0,0][100,100]', 'center': (50, 50), 'clickable': True},\n    {'text': '', 'desc': 'Create', 'id': 'create_btn', 'bounds': '[100,0][200,100]', 'center': (150, 50), 'clickable': True},\n]\nresult = analyzer._format_elements(elements)\nassert 'text=\\\"Home\\\"' in result\nassert 'desc=\\\"Create\\\"' in result\nassert 'CLICKABLE' in result\nassert 'center=' in result\nprint('_format_elements OK')\n\"\n```\n\n### 6. Unit Test - _parse_json_response() handles code blocks\n```bash\npython -c \"\nfrom claude_analyzer import ClaudeUIAnalyzer\nanalyzer = ClaudeUIAnalyzer()\n\n# Test plain JSON\nplain = '{\\\"action\\\": \\\"tap\\\", \\\"element_index\\\": 0}'\nresult = analyzer._parse_json_response(plain, 0)\nassert result['action'] == 'tap'\n\n# Test markdown code block\nmarkdown = '\\`\\`\\`json\\n{\\\"action\\\": \\\"back\\\"}\\n\\`\\`\\`'\nresult = analyzer._parse_json_response(markdown, 0)\nassert result['action'] == 'back'\nprint('_parse_json_response OK')\n\"\n```\n\n### 7. Unit Test - _build_prompt() includes all required sections\n```bash\npython -c \"\nfrom claude_analyzer import ClaudeUIAnalyzer, PostingState\nanalyzer = ClaudeUIAnalyzer()\nstate = PostingState(video_uploaded=True, caption='Test caption', caption_entered=False, share_clicked=False)\nprompt = analyzer._build_prompt('UI elements here', state)\nassert 'Video uploaded to phone: True' in prompt\nassert 'Caption entered: False' in prompt\nassert 'Test caption' in prompt\nassert 'Instagram posting flow:' in prompt\nassert 'CRITICAL RULES' in prompt\nassert 'Only output JSON' in prompt\nprint('_build_prompt OK')\n\"\n```\n\n### 8. Integration Test - SmartInstagramPoster uses ClaudeUIAnalyzer\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nposter = SmartInstagramPoster('test_phone')\nassert hasattr(poster, 'ui_analyzer'), 'SmartInstagramPoster should have ui_analyzer attribute'\nprint('Integration OK')\n\"\n```\n\n### 9. Integration Test - Full analyze() call (requires ANTHROPIC_API_KEY)\n```bash\npython -c \"\nimport os\nif not os.getenv('ANTHROPIC_API_KEY'):\n    print('SKIP: ANTHROPIC_API_KEY not set')\nelse:\n    from claude_analyzer import ClaudeUIAnalyzer, PostingState\n    analyzer = ClaudeUIAnalyzer()\n    elements = [\n        {'text': 'Home', 'desc': '', 'id': 'home_tab', 'bounds': '[0,1200][144,1280]', 'center': (72, 1240), 'clickable': True},\n        {'text': '', 'desc': 'Create', 'id': 'creation_tab', 'bounds': '[288,1200][432,1280]', 'center': (360, 1240), 'clickable': True},\n    ]\n    state = PostingState(video_uploaded=False, caption='Test', caption_entered=False, share_clicked=False)\n    action = analyzer.analyze(elements, state)\n    assert action.action in ['tap', 'tap_and_type', 'back', 'scroll_down', 'scroll_up', 'home', 'open_instagram', 'done']\n    print(f'Integration test passed: action={action.action}, reason={action.reason}')\n\"\n```\n\n### 10. Regression Test - Existing post_reel_smart.py behavior unchanged\n```bash\n# Run a quick dry test to ensure posting_scheduler still works\npython posting_scheduler.py --status\n```",
        "status": "done",
        "dependencies": [
          "6",
          "25",
          "37"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-13T11:02:05.391Z"
      },
      {
        "id": "39",
        "title": "Extract AppiumUIController from SmartInstagramPoster",
        "description": "Create appium_ui_controller.py with an AppiumUIController class that encapsulates all Appium-based UI interaction methods, extracting tap(), swipe(), press_key(), type_text_via_appium(), and dump_ui() from post_reel_smart.py to create a clean interface between posting logic and device control.",
        "details": "## Current State Analysis\n\nThe `SmartInstagramPoster` class in `post_reel_smart.py` contains several Appium-based UI interaction methods that should be extracted:\n\n### Methods to Extract (with line numbers):\n\n1. **`tap(x, y)`** (lines 91-97): Taps at coordinates using Appium driver\n   - Requires `self.appium_driver`\n   - Includes 1.5s sleep after tap\n\n2. **`swipe(x1, y1, x2, y2, duration_ms)`** (lines 99-103): Swipes between points\n   - Requires `self.appium_driver`\n\n3. **`press_key(keycode)`** (lines 105-117): Presses Android key codes\n   - Maps string keycodes ('KEYCODE_BACK') to integers\n   - Requires `self.appium_driver`\n\n4. **`type_text(text)`** (lines 441-473): Types text using Appium send_keys\n   - Finds EditText elements or uses active element\n   - Requires `self.appium_driver`\n\n5. **`dump_ui()`** (lines 475-537): Dumps UI hierarchy via Appium page_source\n   - Parses XML, extracts clickable elements with bounds\n   - Handles UiAutomator2 crash recovery via `reconnect_appium()`\n   - Returns tuple of (elements_list, raw_xml)\n\n### Supporting Methods to Also Extract:\n\n6. **`is_uiautomator2_crash(exception)`** (lines 69-77): Detects UiAutomator2 crash signatures\n7. **`reconnect_appium()`** (lines 79-89): Reconnects Appium after crash\n8. **`is_keyboard_visible()`** (lines 422-439): Checks keyboard visibility via ADB dumpsys\n\n## Implementation Plan\n\n### 1. Create `appium_ui_controller.py`\n\n```python\n\"\"\"\nAppium UI Controller - encapsulates all Appium-based UI interactions.\n\nThis module provides a clean interface for device UI control, separating\nposting logic from low-level Appium operations.\n\"\"\"\nimport re\nimport time\nimport xml.etree.ElementTree as ET\nfrom typing import List, Dict, Tuple, Optional, Union, Callable\nfrom dataclasses import dataclass\n\nfrom appium import webdriver\nfrom appium.webdriver.common.appiumby import AppiumBy\n\nfrom config import Config\n\n\n@dataclass\nclass UIElement:\n    \"\"\"Represents a parsed UI element from the hierarchy.\"\"\"\n    text: str\n    desc: str\n    resource_id: str\n    bounds: str\n    center: Tuple[int, int]\n    clickable: bool\n\n\nclass AppiumUIControllerError(Exception):\n    \"\"\"Base exception for AppiumUIController errors.\"\"\"\n    pass\n\n\nclass UIAutomator2CrashError(AppiumUIControllerError):\n    \"\"\"Raised when UiAutomator2 crashes on device.\"\"\"\n    pass\n\n\nclass AppiumUIController:\n    \"\"\"\n    Encapsulates all Appium-based UI interaction methods.\n    \n    This class provides a clean interface for device UI control operations,\n    handling Appium driver interactions, crash recovery, and UI hierarchy parsing.\n    \n    Usage:\n        driver = webdriver.Remote(...)\n        controller = AppiumUIController(driver)\n        \n        # Basic interactions\n        controller.tap(500, 500)\n        controller.swipe(100, 500, 100, 200)\n        controller.press_key('KEYCODE_BACK')\n        controller.type_text(\"Hello world\")\n        \n        # UI inspection\n        elements, xml = controller.dump_ui()\n    \"\"\"\n    \n    # Android keycode mapping\n    KEYCODES = {\n        'KEYCODE_BACK': 4,\n        'KEYCODE_HOME': 3,\n        'KEYCODE_ENTER': 66,\n        'KEYCODE_TAB': 61,\n        'KEYCODE_MENU': 82,\n    }\n    \n    def __init__(\n        self,\n        driver: webdriver.Remote,\n        adb_shell_func: Optional[Callable[[str], str]] = None,\n        reconnect_func: Optional[Callable[[], bool]] = None,\n        tap_delay: float = 1.5\n    ):\n        \"\"\"\n        Initialize AppiumUIController.\n        \n        Args:\n            driver: Appium WebDriver instance\n            adb_shell_func: Optional function to run ADB shell commands (for keyboard detection)\n            reconnect_func: Optional function to reconnect Appium after crash\n            tap_delay: Delay in seconds after tap (default 1.5)\n        \"\"\"\n        self._driver = driver\n        self._adb_shell = adb_shell_func\n        self._reconnect = reconnect_func\n        self._tap_delay = tap_delay\n    \n    @property\n    def driver(self) -> webdriver.Remote:\n        \"\"\"Get the underlying Appium driver.\"\"\"\n        return self._driver\n    \n    def set_driver(self, driver: webdriver.Remote) -> None:\n        \"\"\"Update the Appium driver (e.g., after reconnection).\"\"\"\n        self._driver = driver\n    \n    def _ensure_driver(self) -> None:\n        \"\"\"Ensure driver is connected, raise if not.\"\"\"\n        if not self._driver:\n            raise AppiumUIControllerError(\"Appium driver not connected\")\n    \n    def tap(self, x: int, y: int, delay: Optional[float] = None) -> None:\n        \"\"\"\n        Tap at coordinates.\n        \n        Args:\n            x: X coordinate\n            y: Y coordinate\n            delay: Optional custom delay after tap (uses tap_delay if not specified)\n        \"\"\"\n        self._ensure_driver()\n        self._driver.tap([(x, y)])\n        time.sleep(delay if delay is not None else self._tap_delay)\n    \n    def swipe(\n        self,\n        x1: int, y1: int,\n        x2: int, y2: int,\n        duration_ms: int = 300\n    ) -> None:\n        \"\"\"\n        Swipe from one point to another.\n        \n        Args:\n            x1, y1: Start coordinates\n            x2, y2: End coordinates\n            duration_ms: Swipe duration in milliseconds\n        \"\"\"\n        self._ensure_driver()\n        self._driver.swipe(x1, y1, x2, y2, duration_ms)\n    \n    def press_key(self, keycode: Union[int, str]) -> None:\n        \"\"\"\n        Press an Android key.\n        \n        Args:\n            keycode: Integer keycode or string like 'KEYCODE_BACK'\n        \"\"\"\n        self._ensure_driver()\n        if isinstance(keycode, str):\n            keycode = self.KEYCODES.get(keycode, 4)  # Default to BACK\n        self._driver.press_keycode(keycode)\n    \n    def type_text(self, text: str) -> bool:\n        \"\"\"\n        Type text into the currently focused field.\n        \n        Args:\n            text: Text to type (supports Unicode, emojis, newlines)\n            \n        Returns:\n            True if text was sent successfully, False otherwise\n        \"\"\"\n        self._ensure_driver()\n        \n        try:\n            # Find EditText elements\n            edit_texts = self._driver.find_elements(\n                AppiumBy.CLASS_NAME, \"android.widget.EditText\"\n            )\n            \n            for et in edit_texts:\n                if et.is_displayed():\n                    et.send_keys(text)\n                    time.sleep(0.8)\n                    return True\n            \n            # Fallback: try active element\n            active = self._driver.switch_to.active_element\n            if active:\n                active.send_keys(text)\n                time.sleep(0.8)\n                return True\n            \n            return False\n            \n        except Exception as e:\n            raise AppiumUIControllerError(f\"Typing failed: {e}\")\n    \n    def is_uiautomator2_crash(self, exception: Exception) -> bool:\n        \"\"\"Check if exception indicates UiAutomator2 crashed.\"\"\"\n        error_msg = str(exception).lower()\n        crash_indicators = [\n            'instrumentation process is not running',\n            'uiautomator2 server',\n            'cannot be proxied',\n            'probably crashed',\n        ]\n        return any(indicator in error_msg for indicator in crash_indicators)\n    \n    def is_keyboard_visible(self) -> bool:\n        \"\"\"\n        Check if the keyboard is currently visible.\n        \n        Requires adb_shell_func to be set.\n        \"\"\"\n        if not self._adb_shell:\n            return False  # Cannot determine without ADB\n        \n        # Method 1: Check dumpsys for keyboard visibility\n        result = self._adb_shell(\"dumpsys input_method | grep mInputShown\")\n        if \"mInputShown=true\" in result:\n            return True\n        \n        # Method 2: Check window visibility\n        result = self._adb_shell(\"dumpsys window | grep -i keyboard\")\n        if \"isVisible=true\" in result.lower() or \"mhasfocus=true\" in result.lower():\n            return True\n        \n        # Method 3: Check InputMethod window\n        result = self._adb_shell(\"dumpsys window windows | grep -E 'mCurrentFocus|mFocusedApp'\")\n        if \"InputMethod\" in result:\n            return True\n        \n        return False\n    \n    def dump_ui(self) -> Tuple[List[Dict], str]:\n        \"\"\"\n        Dump UI hierarchy and return parsed elements.\n        \n        Returns:\n            Tuple of (elements_list, raw_xml_string)\n            \n        Raises:\n            AppiumUIControllerError: If UI dump fails after recovery attempts\n        \"\"\"\n        self._ensure_driver()\n        \n        elements = []\n        xml_str = \"\"\n        \n        try:\n            xml_str = self._driver.page_source\n        except Exception as e:\n            if self.is_uiautomator2_crash(e):\n                # Try to recover\n                if self._reconnect and self._reconnect():\n                    try:\n                        xml_str = self._driver.page_source\n                    except Exception as e2:\n                        raise UIAutomator2CrashError(\n                            f\"Recovery failed: {type(e2).__name__}: {e2}\"\n                        )\n                else:\n                    raise UIAutomator2CrashError(\"Appium reconnect failed\")\n            else:\n                raise AppiumUIControllerError(\n                    f\"UI dump failed: {type(e).__name__}: {str(e)[:100]}\"\n                )\n        \n        if '<?xml' not in xml_str:\n            return elements, xml_str\n        \n        xml_clean = xml_str[xml_str.find('<?xml'):]\n        try:\n            root = ET.fromstring(xml_clean)\n            # Appium uses class names as tags, not <node>\n            for elem in root.iter():\n                text = elem.get('text', '')\n                desc = elem.get('content-desc', '')\n                res_id = elem.get('resource-id', '')\n                bounds = elem.get('bounds', '')\n                clickable = elem.get('clickable', 'false')\n                \n                if bounds and (text or desc or clickable == 'true'):\n                    m = re.match(r'\\[(\\d+),(\\d+)\\]\\[(\\d+),(\\d+)\\]', bounds)\n                    if m:\n                        x1, y1, x2, y2 = map(int, m.groups())\n                        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n                        elements.append({\n                            'text': text,\n                            'desc': desc,\n                            'id': res_id.split('/')[-1] if '/' in res_id else res_id,\n                            'bounds': bounds,\n                            'center': (cx, cy),\n                            'clickable': clickable == 'true'\n                        })\n        except ET.ParseError as e:\n            pass  # Return partial results\n        \n        return elements, xml_str\n```\n\n### 2. Update SmartInstagramPoster to Use AppiumUIController\n\nAfter creating the controller, update `post_reel_smart.py`:\n\n```python\nfrom appium_ui_controller import AppiumUIController, AppiumUIControllerError\n\nclass SmartInstagramPoster:\n    def __init__(self, phone_name, system_port=8200, appium_url=None):\n        # ... existing init code ...\n        self.ui_controller = None  # Will be set after Appium connects\n    \n    def connect_appium(self, retries=3):\n        # ... existing connection code ...\n        # After successful connection:\n        self.ui_controller = AppiumUIController(\n            driver=self.appium_driver,\n            adb_shell_func=self.adb,\n            reconnect_func=self._do_reconnect_appium,\n            tap_delay=1.5\n        )\n    \n    # Delegate methods to controller (thin wrappers for backward compatibility)\n    def tap(self, x, y):\n        print(f\"  [TAP] ({x}, {y})\")\n        self.ui_controller.tap(x, y)\n    \n    def swipe(self, x1, y1, x2, y2, duration_ms=300):\n        self.ui_controller.swipe(x1, y1, x2, y2, duration_ms)\n    \n    def press_key(self, keycode):\n        self.ui_controller.press_key(keycode)\n    \n    def type_text(self, text):\n        print(f\"    Typing via Appium ({len(text)} chars)...\")\n        result = self.ui_controller.type_text(text)\n        if result:\n            print(\"    Appium: text sent successfully\")\n        else:\n            print(\"    ERROR: No text field found to type into\")\n        return result\n    \n    def dump_ui(self):\n        return self.ui_controller.dump_ui()\n    \n    def is_keyboard_visible(self):\n        return self.ui_controller.is_keyboard_visible()\n```\n\n### 3. Integration with Task 37 (DeviceConnectionManager)\n\nThe `AppiumUIController` should receive the driver from `DeviceConnectionManager`. When Task 37 is implemented:\n\n```python\n# In SmartInstagramPoster after Task 37 integration\nconnection_manager = DeviceConnectionManager(geelark_client)\ndevice_info = connection_manager.connect(phone_name)\n\n# Create UI controller with the Appium driver\nself.ui_controller = AppiumUIController(\n    driver=device_info.appium_driver,\n    adb_shell_func=lambda cmd: connection_manager.adb_shell(cmd),\n    reconnect_func=lambda: connection_manager.reconnect_appium()\n)\n```\n\n### Key Design Decisions:\n\n1. **Constructor Injection**: The Appium driver is injected via constructor, not created internally\n2. **Optional ADB**: `adb_shell_func` is optional - keyboard detection gracefully degrades\n3. **Optional Recovery**: `reconnect_func` callback allows crash recovery without tight coupling\n4. **Backward Compatibility**: SmartInstagramPoster keeps thin wrapper methods for existing callers\n5. **Clean Interface**: All Appium operations go through the controller\n6. **Exception Hierarchy**: Custom exceptions for different failure modes",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Test - Module imports successfully\n```bash\npython -c \"from appium_ui_controller import AppiumUIController, AppiumUIControllerError, UIAutomator2CrashError; print('Import OK')\"\n```\n\n### 2. Unit Test - AppiumUIController instantiation\n```bash\npython -c \"\nfrom appium_ui_controller import AppiumUIController\n\n# Create with mock driver\nclass MockDriver:\n    def tap(self, coords): pass\n    def swipe(self, *args): pass\n    def press_keycode(self, code): pass\n    def find_elements(self, by, value): return []\n    @property\n    def page_source(self): return '<hierarchy></hierarchy>'\n\ncontroller = AppiumUIController(MockDriver())\nprint('Instantiation OK')\nprint(f'Driver set: {controller.driver is not None}')\n\"\n```\n\n### 3. Unit Test - tap() delegates correctly\n```bash\npython -c \"\nfrom appium_ui_controller import AppiumUIController\n\ntap_calls = []\n\nclass MockDriver:\n    def tap(self, coords):\n        tap_calls.append(coords)\n\ncontroller = AppiumUIController(MockDriver(), tap_delay=0)\ncontroller.tap(100, 200)\n\nassert tap_calls == [[(100, 200)]], f'Expected [[(100, 200)]], got {tap_calls}'\nprint('tap() delegation OK')\n\"\n```\n\n### 4. Unit Test - press_key() maps string keycodes\n```bash\npython -c \"\nfrom appium_ui_controller import AppiumUIController\n\npressed = []\n\nclass MockDriver:\n    def press_keycode(self, code):\n        pressed.append(code)\n\ncontroller = AppiumUIController(MockDriver())\ncontroller.press_key('KEYCODE_BACK')\ncontroller.press_key('KEYCODE_HOME')\ncontroller.press_key(66)  # Raw int\n\nassert pressed == [4, 3, 66], f'Expected [4, 3, 66], got {pressed}'\nprint('press_key() mapping OK')\n\"\n```\n\n### 5. Unit Test - dump_ui() parses XML correctly\n```bash\npython -c \"\nfrom appium_ui_controller import AppiumUIController\n\nclass MockDriver:\n    @property\n    def page_source(self):\n        return '''<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\n<hierarchy>\n  <android.widget.Button text=\\\"OK\\\" bounds=\\\"[10,20][100,80]\\\" clickable=\\\"true\\\" resource-id=\\\"com.app/btn\\\" content-desc=\\\"Confirm\\\" />\n</hierarchy>'''\n\ncontroller = AppiumUIController(MockDriver())\nelements, xml = controller.dump_ui()\n\nassert len(elements) == 1, f'Expected 1 element, got {len(elements)}'\nelem = elements[0]\nassert elem['text'] == 'OK', f'Expected text=OK, got {elem[\\\"text\\\"]}'\nassert elem['center'] == (55, 50), f'Expected center=(55,50), got {elem[\\\"center\\\"]}'\nassert elem['clickable'] == True, f'Expected clickable=True'\nprint('dump_ui() parsing OK')\n\"\n```\n\n### 6. Unit Test - is_uiautomator2_crash() detection\n```bash\npython -c \"\nfrom appium_ui_controller import AppiumUIController\n\ncontroller = AppiumUIController(None)\n\n# Should detect crash\ne1 = Exception('instrumentation process is not running')\nassert controller.is_uiautomator2_crash(e1) == True\n\ne2 = Exception('Original error: cannot be proxied')\nassert controller.is_uiautomator2_crash(e2) == True\n\n# Should not detect crash\ne3 = Exception('Connection timeout')\nassert controller.is_uiautomator2_crash(e3) == False\n\nprint('is_uiautomator2_crash() detection OK')\n\"\n```\n\n### 7. Unit Test - Error raised when no driver\n```bash\npython -c \"\nfrom appium_ui_controller import AppiumUIController, AppiumUIControllerError\n\ncontroller = AppiumUIController(None)\n\ntry:\n    controller.tap(100, 100)\n    print('ERROR: Should have raised exception')\n    exit(1)\nexcept AppiumUIControllerError as e:\n    assert 'not connected' in str(e).lower()\n    print('No-driver error handling OK')\n\"\n```\n\n### 8. Integration Test - SmartInstagramPoster uses AppiumUIController\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\n\nposter = SmartInstagramPoster('test_phone')\n# Before connect, ui_controller should be None\nprint(f'UI controller before connect: {getattr(poster, \\\"ui_controller\\\", \\\"NOT_ATTR\\\")}')\nprint('SmartInstagramPoster integration structure OK')\n\"\n```\n\n### 9. Integration Test - Full flow with mock Appium\n```bash\npython -c \"\nfrom appium_ui_controller import AppiumUIController\n\nactions = []\n\nclass MockDriver:\n    def tap(self, coords):\n        actions.append(('tap', coords))\n    def swipe(self, x1, y1, x2, y2, duration):\n        actions.append(('swipe', x1, y1, x2, y2))\n    def press_keycode(self, code):\n        actions.append(('key', code))\n    @property\n    def page_source(self):\n        return '<?xml version=\\\"1.0\\\"?><hierarchy><btn bounds=\\\"[0,0][100,100]\\\" clickable=\\\"true\\\"/></hierarchy>'\n\nadb_calls = []\ndef mock_adb(cmd):\n    adb_calls.append(cmd)\n    return 'mInputShown=true' if 'input_method' in cmd else ''\n\ncontroller = AppiumUIController(\n    MockDriver(),\n    adb_shell_func=mock_adb,\n    tap_delay=0\n)\n\n# Run sequence\ncontroller.tap(50, 50)\ncontroller.swipe(0, 100, 0, 0, 300)\ncontroller.press_key('KEYCODE_BACK')\nelements, _ = controller.dump_ui()\nkb_visible = controller.is_keyboard_visible()\n\nassert len(actions) == 3, f'Expected 3 actions, got {actions}'\nassert len(elements) == 1, f'Expected 1 element'\nassert kb_visible == True, 'Expected keyboard visible'\nprint('Full flow integration OK')\n\"\n```\n\n### 10. Live Test - With real Appium server (manual)\n```bash\n# Start Appium server first: appium --port 4723\n\npython -c \"\nfrom appium import webdriver\nfrom appium.options.android import UiAutomator2Options\nfrom appium_ui_controller import AppiumUIController\n\n# Connect to a test device (update device address)\noptions = UiAutomator2Options()\noptions.platform_name = 'Android'\noptions.automation_name = 'UiAutomator2'\noptions.device_name = '192.168.1.100:5555'  # Update this\noptions.no_reset = True\n\ntry:\n    driver = webdriver.Remote('http://127.0.0.1:4723', options=options)\n    controller = AppiumUIController(driver)\n    \n    # Test dump_ui\n    elements, xml = controller.dump_ui()\n    print(f'Found {len(elements)} UI elements')\n    \n    # Test tap (tap center of screen)\n    controller.tap(360, 640)\n    print('Tap executed')\n    \n    driver.quit()\n    print('Live test PASSED')\nexcept Exception as e:\n    print(f'Live test skipped or failed: {e}')\n\"\n```",
        "status": "done",
        "dependencies": [
          "25",
          "37",
          "23"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T11:06:16.759Z"
      },
      {
        "id": "40",
        "title": "Consolidate ADB operations into DeviceConnectionManager",
        "description": "Extract all ADB subprocess calls from SmartInstagramPoster and route them through DeviceConnectionManager, establishing a clear boundary where the posting engine never calls subprocess directly for ADB operations.",
        "details": "## Current State Analysis\n\n### SmartInstagramPoster.adb() - Lines 113-120 in post_reel_smart.py:\n```python\ndef adb(self, cmd, timeout=30):\n    \"\"\"Run ADB shell command\"\"\"\n    result = subprocess.run(\n        [ADB_PATH, \"-s\", self.device, \"shell\", cmd],\n        capture_output=True, timeout=timeout,\n        encoding='utf-8', errors='replace'\n    )\n    return result.stdout.strip() if result.stdout else \"\"\n```\n\n### DeviceConnectionManager.adb_command() - Lines 53-62 in device_connection.py:\n```python\ndef adb_command(self, cmd: str, timeout: int = 30) -> str:\n    \"\"\"Run ADB shell command on the connected device.\"\"\"\n    if not self.device:\n        raise Exception(\"No device connected - call connect() first\")\n    result = subprocess.run(\n        [ADB_PATH, \"-s\", self.device, \"shell\", cmd],\n        capture_output=True, timeout=timeout,\n        encoding='utf-8', errors='replace'\n    )\n    return result.stdout.strip() if result.stdout else \"\"\n```\n\nBoth methods are functionally identical. SmartInstagramPoster already has `self._conn` which is a DeviceConnectionManager instance.\n\n## Implementation Steps\n\n### Step 1: Update SmartInstagramPoster.adb() to delegate\nReplace lines 113-120 in `post_reel_smart.py`:\n```python\ndef adb(self, cmd, timeout=30):\n    \"\"\"Run ADB shell command - delegates to DeviceConnectionManager\"\"\"\n    return self._conn.adb_command(cmd, timeout=timeout)\n```\n\n### Step 2: Remove subprocess import\nRemove line 23 from `post_reel_smart.py`:\n```python\nimport subprocess  # REMOVE THIS LINE\n```\n\n### Step 3: Verify all ADB callers work unchanged\nThe following calls in `post_reel_smart.py` use `self.adb()` and should continue working:\n- Line 459: `self.adb(\"dumpsys input_method | grep mInputShown\")` in `is_keyboard_visible()`\n- Line 464: `self.adb(\"dumpsys window | grep -i keyboard\")` in `is_keyboard_visible()`\n- Line 469: `self.adb(\"dumpsys window windows | grep -E 'mCurrentFocus|mFocusedApp'\")` in `is_keyboard_visible()`\n- Line 586: `self.adb(\"am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE...\")` in `upload_video()`\n- Lines 590-591: `self.adb(\"rm -f /sdcard/DCIM/Camera/IMG_*.png\")` and screenshot cleanup in `upload_video()`\n- Line 612: `self.adb(\"am force-stop com.instagram.android\")` in `post()`\n- Line 614: `self.adb(\"monkey -p com.instagram.android 1\")` in `post()`\n- Lines 695, 697, 771, 773: More `adb input swipe` and app control commands in `post()`\n- Lines 804, 806: App restart commands in loop recovery\n- Line 822: `self.adb(\"rm -f /sdcard/Download/*.mp4\")` in `cleanup()`\n\n## Architecture After Change\n\n```\nBefore:\nSmartInstagramPoster.adb() -> subprocess.run() [direct infrastructure coupling]\n\nAfter:\nSmartInstagramPoster.adb() -> self._conn.adb_command() -> subprocess.run()\n                              [single point of ADB access via DeviceConnectionManager]\n```\n\n## Benefits\n1. **Clear boundary**: SmartInstagramPoster becomes a pure posting logic class\n2. **Single responsibility**: DeviceConnectionManager owns ALL device communication\n3. **Testability**: Can mock DeviceConnectionManager for unit testing SmartInstagramPoster\n4. **Consistency**: All ADB operations go through the same path with consistent error handling",
        "testStrategy": "## Test Strategy\n\n### 1. Syntax and Import Verification\n```bash\n# Verify the file has no syntax errors and imports correctly\npython -c \"from post_reel_smart import SmartInstagramPoster; print('Import successful')\"\n```\n\n### 2. Verify subprocess is NOT imported in post_reel_smart.py\n```bash\n# This should return empty (no matches)\npython -c \"\nwith open('post_reel_smart.py') as f:\n    content = f.read()\n    lines = [l for l in content.split('\\n') if 'import subprocess' in l and not l.strip().startswith('#')]\n    if lines:\n        print('FAIL: subprocess still imported:', lines)\n        exit(1)\n    print('PASS: subprocess not imported')\n\"\n```\n\n### 3. Verify adb() method delegates to DeviceConnectionManager\n```bash\n# Inspect the adb method to confirm it calls self._conn.adb_command\npython -c \"\nimport inspect\nfrom post_reel_smart import SmartInstagramPoster\nsource = inspect.getsource(SmartInstagramPoster.adb)\nif 'self._conn.adb_command' in source:\n    print('PASS: adb() delegates to self._conn.adb_command()')\nelse:\n    print('FAIL: adb() does not delegate to DeviceConnectionManager')\n    print(source)\n    exit(1)\n\"\n```\n\n### 4. Integration Test - Existing Scripts Work Unchanged\n```bash\n# Test that posting_scheduler.py still imports and runs\npython -c \"from posting_scheduler import PostingScheduler; print('PostingScheduler import OK')\"\n\n# Test that parallel_worker.py still imports and runs\npython -c \"from parallel_worker import ParallelWorker; print('ParallelWorker import OK')\"\n\n# Test that parallel_orchestrator.py still imports and runs\npython -c \"from parallel_orchestrator import run_orchestrator; print('Orchestrator import OK')\"\n```\n\n### 5. Manual Test - Full Posting Flow (Optional)\n```bash\n# Run a single phone posting test to verify all ADB commands work\n# This requires a running Appium server and available Geelark phone\npython post_reel_smart.py <test_phone_name> <test_video.mp4> \"Test caption\"\n```\n\n### 6. Verify DeviceConnectionManager.adb_command() Still Works\n```bash\n# Unit test the underlying adb_command method\npython -c \"\nfrom device_connection import DeviceConnectionManager\n# Just verify the method exists and has correct signature\nimport inspect\nsig = inspect.signature(DeviceConnectionManager.adb_command)\nparams = list(sig.parameters.keys())\nassert 'cmd' in params, 'Missing cmd parameter'\nassert 'timeout' in params, 'Missing timeout parameter'\nprint('PASS: DeviceConnectionManager.adb_command() has correct signature')\n\"\n```\n\n### 7. Regression Check - No Other subprocess Usages\n```bash\n# Ensure no stray subprocess calls remain in post_reel_smart.py\ngrep -n \"subprocess\" post_reel_smart.py && echo \"FAIL: Found subprocess references\" || echo \"PASS: No subprocess references\"\n```",
        "status": "done",
        "dependencies": [
          "37",
          "25"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T11:12:44.672Z"
      },
      {
        "id": "41",
        "title": "Extract _handle_tap_and_type helper from post() method",
        "description": "Extract the tap_and_type action handler (lines 701-758) from the post() method into a dedicated _handle_tap_and_type() helper method to reduce nesting complexity and improve code organization.",
        "details": "## Current State Analysis\n\nThe `post()` method in `post_reel_smart.py` contains an inline tap_and_type handler at lines 701-758 with 4 levels of nesting for keyboard state management:\n\n### Current Structure (lines 701-758):\n```python\nelif action['action'] == 'tap_and_type':\n    # Level 1: Check if caption already entered\n    if self.caption_entered:\n        # Skip logic - find Share button\n        continue\n    \n    # Get element info\n    idx = action.get('element_index', 0)\n    text = action.get('text', caption)\n    \n    # Level 2: Check keyboard visibility\n    keyboard_up = self.is_keyboard_visible()\n    \n    if not keyboard_up:\n        # Level 3: Tap and recheck\n        if 0 <= idx < len(elements):\n            self.tap(...)\n        keyboard_up = self.is_keyboard_visible()\n        \n        if not keyboard_up:\n            # Level 4: Tap again\n            if 0 <= idx < len(elements):\n                self.tap(...)\n            keyboard_up = self.is_keyboard_visible()\n    \n    if keyboard_up:\n        # Type text and verify\n        self.type_text(text)\n        # Verification logic\n        self.caption_entered = True\n        self.press_key('KEYCODE_BACK')\n    else:\n        print(\"ERROR: Could not get keyboard\")\n```\n\n## Implementation Plan\n\n### Step 1: Create the _handle_tap_and_type() method\n\nAdd a new private method to `SmartInstagramPoster` class (place it before `post()` method, around line 589):\n\n```python\ndef _handle_tap_and_type(self, action: dict, elements: list, caption: str) -> bool:\n    \"\"\"Handle tap_and_type action with keyboard state management.\n    \n    Args:\n        action: Action dict from Claude analysis with element_index and text\n        elements: Current UI elements list\n        caption: Original caption text (used as fallback for text)\n    \n    Returns:\n        True if loop should continue to next step (handled internally)\n        False if normal flow should continue\n    \"\"\"\n    # Early exit if caption already entered - tap Share instead\n    if self.caption_entered:\n        print(\"  [SKIP] Caption already entered! Tapping Share instead.\")\n        share_elements = [\n            e for e in elements \n            if e.get('text', '').lower() == 'share' \n            or e.get('desc', '').lower() == 'share'\n        ]\n        if share_elements:\n            self.tap(share_elements[0]['center'][0], share_elements[0]['center'][1])\n            self.share_clicked = True\n        return True  # Signal to continue loop\n    \n    idx = action.get('element_index', 0)\n    text = action.get('text', caption)\n    \n    # Ensure keyboard is visible before typing\n    keyboard_up = self._ensure_keyboard_visible(idx, elements)\n    \n    if keyboard_up:\n        self._type_and_verify_caption(text)\n    else:\n        print(\"  ERROR: Could not get keyboard to appear. Will retry on next step.\")\n    \n    return False  # Normal flow continues\n```\n\n### Step 2: Create _ensure_keyboard_visible() helper\n\n```python\ndef _ensure_keyboard_visible(self, element_index: int, elements: list) -> bool:\n    \"\"\"Ensure keyboard is visible by tapping element if needed.\n    \n    Args:\n        element_index: Index of element to tap\n        elements: Current UI elements list\n    \n    Returns:\n        True if keyboard is now visible\n    \"\"\"\n    print(\"  Checking if keyboard is up...\")\n    keyboard_up = self.is_keyboard_visible()\n    \n    if keyboard_up:\n        return True\n    \n    # First tap attempt\n    if 0 <= element_index < len(elements):\n        elem = elements[element_index]\n        print(f\"  Keyboard not up. Tapping caption field at ({elem['center'][0]}, {elem['center'][1]})\")\n        self.tap(elem['center'][0], elem['center'][1])\n        time.sleep(1.5)\n    \n    print(\"  Checking keyboard again...\")\n    keyboard_up = self.is_keyboard_visible()\n    \n    if keyboard_up:\n        return True\n    \n    # Second tap attempt\n    print(\"  Keyboard still not up. Tapping again...\")\n    if 0 <= element_index < len(elements):\n        elem = elements[element_index]\n        self.tap(elem['center'][0], elem['center'][1])\n        time.sleep(1.5)\n    \n    return self.is_keyboard_visible()\n```\n\n### Step 3: Create _type_and_verify_caption() helper\n\n```python\ndef _type_and_verify_caption(self, text: str) -> None:\n    \"\"\"Type caption text and verify it was entered.\n    \n    Args:\n        text: Caption text to type\n    \"\"\"\n    print(f\"  Keyboard is up. Typing: {text[:50]}...\")\n    self.type_text(text)\n    time.sleep(1)\n    \n    # Best-effort verification\n    print(\"  Verifying caption was typed...\")\n    verify_elements, _ = self.dump_ui()\n    caption_found = any(text[:20] in elem.get('text', '') for elem in verify_elements)\n    \n    if caption_found:\n        print(\"  Caption appears in UI dump.\")\n    else:\n        print(\"  Caption not visible in UI dump (normal for IG caption field); assuming entered.\")\n    \n    self.caption_entered = True\n    \n    # Hide keyboard\n    self.press_key('KEYCODE_BACK')\n    time.sleep(0.5)\n```\n\n### Step 4: Update post() method to use the helper\n\nReplace lines 701-758 with:\n```python\nelif action['action'] == 'tap_and_type':\n    if self._handle_tap_and_type(action, elements, caption):\n        continue  # Handler signaled to skip to next step\n```\n\n## Method Placement\n\nInsert the new methods in this order before `post()`:\n1. `_ensure_keyboard_visible()` - around line 565 (after `connect_appium()`)\n2. `_type_and_verify_caption()` - around line 590\n3. `_handle_tap_and_type()` - around line 610\n\n## Benefits of Extraction\n\n1. **Reduced nesting**: post() goes from 4 nested levels to 1 level for tap_and_type handling\n2. **Single responsibility**: Each helper method does one thing\n3. **Testability**: Individual helpers can be unit tested\n4. **Readability**: post() main loop is cleaner and easier to follow\n5. **Reusability**: _ensure_keyboard_visible() could be reused elsewhere",
        "testStrategy": "## Test Strategy\n\n### 1. Syntax and Import Verification\n```bash\n# Verify the file has no syntax errors and imports correctly\npython -c \"from post_reel_smart import SmartInstagramPoster; print('Import successful')\"\n```\n\n### 2. Method Existence Verification\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nposter = SmartInstagramPoster.__new__(SmartInstagramPoster)\nassert hasattr(poster, '_handle_tap_and_type'), 'Missing _handle_tap_and_type'\nassert hasattr(poster, '_ensure_keyboard_visible'), 'Missing _ensure_keyboard_visible'\nassert hasattr(poster, '_type_and_verify_caption'), 'Missing _type_and_verify_caption'\nprint('All helper methods exist')\n\"\n```\n\n### 3. Method Signature Verification\n```bash\npython -c \"\nimport inspect\nfrom post_reel_smart import SmartInstagramPoster\n\n# Check _handle_tap_and_type signature\nsig = inspect.signature(SmartInstagramPoster._handle_tap_and_type)\nparams = list(sig.parameters.keys())\nassert 'action' in params, 'Missing action parameter'\nassert 'elements' in params, 'Missing elements parameter'\nassert 'caption' in params, 'Missing caption parameter'\nprint('_handle_tap_and_type signature correct:', params)\n\n# Check _ensure_keyboard_visible signature\nsig = inspect.signature(SmartInstagramPoster._ensure_keyboard_visible)\nparams = list(sig.parameters.keys())\nassert 'element_index' in params, 'Missing element_index parameter'\nassert 'elements' in params, 'Missing elements parameter'\nprint('_ensure_keyboard_visible signature correct:', params)\n\n# Check _type_and_verify_caption signature\nsig = inspect.signature(SmartInstagramPoster._type_and_verify_caption)\nparams = list(sig.parameters.keys())\nassert 'text' in params, 'Missing text parameter'\nprint('_type_and_verify_caption signature correct:', params)\n\"\n```\n\n### 4. Return Type Verification\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nimport inspect\n\n# Check that _handle_tap_and_type returns bool\nsource = inspect.getsource(SmartInstagramPoster._handle_tap_and_type)\nassert 'return True' in source, '_handle_tap_and_type should return True'\nassert 'return False' in source, '_handle_tap_and_type should return False'\nprint('_handle_tap_and_type has correct return statements')\n\"\n```\n\n### 5. Integration Test - Verify tap_and_type action handling\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nimport inspect\n\n# Get post method source to verify it uses the helper\nsource = inspect.getsource(SmartInstagramPoster.post)\n\n# Verify inline tap_and_type logic is removed\nassert 'Checking if keyboard is up...' not in source, 'Old inline keyboard check still in post()'\nassert 'Keyboard still not up. Tapping again' not in source, 'Old inline retry logic still in post()'\n\n# Verify helper is called\nassert '_handle_tap_and_type' in source, 'post() should call _handle_tap_and_type'\nprint('post() correctly delegates to _handle_tap_and_type helper')\n\"\n```\n\n### 6. Line Count Reduction Verification\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nimport inspect\n\n# Get post method source\nsource = inspect.getsource(SmartInstagramPoster.post)\nlines = [l for l in source.split('\\n') if l.strip()]\n\n# Count lines related to tap_and_type in post()\ntap_type_lines = [l for l in lines if 'tap_and_type' in l.lower()]\nprint(f'Lines mentioning tap_and_type in post(): {len(tap_type_lines)}')\n\n# The tap_and_type block in post() should be minimal (< 5 lines)\n# The logic is now in the helper methods\n\"\n```\n\n### 7. Live Test with Actual Posting (Optional)\n```bash\n# Only run if you want to test with a real account\n# Uses the parallel orchestrator which calls post() internally\npython parallel_orchestrator.py --workers 1 --run --max-posts 1\n```\n\n### 8. Behavioral Equivalence Test\nVerify the refactored code behaves identically:\n1. Start a post that requires caption entry\n2. Verify keyboard detection still works\n3. Verify caption typing still works\n4. Verify caption verification still works\n5. Verify keyboard dismissal still happens",
        "status": "done",
        "dependencies": [
          "39",
          "40"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T11:24:31.758Z"
      },
      {
        "id": "42",
        "title": "Extract _detect_and_recover_from_loop helper from post() method",
        "description": "Extract the loop detection and recovery logic (lines 769-804) from the post() method into a dedicated _detect_and_recover_from_loop() helper method to improve readability and reduce the complexity of the main posting loop.",
        "details": "## Current State Analysis\n\nThe `post()` method in `post_reel_smart.py` contains inline loop detection and recovery logic at lines 769-804:\n\n### Current Structure (lines 614-618, 769-804):\n\n**Initialization (lines 614-618):**\n```python\n# Loop detection - track recent actions to detect stuck states\nrecent_actions = []  # List of (action_type, x, y) tuples\nLOOP_THRESHOLD = 5  # If 5 consecutive same actions, we're stuck\nloop_recovery_count = 0  # How many times we've tried to recover\nMAX_LOOP_RECOVERIES = 2  # Give up after this many recovery attempts\n```\n\n**Action tracking (lines 769-778):**\n```python\n# Track action for loop detection\naction_signature = action['action']\nif action['action'] == 'tap' and 'element_index' in action:\n    idx = action.get('element_index', 0)\n    if 0 <= idx < len(elements):\n        x, y = elements[idx]['center']\n        action_signature = f\"tap_{x}_{y}\"\nrecent_actions.append(action_signature)\nif len(recent_actions) > LOOP_THRESHOLD:\n    recent_actions.pop(0)\n```\n\n**Loop detection and recovery (lines 780-804):**\n```python\n# Check for loop - if last N actions are all identical, we're stuck\nif len(recent_actions) >= LOOP_THRESHOLD and len(set(recent_actions)) == 1:\n    loop_recovery_count += 1\n    print(f\"\\n  [LOOP DETECTED] Same action '{recent_actions[0]}' repeated {LOOP_THRESHOLD} times!\")\n    print(f\"  [RECOVERY] Attempt {loop_recovery_count}/{MAX_LOOP_RECOVERIES}\")\n\n    if loop_recovery_count > MAX_LOOP_RECOVERIES:\n        print(\"  [ABORT] Too many loop recoveries, giving up\")\n        return False\n\n    # Recovery: press back 5 times and restart Instagram\n    print(\"  Pressing BACK 5 times to escape stuck state...\")\n    for _ in range(5):\n        self.press_key('KEYCODE_BACK')\n        time.sleep(0.5)\n\n    print(\"  Reopening Instagram...\")\n    self.adb(\"am force-stop com.instagram.android\")\n    time.sleep(2)\n    self.adb(\"monkey -p com.instagram.android 1\")\n    time.sleep(5)\n\n    # Reset action tracking\n    recent_actions = []\n    print(\"  [RECOVERY] Restarted - continuing from step\", step + 1)\n```\n\n## Implementation Plan\n\n### Step 1: Define constants as class-level attributes (add after line 70)\n\n```python\n# Loop detection constants\nLOOP_THRESHOLD = 5  # If N consecutive same actions, we're stuck\nMAX_LOOP_RECOVERIES = 2  # Give up after this many recovery attempts\n```\n\n### Step 2: Create _build_action_signature() helper (add after cleanup() method, ~line 820)\n\n```python\ndef _build_action_signature(self, action, elements):\n    \"\"\"Build a unique signature for an action to detect loops.\n    \n    Args:\n        action: The action dict from Claude's analysis\n        elements: List of UI elements\n        \n    Returns:\n        str: Action signature for loop comparison\n    \"\"\"\n    action_signature = action['action']\n    if action['action'] == 'tap' and 'element_index' in action:\n        idx = action.get('element_index', 0)\n        if 0 <= idx < len(elements):\n            x, y = elements[idx]['center']\n            action_signature = f\"tap_{x}_{y}\"\n    return action_signature\n```\n\n### Step 3: Create _detect_and_recover_from_loop() helper (add after _build_action_signature)\n\n```python\ndef _detect_and_recover_from_loop(self, recent_actions, loop_recovery_count, step):\n    \"\"\"Detect if we're stuck in a loop and attempt recovery.\n    \n    Args:\n        recent_actions: List of recent action signatures\n        loop_recovery_count: Current number of recovery attempts\n        step: Current step number (for logging)\n        \n    Returns:\n        tuple: (should_abort: bool, new_recovery_count: int, reset_actions: bool)\n            - should_abort: True if we should abort the entire post operation\n            - new_recovery_count: Updated recovery count\n            - reset_actions: True if recent_actions should be cleared\n    \"\"\"\n    # Not enough actions to detect a loop yet\n    if len(recent_actions) < self.LOOP_THRESHOLD:\n        return (False, loop_recovery_count, False)\n    \n    # Check if all recent actions are identical (loop detected)\n    if len(set(recent_actions)) != 1:\n        return (False, loop_recovery_count, False)\n    \n    # Loop detected!\n    loop_recovery_count += 1\n    print(f\"\\n  [LOOP DETECTED] Same action '{recent_actions[0]}' repeated {self.LOOP_THRESHOLD} times!\")\n    print(f\"  [RECOVERY] Attempt {loop_recovery_count}/{self.MAX_LOOP_RECOVERIES}\")\n    \n    # Check if we've exceeded max recovery attempts\n    if loop_recovery_count > self.MAX_LOOP_RECOVERIES:\n        print(\"  [ABORT] Too many loop recoveries, giving up\")\n        return (True, loop_recovery_count, False)\n    \n    # Attempt recovery: press back 5 times and restart Instagram\n    print(\"  Pressing BACK 5 times to escape stuck state...\")\n    for _ in range(5):\n        self.press_key('KEYCODE_BACK')\n        time.sleep(0.5)\n    \n    print(\"  Reopening Instagram...\")\n    self.adb(\"am force-stop com.instagram.android\")\n    time.sleep(2)\n    self.adb(\"monkey -p com.instagram.android 1\")\n    time.sleep(5)\n    \n    print(f\"  [RECOVERY] Restarted - continuing from step {step + 1}\")\n    return (False, loop_recovery_count, True)\n```\n\n### Step 4: Refactor post() method to use the helpers\n\nReplace lines 614-618 (initialization):\n```python\n# Loop detection state\nrecent_actions = []\nloop_recovery_count = 0\n```\n\nReplace lines 769-804 with:\n```python\n# Track action for loop detection\naction_signature = self._build_action_signature(action, elements)\nrecent_actions.append(action_signature)\nif len(recent_actions) > self.LOOP_THRESHOLD:\n    recent_actions.pop(0)\n\n# Check for loop and attempt recovery if needed\nshould_abort, loop_recovery_count, reset_actions = self._detect_and_recover_from_loop(\n    recent_actions, loop_recovery_count, step\n)\nif should_abort:\n    return False\nif reset_actions:\n    recent_actions = []\n```\n\n## Key Design Decisions\n\n1. **Return tuple pattern**: The helper returns a tuple `(should_abort, new_recovery_count, reset_actions)` to communicate multiple outcomes without side effects on mutable arguments.\n\n2. **Class-level constants**: Moving `LOOP_THRESHOLD` and `MAX_LOOP_RECOVERIES` to class attributes allows easy configuration and testing.\n\n3. **Separate signature builder**: The `_build_action_signature()` helper is small but encapsulates the logic of creating comparable action signatures, improving testability.\n\n4. **Preserve logging**: All print statements are preserved in the helper to maintain the same debug output.\n\n5. **No behavior changes**: The extracted code must produce identical behavior to the current implementation.\n\n## Files Modified\n\n- `post_reel_smart.py`: Add class constants, add two helper methods, refactor post() loop detection section",
        "testStrategy": "## Test Strategy\n\n### 1. Syntax and Import Verification\n```bash\n# Verify the file has no syntax errors and imports correctly\npython -c \"from post_reel_smart import SmartInstagramPoster; print('Import successful')\"\n```\n\n### 2. Method Existence Verification\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nposter = SmartInstagramPoster.__new__(SmartInstagramPoster)\n\n# Verify new methods exist\nassert hasattr(poster, '_build_action_signature'), 'Missing _build_action_signature'\nassert hasattr(poster, '_detect_and_recover_from_loop'), 'Missing _detect_and_recover_from_loop'\nassert callable(poster._build_action_signature), '_build_action_signature not callable'\nassert callable(poster._detect_and_recover_from_loop), '_detect_and_recover_from_loop not callable'\n\n# Verify class constants exist\nassert hasattr(SmartInstagramPoster, 'LOOP_THRESHOLD'), 'Missing LOOP_THRESHOLD constant'\nassert hasattr(SmartInstagramPoster, 'MAX_LOOP_RECOVERIES'), 'Missing MAX_LOOP_RECOVERIES constant'\nassert SmartInstagramPoster.LOOP_THRESHOLD == 5, f'LOOP_THRESHOLD should be 5, got {SmartInstagramPoster.LOOP_THRESHOLD}'\nassert SmartInstagramPoster.MAX_LOOP_RECOVERIES == 2, f'MAX_LOOP_RECOVERIES should be 2, got {SmartInstagramPoster.MAX_LOOP_RECOVERIES}'\n\nprint('Method and constant verification passed')\n\"\n```\n\n### 3. Unit Test - _build_action_signature()\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nposter = SmartInstagramPoster.__new__(SmartInstagramPoster)\n\n# Test basic action signature\naction = {'action': 'scroll_down'}\nelements = []\nsig = poster._build_action_signature(action, elements)\nassert sig == 'scroll_down', f'Expected scroll_down, got {sig}'\n\n# Test tap action with element\naction = {'action': 'tap', 'element_index': 0}\nelements = [{'center': (100, 200)}]\nsig = poster._build_action_signature(action, elements)\nassert sig == 'tap_100_200', f'Expected tap_100_200, got {sig}'\n\n# Test tap action with invalid index\naction = {'action': 'tap', 'element_index': 99}\nelements = [{'center': (100, 200)}]\nsig = poster._build_action_signature(action, elements)\nassert sig == 'tap', f'Expected tap (invalid index), got {sig}'\n\n# Test tap action without element_index\naction = {'action': 'tap'}\nelements = [{'center': (100, 200)}]\nsig = poster._build_action_signature(action, elements)\nassert sig == 'tap', f'Expected tap (no index), got {sig}'\n\nprint('_build_action_signature tests passed')\n\"\n```\n\n### 4. Unit Test - _detect_and_recover_from_loop() (no loop case)\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nfrom unittest.mock import MagicMock\n\nposter = SmartInstagramPoster.__new__(SmartInstagramPoster)\nposter.LOOP_THRESHOLD = 5\nposter.MAX_LOOP_RECOVERIES = 2\n\n# Test with fewer than threshold actions (no loop)\nrecent_actions = ['tap_100_200', 'tap_150_300', 'scroll_down']\nshould_abort, new_count, reset = poster._detect_and_recover_from_loop(recent_actions, 0, 5)\nassert not should_abort, 'Should not abort with few actions'\nassert new_count == 0, 'Recovery count should remain 0'\nassert not reset, 'Should not reset actions'\n\n# Test with different actions (no loop)\nrecent_actions = ['tap_100_200', 'scroll_down', 'tap_150_300', 'back', 'tap_200_400']\nshould_abort, new_count, reset = poster._detect_and_recover_from_loop(recent_actions, 0, 5)\nassert not should_abort, 'Should not abort with varied actions'\nassert new_count == 0, 'Recovery count should remain 0'\nassert not reset, 'Should not reset actions'\n\nprint('No-loop detection tests passed')\n\"\n```\n\n### 5. Unit Test - _detect_and_recover_from_loop() (loop with recovery)\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nfrom unittest.mock import MagicMock\nimport time\n\nposter = SmartInstagramPoster.__new__(SmartInstagramPoster)\nposter.LOOP_THRESHOLD = 5\nposter.MAX_LOOP_RECOVERIES = 2\nposter.press_key = MagicMock()\nposter.adb = MagicMock()\n\n# Patch time.sleep to speed up test\noriginal_sleep = time.sleep\ntime.sleep = lambda x: None\n\ntry:\n    # Test loop detected - first recovery\n    recent_actions = ['tap_100_200'] * 5\n    should_abort, new_count, reset = poster._detect_and_recover_from_loop(recent_actions, 0, 10)\n    assert not should_abort, 'Should not abort on first recovery'\n    assert new_count == 1, f'Recovery count should be 1, got {new_count}'\n    assert reset, 'Should reset actions after recovery'\n    assert poster.press_key.call_count == 5, f'Should press BACK 5 times, called {poster.press_key.call_count}'\n    assert poster.adb.call_count >= 2, 'Should call adb for force-stop and monkey'\n    \n    print('Loop recovery test passed')\nfinally:\n    time.sleep = original_sleep\n\"\n```\n\n### 6. Unit Test - _detect_and_recover_from_loop() (max recoveries exceeded)\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nfrom unittest.mock import MagicMock\n\nposter = SmartInstagramPoster.__new__(SmartInstagramPoster)\nposter.LOOP_THRESHOLD = 5\nposter.MAX_LOOP_RECOVERIES = 2\n\n# Test max recoveries exceeded - should abort\nrecent_actions = ['tap_100_200'] * 5\nshould_abort, new_count, reset = poster._detect_and_recover_from_loop(recent_actions, 2, 10)\nassert should_abort, 'Should abort when max recoveries exceeded'\nassert new_count == 3, f'Recovery count should be 3, got {new_count}'\n\nprint('Max recovery abort test passed')\n\"\n```\n\n### 7. Integration Test - Full post() method still works\n```bash\n# Verify post() method still exists and has proper structure\npython -c \"\nimport inspect\nfrom post_reel_smart import SmartInstagramPoster\n\n# Check post method signature\nsig = inspect.signature(SmartInstagramPoster.post)\nparams = list(sig.parameters.keys())\nexpected = ['self', 'video_path', 'caption', 'max_steps', 'humanize']\nassert params == expected, f'post() params changed: {params} != {expected}'\n\n# Check that post() uses the helper methods (by inspecting source)\nsource = inspect.getsource(SmartInstagramPoster.post)\nassert '_build_action_signature' in source, 'post() should call _build_action_signature'\nassert '_detect_and_recover_from_loop' in source, 'post() should call _detect_and_recover_from_loop'\nassert 'recent_actions' in source, 'post() should still track recent_actions'\nassert 'loop_recovery_count' in source, 'post() should still track loop_recovery_count'\n\nprint('Integration check passed')\n\"\n```\n\n### 8. Line Count Verification\n```bash\n# Verify the post() method is shorter after extraction\npython -c \"\nimport inspect\nfrom post_reel_smart import SmartInstagramPoster\n\nsource = inspect.getsource(SmartInstagramPoster.post)\nlines = [l for l in source.split('\\n') if l.strip()]\nprint(f'post() method: {len(lines)} non-empty lines')\n\n# The inline loop detection was ~35 lines, now ~10 lines\n# post() should be noticeably shorter\nassert len(lines) < 250, f'post() should be shorter after extraction, got {len(lines)} lines'\nprint('Line count check passed')\n\"\n```\n\n### 9. Live Test - Run with actual posting (manual verification)\n```bash\n# This should be run manually to verify behavior is unchanged\n# python post_reel_smart.py test_phone test_video.mp4 \"Test caption\"\n# Verify loop detection still works by observing logs during stuck states\n```",
        "status": "done",
        "dependencies": [
          "39",
          "40",
          "41"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T11:25:35.704Z"
      },
      {
        "id": "43",
        "title": "Extract humanize action handlers from humanize_before_post()",
        "description": "Extract the 4 inline action handlers (scroll_feed, view_story, scroll_reels, check_notifications) from humanize_before_post() method (lines 151-243) into dedicated private helper methods, reducing the 93-line method to a clean ~25-line dispatch loop.",
        "details": "## Current State Analysis\n\nThe `humanize_before_post()` method in `post_reel_smart.py` (lines 151-243, ~93 lines) contains 4 inline action handlers with nested loops:\n\n### Current Structure:\n```python\ndef humanize_before_post(self):\n    print(\"\\n[HUMANIZE] Performing random actions before posting...\")\n    actions_done = 0\n    max_actions = random.randint(2, 4)\n\n    for _ in range(max_actions):\n        action = random.choice(['scroll_feed', 'view_story', 'scroll_reels', 'check_notifications'])\n\n        if action == 'scroll_feed':\n            # ~11 lines of inline scroll logic (lines 160-170)\n            ...\n        elif action == 'view_story':\n            # ~19 lines of inline story viewing logic (lines 172-190)\n            ...\n        elif action == 'scroll_reels':\n            # ~29 lines of inline reels browsing logic (lines 192-220)\n            ...\n        elif action == 'check_notifications':\n            # ~15 lines of inline notification checking logic (lines 222-236)\n            ...\n```\n\n## Implementation Plan\n\n### Step 1: Extract `_humanize_scroll_feed()` (~15 lines)\n\n**Location:** Add after `random_delay()` method (around line 150)\n\n```python\ndef _humanize_scroll_feed(self):\n    \"\"\"Scroll through Instagram feed randomly.\"\"\"\n    print(\"  - Scrolling feed...\")\n    scroll_count = random.randint(1, 3)\n    for _ in range(scroll_count):\n        self.swipe(360, 900, 360, 400, random.randint(200, 400))\n        self.random_delay(1.0, 3.0)\n    # Scroll back up sometimes\n    if random.random() < 0.3:\n        self.swipe(360, 400, 360, 900, 300)\n        self.random_delay(0.5, 1.5)\n    return True  # Action always succeeds\n```\n\n### Step 2: Extract `_humanize_view_story()` (~25 lines)\n\n```python\ndef _humanize_view_story(self):\n    \"\"\"View Instagram stories randomly.\n    \n    Returns True if a story was viewed, False if no unseen stories found.\n    \"\"\"\n    print(\"  - Viewing a story...\")\n    elements, _ = self.dump_ui()\n    story_elements = [e for e in elements \n                      if 'story' in e.get('desc', '').lower() \n                      and 'unseen' in e.get('desc', '').lower()]\n    if not story_elements:\n        return False\n    \n    story = random.choice(story_elements)\n    self.tap(story['center'][0], story['center'][1])\n    view_time = random.uniform(3, 8)\n    print(f\"    Watching for {view_time:.1f}s...\")\n    time.sleep(view_time)\n    \n    # Tap through a few more stories sometimes\n    if random.random() < 0.5:\n        for _ in range(random.randint(1, 3)):\n            self.tap(650, 640)  # Tap right side to skip to next story\n            time.sleep(random.uniform(2, 5))\n    \n    # Go back\n    self.press_key('KEYCODE_BACK')\n    self.random_delay(1.0, 2.0)\n    return True\n```\n\n### Step 3: Extract `_humanize_scroll_reels()` (~30 lines)\n\n```python\ndef _humanize_scroll_reels(self):\n    \"\"\"Browse Instagram Reels tab randomly.\n    \n    Returns True if reels were browsed, False if Reels tab not found.\n    \"\"\"\n    print(\"  - Browsing reels...\")\n    elements, _ = self.dump_ui()\n    reels_tab = [e for e in elements \n                 if 'reels' in e.get('desc', '').lower() and e['clickable']]\n    if not reels_tab:\n        return False\n    \n    self.tap(reels_tab[0]['center'][0], reels_tab[0]['center'][1])\n    self.random_delay(2.0, 4.0)\n    \n    # Watch a few reels\n    for _ in range(random.randint(1, 3)):\n        watch_time = random.uniform(3, 10)\n        print(f\"    Watching reel for {watch_time:.1f}s...\")\n        time.sleep(watch_time)\n        # Sometimes double-tap to like\n        if random.random() < 0.15:\n            print(\"    Double-tap like!\")\n            self.tap(360, 640)\n            time.sleep(0.1)\n            self.tap(360, 640)\n            self.random_delay(0.5, 1.0)\n        # Swipe to next reel\n        self.swipe(360, 1000, 360, 300, 200)\n        self.random_delay(0.5, 1.5)\n    \n    # Go back to home\n    elements, _ = self.dump_ui()\n    home_tab = [e for e in elements \n                if 'home' in e.get('desc', '').lower() and e['clickable']]\n    if home_tab:\n        self.tap(home_tab[0]['center'][0], home_tab[0]['center'][1])\n    self.random_delay(1.0, 2.0)\n    return True\n```\n\n### Step 4: Extract `_humanize_check_notifications()` (~20 lines)\n\n```python\ndef _humanize_check_notifications(self):\n    \"\"\"Check Instagram notifications/activity tab randomly.\n    \n    Returns True if notifications were checked, False if tab not found.\n    \"\"\"\n    print(\"  - Checking notifications...\")\n    elements, _ = self.dump_ui()\n    notif_btn = [e for e in elements \n                 if ('notification' in e.get('desc', '').lower() \n                     or 'activity' in e.get('desc', '').lower()) \n                 and e['clickable']]\n    if not notif_btn:\n        return False\n    \n    self.tap(notif_btn[0]['center'][0], notif_btn[0]['center'][1])\n    self.random_delay(2.0, 4.0)\n    \n    # Scroll through notifications sometimes\n    if random.random() < 0.5:\n        self.swipe(360, 800, 360, 400, 300)\n        self.random_delay(1.0, 2.0)\n    \n    # Go back\n    self.press_key('KEYCODE_BACK')\n    self.random_delay(1.0, 2.0)\n    return True\n```\n\n### Step 5: Refactor `humanize_before_post()` to Clean Dispatch Loop (~25 lines)\n\n```python\ndef humanize_before_post(self):\n    \"\"\"Perform random human-like actions before posting.\"\"\"\n    print(\"\\n[HUMANIZE] Performing random actions before posting...\")\n    \n    # Map action names to handler methods\n    action_handlers = {\n        'scroll_feed': self._humanize_scroll_feed,\n        'view_story': self._humanize_view_story,\n        'scroll_reels': self._humanize_scroll_reels,\n        'check_notifications': self._humanize_check_notifications,\n    }\n    \n    actions_done = 0\n    max_actions = random.randint(2, 4)\n\n    for _ in range(max_actions):\n        action = random.choice(list(action_handlers.keys()))\n        handler = action_handlers[action]\n        \n        if handler():\n            actions_done += 1\n        \n        if actions_done >= max_actions:\n            break\n\n    print(f\"[HUMANIZE] Completed {actions_done} random actions\")\n    # Small delay before proceeding\n    self.random_delay(1.0, 3.0)\n```\n\n## Key Design Decisions\n\n1. **Return values for success tracking**: Each helper returns `True` if the action was performed, `False` if UI elements weren't found. This preserves the original behavior where `actions_done` only increments on successful actions.\n\n2. **Naming convention**: Using `_humanize_*` prefix to:\n   - Indicate private methods (underscore prefix)\n   - Group them logically with the humanization feature\n   - Make them easy to find via search/autocomplete\n\n3. **No parameter passing**: All helpers use `self` to access `dump_ui()`, `tap()`, `swipe()`, `press_key()`, and `random_delay()`. This keeps signatures clean since all state is on the instance.\n\n4. **Preserve exact behavior**: The random delays, tap coordinates, and conditional logic are preserved exactly as-is to avoid changing humanization behavior.\n\n## File Changes Summary\n\n| Change | Lines Affected |\n|--------|----------------|\n| Add `_humanize_scroll_feed()` | Insert ~15 lines after line 149 |\n| Add `_humanize_view_story()` | Insert ~25 lines |\n| Add `_humanize_scroll_reels()` | Insert ~30 lines |\n| Add `_humanize_check_notifications()` | Insert ~20 lines |\n| Replace `humanize_before_post()` body | Lines 151-243 → ~25 lines |\n\n**Net effect**: From 93 lines to ~25 lines in main method, with 4 focused helper methods (~90 lines total). Total code grows slightly but complexity per method decreases significantly.",
        "testStrategy": "## Test Strategy\n\n### 1. Syntax and Import Verification\n```bash\n# Verify the file has no syntax errors and imports correctly\npython -c \"from post_reel_smart import SmartInstagramPoster; print('Import successful')\"\n```\n\n### 2. Method Existence Verification\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nposter = SmartInstagramPoster.__new__(SmartInstagramPoster)\nmethods = ['_humanize_scroll_feed', '_humanize_view_story', '_humanize_scroll_reels', '_humanize_check_notifications', 'humanize_before_post']\nfor m in methods:\n    assert hasattr(poster, m), f'Missing method: {m}'\n    assert callable(getattr(poster, m)), f'Not callable: {m}'\nprint('All humanize methods exist and are callable')\n\"\n```\n\n### 3. Method Signature Verification\n```bash\npython -c \"\nimport inspect\nfrom post_reel_smart import SmartInstagramPoster\n\n# All humanize helpers should take only self (no extra params)\nfor name in ['_humanize_scroll_feed', '_humanize_view_story', '_humanize_scroll_reels', '_humanize_check_notifications']:\n    method = getattr(SmartInstagramPoster, name)\n    sig = inspect.signature(method)\n    params = list(sig.parameters.keys())\n    assert params == ['self'], f'{name} should only have self param, got: {params}'\nprint('All helper methods have correct (self-only) signature')\n\"\n```\n\n### 4. Return Type Verification (Static Analysis)\n```bash\n# Check that helpers return bool values\ngrep -A 2 \"def _humanize_\" post_reel_smart.py | grep \"return True\\|return False\"\n# Expected: Should see return True/False in each helper\n```\n\n### 5. Main Method Structure Verification\n```bash\npython -c \"\nimport inspect\nfrom post_reel_smart import SmartInstagramPoster\n\n# Get source of humanize_before_post\nsource = inspect.getsource(SmartInstagramPoster.humanize_before_post)\nlines = source.strip().split('\\n')\nprint(f'humanize_before_post() has {len(lines)} lines')\nassert len(lines) <= 30, f'Expected ~25 lines, got {len(lines)}'\n\n# Should contain dispatch logic, not inline handlers\nassert 'action_handlers' in source or 'handler()' in source, 'Should use dispatch pattern'\nassert 'for _ in range(scroll_count)' not in source, 'Should not have inline scroll loop'\nprint('Main method is properly refactored to dispatch pattern')\n\"\n```\n\n### 6. Live Behavior Test (Integration)\n```bash\n# Run with humanize flag on a test account to verify behavior unchanged\n# Note: This requires a real Geelark phone to be available\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nimport unittest.mock as mock\n\n# Create poster with mocked connection\nposter = SmartInstagramPoster('test_phone')\n\n# Mock the UI controller methods to avoid needing real device\nposter._ui_controller = mock.MagicMock()\nposter._conn.appium_driver = mock.MagicMock()\n\n# Mock dump_ui to return elements that will trigger actions\nposter.dump_ui = mock.MagicMock(return_value=([\n    {'desc': 'unseen story', 'text': '', 'center': (100, 100), 'clickable': True},\n    {'desc': 'reels', 'text': '', 'center': (200, 200), 'clickable': True},\n    {'desc': 'notification', 'text': '', 'center': (300, 300), 'clickable': True},\n    {'desc': 'home', 'text': '', 'center': (50, 50), 'clickable': True},\n], ''))\n\n# Run humanize - should call helper methods\nimport random\nrandom.seed(42)  # Deterministic for testing\nposter.humanize_before_post()\n\n# Verify swipe/tap/press_key were called (humanization happened)\nassert poster._ui_controller.swipe.called or poster._ui_controller.tap.called, 'Humanization should have done something'\nprint('Humanize behavior test passed')\n\"\n```\n\n### 7. Verify Original Behavior Preserved\n```bash\n# Check that random selection and max_actions limit are preserved\ngrep -A 5 \"def humanize_before_post\" post_reel_smart.py | grep -E \"random.choice|max_actions|random.randint\"\n# Expected: Should see random.choice for action selection and random.randint(2, 4) for max_actions\n```\n\n### 8. Full Integration Test (Optional - Requires Live Phone)\n```bash\n# Test with real phone to verify humanization still works\n# Use --humanize flag if posting_scheduler supports it, or test directly:\npython -c \"\n# Only run this with a real test account\n# python post_reel_smart.py test_account test_video.mp4 'test caption' --humanize\nprint('Skip live test - run manually with real phone')\n\"\n```",
        "status": "done",
        "dependencies": [
          "39",
          "40"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T11:26:39.314Z"
      },
      {
        "id": "44",
        "title": "Create PhoneConnector helper for setup scripts",
        "description": "Create a lightweight PhoneConnector class that encapsulates the find→start→enable ADB→connect flow for use by setup scripts, eliminating ~70 lines of duplicated setup_phone() logic in setup_adbkeyboard.py and setup_clipboard_helper.py.",
        "details": "## Current State Analysis\n\nBoth `setup_adbkeyboard.py` and `setup_clipboard_helper.py` contain nearly identical `setup_phone()` logic (lines 42-99 in each file):\n\n**Duplicated pattern in both scripts:**\n```python\n# 1. Find phone (lines 50-64)\nfor page in range(1, 10):\n    result = client.list_phones(page=page, page_size=100)\n    for p in result[\"items\"]:\n        if p[\"serialName\"] == phone_name:\n            phone = p\n            break\n\n# 2. Start phone if needed (lines 69-80)\nif phone[\"status\"] != 0:\n    client.start_phone(phone_id)\n    for i in range(60):\n        time.sleep(2)\n        status = client.get_phone_status([phone_id])\n        ...\n\n# 3. Enable ADB and connect (lines 82-99)\nclient.enable_adb(phone_id)\nadb_info = client.get_adb_info(phone_id)\ndevice = f\"{adb_info['ip']}:{adb_info['port']}\"\nsubprocess.run([ADB_PATH, \"connect\", device])\nadb(device, f\"glogin {password}\")\n```\n\n## Architecture Decision\n\n**Why not use DeviceConnectionManager?**\n- `DeviceConnectionManager` (device_connection.py) is designed for the full posting workflow with Appium\n- It has Appium-specific dependencies (`from appium import webdriver`)\n- Setup scripts don't need Appium - they only need ADB access\n- A lightweight helper avoids pulling in unnecessary dependencies\n\n**Two-tier architecture:**\n- **PhoneConnector** (new): Lightweight ADB-only flow for setup scripts\n- **DeviceConnectionManager** (existing): Full Appium workflow for posting\n\n## Implementation Plan\n\n### 1. Create `phone_connector.py`\n\n```python\n\"\"\"\nLightweight phone connector for setup scripts.\n\nThis provides the basic find→start→ADB enable→connect flow without Appium.\nFor full posting workflow with Appium, use DeviceConnectionManager instead.\n\"\"\"\nimport subprocess\nimport time\nfrom typing import Optional, Tuple\nfrom dataclasses import dataclass\n\nfrom config import Config\nfrom geelark_client import GeelarkClient\n\nADB_PATH = Config.ADB_PATH\n\n\n@dataclass\nclass PhoneConnection:\n    \"\"\"Result of a successful phone connection.\"\"\"\n    client: GeelarkClient\n    phone_id: str\n    phone_name: str\n    device_string: str  # \"ip:port\" format for ADB\n    password: str\n\n\nclass PhoneConnectorError(Exception):\n    \"\"\"Raised when phone connection fails.\"\"\"\n    pass\n\n\nclass PhoneConnector:\n    \"\"\"\n    Lightweight connector for Geelark phones - ADB only, no Appium.\n    \n    For setup scripts that need ADB access but not Appium.\n    For full posting workflow, use DeviceConnectionManager instead.\n    \"\"\"\n    \n    def __init__(self, geelark_client: GeelarkClient = None):\n        \"\"\"\n        Initialize the phone connector.\n        \n        Args:\n            geelark_client: Optional GeelarkClient instance for dependency injection.\n        \"\"\"\n        self.client = geelark_client or GeelarkClient()\n    \n    def find_phone(self, phone_name: str) -> Tuple[str, dict]:\n        \"\"\"\n        Find a phone by name in Geelark.\n        \n        Args:\n            phone_name: The serialName of the phone to find.\n            \n        Returns:\n            Tuple of (phone_id, phone_info_dict)\n            \n        Raises:\n            PhoneConnectorError: If phone not found.\n        \"\"\"\n        print(f\"Finding phone: {phone_name}\")\n        \n        for page in range(1, 10):\n            result = self.client.list_phones(page=page, page_size=100)\n            for p in result[\"items\"]:\n                if p[\"serialName\"] == phone_name:\n                    phone_id = p[\"id\"]\n                    print(f\"  Found: {p['serialName']} (Status: {p['status']})\")\n                    return phone_id, p\n            if len(result[\"items\"]) < 100:\n                break\n        \n        raise PhoneConnectorError(f\"Phone not found: {phone_name}\")\n    \n    def ensure_running(self, phone_id: str, phone_status: int) -> bool:\n        \"\"\"\n        Ensure the phone is running, starting it if necessary.\n        \n        Args:\n            phone_id: The Geelark phone ID.\n            phone_status: Current status (0=running, other=stopped).\n            \n        Returns:\n            True when phone is ready.\n        \"\"\"\n        if phone_status == 0:\n            return True  # Already running\n        \n        print(\"  Starting phone...\")\n        self.client.start_phone(phone_id)\n        \n        for i in range(60):\n            time.sleep(2)\n            status = self.client.get_phone_status([phone_id])\n            items = status.get(\"successDetails\", [])\n            if items and items[0].get(\"status\") == 0:\n                print(f\"    Ready after {(i+1)*2}s\")\n                time.sleep(5)  # Extra stabilization time\n                return True\n        \n        raise PhoneConnectorError(f\"Phone {phone_id} failed to start after 120s\")\n    \n    def connect_adb(self, phone_id: str) -> Tuple[str, str]:\n        \"\"\"\n        Enable ADB and establish connection.\n        \n        Args:\n            phone_id: The Geelark phone ID.\n            \n        Returns:\n            Tuple of (device_string, password) where device_string is \"ip:port\".\n        \"\"\"\n        print(\"  Enabling ADB...\")\n        self.client.enable_adb(phone_id)\n        time.sleep(5)\n        \n        adb_info = self.client.get_adb_info(phone_id)\n        device = f\"{adb_info['ip']}:{adb_info['port']}\"\n        password = adb_info['pwd']\n        \n        print(f\"  Connecting to {device}...\")\n        subprocess.run([ADB_PATH, \"connect\", device], capture_output=True)\n        time.sleep(1)\n        \n        # glogin authentication\n        result = subprocess.run(\n            [ADB_PATH, \"-s\", device, \"shell\", f\"glogin {password}\"],\n            capture_output=True, timeout=30,\n            encoding='utf-8', errors='replace'\n        )\n        login_result = result.stdout.strip() if result.stdout else \"\"\n        print(f\"  Login: {login_result or 'OK'}\")\n        \n        return device, password\n    \n    def setup_for_adb(self, phone_name: str) -> PhoneConnection:\n        \"\"\"\n        Complete setup flow: find → start → enable ADB → connect.\n        \n        This is the main entry point for setup scripts.\n        \n        Args:\n            phone_name: The serialName of the phone to connect.\n            \n        Returns:\n            PhoneConnection with all connection details.\n            \n        Raises:\n            PhoneConnectorError: On any failure.\n        \"\"\"\n        phone_id, phone_info = self.find_phone(phone_name)\n        self.ensure_running(phone_id, phone_info[\"status\"])\n        device, password = self.connect_adb(phone_id)\n        \n        return PhoneConnection(\n            client=self.client,\n            phone_id=phone_id,\n            phone_name=phone_name,\n            device_string=device,\n            password=password\n        )\n```\n\n### 2. Update `setup_adbkeyboard.py`\n\nReplace lines 42-99 with:\n\n```python\ndef setup_phone(phone_name):\n    \"\"\"Setup ADBKeyboard on a single phone\"\"\"\n    from phone_connector import PhoneConnector, PhoneConnectorError\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"Setting up ADBKeyboard on: {phone_name}\")\n    print('='*50)\n    \n    try:\n        connector = PhoneConnector()\n        conn = connector.setup_for_adb(phone_name)\n        device = conn.device_string\n    except PhoneConnectorError as e:\n        print(f\"  ERROR: {e}\")\n        return False\n    \n    # Force uninstall first (clean slate)\n    print(\"  Uninstalling existing ADBKeyboard (if any)...\")\n    uninstall_result = adb(device, \"pm uninstall com.android.adbkeyboard\")\n    print(f\"    {uninstall_result or 'Not installed'}\")\n    time.sleep(1)\n    \n    # ... rest of ADBKeyboard-specific logic (lines 107-131)\n```\n\n### 3. Update `setup_clipboard_helper.py`\n\nReplace lines 42-99 with:\n\n```python\ndef setup_phone(phone_name):\n    \"\"\"Setup ClipboardHelper on a single phone\"\"\"\n    from phone_connector import PhoneConnector, PhoneConnectorError\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"Setting up ClipboardHelper on: {phone_name}\")\n    print('='*50)\n    \n    try:\n        connector = PhoneConnector()\n        conn = connector.setup_for_adb(phone_name)\n        device = conn.device_string\n    except PhoneConnectorError as e:\n        print(f\"  ERROR: {e}\")\n        return False\n    \n    # Check if already installed\n    print(\"  Checking if ClipboardHelper is installed...\")\n    packages = adb(device, \"pm list packages | grep geelark.clipboard\")\n    # ... rest of ClipboardHelper-specific logic (lines 104-129)\n```\n\n## Key Design Decisions\n\n1. **Separate module, not in DeviceConnectionManager**: Keeps Appium dependency isolated\n2. **PhoneConnection dataclass**: Clean return type with all connection details\n3. **PhoneConnectorError exception**: Specific error handling without polluting DeviceConnectionError\n4. **Dependency injection**: Optional GeelarkClient parameter for testing\n5. **Idempotent**: Can be called multiple times safely (uses existing running phone)",
        "testStrategy": "## Test Strategy\n\n### 1. Unit Test - Module imports successfully\n```bash\npython -c \"from phone_connector import PhoneConnector, PhoneConnection, PhoneConnectorError; print('Import OK')\"\n```\n\n### 2. Unit Test - PhoneConnector instantiation\n```bash\npython -c \"\nfrom phone_connector import PhoneConnector\nconnector = PhoneConnector()\nprint(f'Client type: {type(connector.client).__name__}')\nprint('Instantiation OK')\n\"\n```\n\n### 3. Integration Test - Find phone (read-only, safe)\n```bash\npython -c \"\nfrom phone_connector import PhoneConnector\nconnector = PhoneConnector()\n\n# Use a known test phone name from accounts.txt\nphone_id, info = connector.find_phone('reelwisdompod_')\nprint(f'Found: {info[\\\"serialName\\\"]} (ID: {phone_id})')\n\"\n```\n\n### 4. Integration Test - Full setup_for_adb flow\n```bash\n# Test with a real phone (will start if needed - costs minutes)\npython -c \"\nfrom phone_connector import PhoneConnector\nconnector = PhoneConnector()\nconn = connector.setup_for_adb('reelwisdompod_')\nprint(f'Connected to: {conn.device_string}')\nprint(f'Phone ID: {conn.phone_id}')\n\"\n```\n\n### 5. End-to-End Test - setup_adbkeyboard.py still works\n```bash\n# Test that the refactored script behaves identically\npython setup_adbkeyboard.py reelwisdompod_\n\n# Verify ADBKeyboard is enabled\nadb -s <device> shell settings get secure default_input_method\n# Should show: com.android.adbkeyboard/.AdbIME\n```\n\n### 6. End-to-End Test - setup_clipboard_helper.py still works\n```bash\n# Test that the refactored script behaves identically\npython setup_clipboard_helper.py reelwisdompod_\n\n# Verify ClipboardHelper is installed\nadb -s <device> shell pm list packages | grep clipboard\n# Should show: package:com.geelark.clipboard\n```\n\n### 7. Verify Code Reduction\n```bash\n# Before: Count lines in setup_phone() functions\n# setup_adbkeyboard.py: lines 42-131 = ~90 lines\n# setup_clipboard_helper.py: lines 42-129 = ~88 lines\n\n# After: Each setup_phone() should be ~30-40 lines (APK-specific logic only)\n# phone_connector.py: ~120 lines (shared by all setup scripts)\n# Net reduction: ~60 lines duplicated code eliminated\n```\n\n### 8. Verify No Appium Dependency\n```bash\n# PhoneConnector should not import Appium\npython -c \"\nimport ast\nwith open('phone_connector.py', 'r') as f:\n    tree = ast.parse(f.read())\nimports = [node.names[0].name for node in ast.walk(tree) if isinstance(node, ast.Import)]\nfrom_imports = [node.module for node in ast.walk(tree) if isinstance(node, ast.ImportFrom)]\nall_imports = imports + [m for m in from_imports if m]\nassert 'appium' not in str(all_imports).lower(), 'PhoneConnector should not import Appium!'\nprint('No Appium dependency - OK')\n\"\n```\n\n### 9. Error Handling Test\n```bash\n# Test with non-existent phone\npython -c \"\nfrom phone_connector import PhoneConnector, PhoneConnectorError\nconnector = PhoneConnector()\ntry:\n    connector.find_phone('nonexistent_phone_12345')\n    print('ERROR: Should have raised PhoneConnectorError')\nexcept PhoneConnectorError as e:\n    print(f'Correctly raised PhoneConnectorError: {e}')\n\"\n```\n\n### 10. Stop phone after testing (CRITICAL)\n```bash\n# ALWAYS stop phones after testing to save billing minutes\npython -c \"\nfrom geelark_client import GeelarkClient\nclient = GeelarkClient()\nfor page in range(1, 20):\n    result = client.list_phones(page=page, page_size=100)\n    for phone in result['items']:\n        if phone['status'] == 1:\n            client.stop_phone(phone['id'])\n            print(f'STOPPED: {phone[\\\"serialName\\\"]}')\n    if len(result['items']) < 100:\n        break\n\"\n```",
        "status": "done",
        "dependencies": [
          "25",
          "31",
          "37"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T11:35:07.353Z"
      },
      {
        "id": "45",
        "title": "Consolidate ADB helper functions into DeviceConnectionManager",
        "description": "Move the standalone ADB helper functions (wait_for_adb, ensure_device_alive, reconnect_adb) from parallel_worker.py into DeviceConnectionManager as class methods, providing a single source for all ADB-related operations and eliminating ~105 lines of duplicated code.",
        "details": "## Current State Analysis\n\n### Duplicated Functions in parallel_worker.py (lines 69-173):\n```python\n# wait_for_adb(device_id, timeout=90, logger=None) -> bool (lines 69-107)\n# - Polls ADB devices list until device appears\n# - Returns True when device shows as \"device\" (not \"offline\")\n# - Used before Appium session creation\n\n# ensure_device_alive(device_id, logger=None) -> bool (lines 110-139)\n# - Single check if device is in ADB devices list\n# - Returns True if device is present and not offline\n# - Used for health checks during job execution\n\n# reconnect_adb(device_id, logger=None) -> bool (lines 142-173)\n# - Disconnects and reconnects ADB to device\n# - Returns True if reconnection successful\n# - Used for recovery from dropped connections\n```\n\n### Similar Methods Already in DeviceConnectionManager:\n- `_wait_for_device_ready()` (lines 198-218) - similar to wait_for_adb but instance-based\n- `verify_adb_connection()` (lines 238-246) - similar to ensure_device_alive\n- `reconnect_adb()` (lines 248-278) - instance-based, fetches password from Geelark\n\n## Implementation Plan\n\n### Step 1: Add Static/Class Methods to DeviceConnectionManager\n\nAdd these as **static methods** (don't require self) for device-agnostic operations:\n\n```python\n# device_connection.py - add after existing imports\n\n@staticmethod\ndef wait_for_device(device_id: str, timeout: int = 90, logger=None) -> bool:\n    \"\"\"\n    Wait for a device to appear in ADB devices list.\n    \n    This is the explicit ADB readiness gate - call AFTER starting phone\n    but BEFORE creating Appium session.\n    \n    Args:\n        device_id: Device identifier (e.g., \"192.168.1.100:5555\")\n        timeout: Maximum seconds to wait (default 90)\n        logger: Optional logger for status updates\n    \n    Returns:\n        True if device is ready, False on timeout\n    \"\"\"\n    deadline = time.time() + timeout\n    check_count = 0\n    \n    while time.time() < deadline:\n        check_count += 1\n        try:\n            result = subprocess.run(\n                [ADB_PATH, \"devices\"],\n                capture_output=True, text=True, timeout=10\n            )\n            for line in result.stdout.splitlines():\n                if device_id in line and \"device\" in line and \"offline\" not in line:\n                    if logger:\n                        logger.info(f\"ADB ready for {device_id} (took {check_count * 2}s)\")\n                    return True\n        except Exception as e:\n            if logger:\n                logger.debug(f\"ADB check error: {e}\")\n        \n        time.sleep(2)\n    \n    if logger:\n        logger.error(f\"ADB timeout ({timeout}s) waiting for {device_id}\")\n    return False\n\n@staticmethod\ndef is_device_alive(device_id: str, logger=None) -> bool:\n    \"\"\"\n    Check if a device is present in ADB devices list.\n    \n    Call periodically during job execution to detect device loss.\n    \n    Args:\n        device_id: Device identifier (e.g., \"192.168.1.100:5555\")\n        logger: Optional logger for status updates\n    \n    Returns:\n        True if device is alive, False if lost\n    \"\"\"\n    try:\n        result = subprocess.run(\n            [ADB_PATH, \"devices\"],\n            capture_output=True, text=True, timeout=10\n        )\n        for line in result.stdout.splitlines():\n            if device_id in line and \"device\" in line and \"offline\" not in line:\n                return True\n        if logger:\n            logger.warning(f\"Device {device_id} not found in ADB devices\")\n        return False\n    except Exception as e:\n        if logger:\n            logger.warning(f\"ADB devices check failed: {e}\")\n        return False\n\n@staticmethod\ndef reconnect_device(device_id: str, logger=None) -> bool:\n    \"\"\"\n    Attempt to reconnect an ADB device (disconnect + connect).\n    \n    Args:\n        device_id: Device identifier (e.g., \"192.168.1.100:5555\")\n        logger: Optional logger for status updates\n    \n    Returns:\n        True if reconnect successful, False otherwise\n    \"\"\"\n    try:\n        # First disconnect\n        subprocess.run([ADB_PATH, \"disconnect\", device_id],\n                      capture_output=True, timeout=10)\n        \n        # Then reconnect\n        result = subprocess.run([ADB_PATH, \"connect\", device_id],\n                               capture_output=True, text=True, timeout=30)\n        \n        if \"connected\" in result.stdout.lower():\n            if logger:\n                logger.info(f\"Reconnected ADB to {device_id}\")\n            return True\n        else:\n            if logger:\n                logger.warning(f\"ADB reconnect failed: {result.stdout}\")\n            return False\n    except Exception as e:\n        if logger:\n            logger.warning(f\"ADB reconnect error: {e}\")\n        return False\n```\n\n### Step 2: Refactor Existing Instance Methods to Use Static Methods\n\nUpdate existing instance methods to delegate to the new static methods:\n\n```python\n# Update _wait_for_device_ready to use wait_for_device\ndef _wait_for_device_ready(self, max_attempts: int = 30) -> None:\n    \"\"\"Wait for device to appear in ADB devices list.\"\"\"\n    timeout = max_attempts * 2  # Each check is ~2 seconds\n    if not DeviceConnectionManager.wait_for_device(self.device, timeout):\n        raise Exception(f\"Device {self.device} never appeared in ADB devices list after {timeout}s\")\n\n# Update verify_adb_connection to use is_device_alive\ndef verify_adb_connection(self) -> bool:\n    \"\"\"Verify device is still connected via ADB.\"\"\"\n    return DeviceConnectionManager.is_device_alive(self.device)\n```\n\n### Step 3: Update parallel_worker.py to Import and Use DeviceConnectionManager\n\n```python\n# parallel_worker.py - change imports\nfrom device_connection import DeviceConnectionManager\n\n# Remove the three standalone functions (lines 69-173)\n# Replace all usages:\n\n# Old: wait_for_adb(device_id, timeout, logger)\n# New: DeviceConnectionManager.wait_for_device(device_id, timeout, logger)\n\n# Old: ensure_device_alive(device_id, logger)  \n# New: DeviceConnectionManager.is_device_alive(device_id, logger)\n\n# Old: reconnect_adb(device_id, logger)\n# New: DeviceConnectionManager.reconnect_device(device_id, logger)\n```\n\n### Step 4: Update Any Other Files Using These Functions\n\nSearch for other files importing these functions and update them similarly.\n\n## Key Design Decisions\n\n1. **Static methods vs instance methods**: Using static methods because these operations don't require instance state - they work on any device ID. This allows parallel_worker.py to call them without instantiating DeviceConnectionManager.\n\n2. **Naming conventions**:\n   - `wait_for_device()` - more general than `wait_for_adb()` \n   - `is_device_alive()` - clearer than `ensure_device_alive()`\n   - `reconnect_device()` - consistent with existing naming\n\n3. **Logger parameter**: Keep optional logger parameter for worker process logging integration.\n\n4. **Backward compatibility**: Existing instance methods (`verify_adb_connection`, `reconnect_adb`) continue to work but delegate to static methods internally.",
        "testStrategy": "## Test Strategy\n\n### 1. Verify Static Methods Import and Work Standalone\n```bash\npython -c \"\nfrom device_connection import DeviceConnectionManager\n\n# Test that static methods exist and are callable\nprint('wait_for_device:', callable(DeviceConnectionManager.wait_for_device))\nprint('is_device_alive:', callable(DeviceConnectionManager.is_device_alive))\nprint('reconnect_device:', callable(DeviceConnectionManager.reconnect_device))\n\n# Test with fake device (should return False, not crash)\nresult = DeviceConnectionManager.is_device_alive('192.168.99.99:5555')\nprint(f'is_device_alive for fake device: {result}')\nassert result == False, 'Should return False for non-existent device'\nprint('All static method tests passed!')\n\"\n```\n\n### 2. Verify parallel_worker.py Imports Successfully\n```bash\npython -c \"\nfrom parallel_worker import run_worker, setup_worker_logging\nfrom device_connection import DeviceConnectionManager\nprint('Import successful - no standalone ADB functions should exist')\n\n# Verify old functions don't exist at module level\nimport parallel_worker\nassert not hasattr(parallel_worker, 'wait_for_adb'), 'wait_for_adb should be removed'\nassert not hasattr(parallel_worker, 'ensure_device_alive'), 'ensure_device_alive should be removed'\nassert not hasattr(parallel_worker, 'reconnect_adb'), 'reconnect_adb should be removed'\nprint('Old functions properly removed!')\n\"\n```\n\n### 3. Verify Instance Methods Still Work\n```bash\npython -c \"\nfrom device_connection import DeviceConnectionManager\n\n# Create instance (won't connect, just verify method exists)\nmanager = DeviceConnectionManager('test_phone')\n\n# Verify instance methods exist and are callable\nassert callable(manager.verify_adb_connection), 'Instance method should exist'\nprint('Instance methods verified!')\n\"\n```\n\n### 4. Line Count Verification\n```bash\n# Before: Count lines in parallel_worker.py\nwc -l parallel_worker.py\n# Should be ~550 lines\n\n# After: Should be ~445 lines (105 lines removed)\n# The functions removed span lines 69-173 (105 lines)\n```\n\n### 5. Integration Test - Run Worker Startup\n```bash\n# Run parallel_worker.py in dry-run mode to verify imports work\npython parallel_worker.py --worker-id 0 --help\n# Should show help without import errors\n```\n\n### 6. Full Integration Test (with actual phones)\n```bash\n# Test full posting flow works with consolidated ADB operations\npython parallel_orchestrator.py --workers 1 --run\n\n# Monitor logs for:\n# - \"ADB ready for\" messages (from wait_for_device)\n# - No import errors\n# - Jobs complete successfully\n```\n\n### 7. Verify Code Deduplication\n```bash\n# Search for duplicate ADB patterns\ngrep -n \"ADB_PATH.*devices\" parallel_worker.py device_connection.py\n# Should only find matches in device_connection.py, not parallel_worker.py\n```",
        "status": "done",
        "dependencies": [
          "40",
          "25",
          "37"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T11:37:56.832Z"
      },
      {
        "id": "46",
        "title": "Convert _classify_error to dict-based pattern lookup table",
        "description": "Refactor the _classify_error() method in progress_tracker.py from a 5-condition if/elif chain to a Strategy pattern using an ERROR_PATTERNS dict that maps error types to lists of matching patterns, improving maintainability and extensibility.",
        "details": "## Current State Analysis\n\nThe `_classify_error()` method in `progress_tracker.py` (lines 541-560) uses an if/elif chain with 5 conditions:\n\n```python\ndef _classify_error(self, error: str) -> str:\n    error_lower = error.lower() if error else ''\n    \n    if 'suspended' in error_lower or 'account has been suspended' in error_lower:\n        return 'suspended'\n    elif 'captcha' in error_lower or 'verify' in error_lower:\n        return 'captcha'\n    elif 'log in' in error_lower or 'logged out' in error_lower or 'sign up' in error_lower:\n        return 'loggedout'\n    elif 'action blocked' in error_lower or 'try again later' in error_lower:\n        return 'actionblocked'\n    elif 'banned' in error_lower or 'disabled' in error_lower:\n        return 'banned'\n    else:\n        return ''  # Retryable\n```\n\n## Implementation Plan\n\n### Step 1: Define ERROR_PATTERNS class constant\n\nAdd a new class constant after `NON_RETRYABLE_ERRORS` (line 93):\n\n```python\n# Non-retryable error types - these failures should not be retried\nNON_RETRYABLE_ERRORS = {'suspended', 'captcha', 'loggedout', 'actionblocked', 'banned'}\n\n# Error classification patterns - maps error_type to list of substrings to match\n# Order matters: first matching error type wins\nERROR_PATTERNS = {\n    'suspended': ['suspended', 'account has been suspended'],\n    'captcha': ['captcha', 'verify'],\n    'loggedout': ['log in', 'logged out', 'sign up'],\n    'actionblocked': ['action blocked', 'try again later'],\n    'banned': ['banned', 'disabled'],\n}\n```\n\n### Step 2: Refactor _classify_error() method\n\nReplace the if/elif chain with a dict-based lookup:\n\n```python\ndef _classify_error(self, error: str) -> str:\n    \"\"\"\n    Classify an error message into an error type.\n\n    Uses ERROR_PATTERNS dict for pattern matching. Returns the first\n    matching error type from NON_RETRYABLE_ERRORS, or empty string\n    for retryable errors.\n    \"\"\"\n    if not error:\n        return ''\n    \n    error_lower = error.lower()\n    \n    for error_type, patterns in self.ERROR_PATTERNS.items():\n        if any(pattern in error_lower for pattern in patterns):\n            return error_type\n    \n    return ''  # Retryable\n```\n\n### Step 3: Ensure consistency between ERROR_PATTERNS and NON_RETRYABLE_ERRORS\n\nAdd a validation assertion in `__init__` (optional but recommended):\n\n```python\ndef __init__(self, progress_file: str, lock_timeout: float = 30.0):\n    # Validate ERROR_PATTERNS keys match NON_RETRYABLE_ERRORS\n    assert set(self.ERROR_PATTERNS.keys()) == self.NON_RETRYABLE_ERRORS, \\\n        f\"ERROR_PATTERNS keys must match NON_RETRYABLE_ERRORS\"\n    # ... rest of __init__\n```\n\n## Benefits\n\n1. **Easier to add new error types**: Add a single line to ERROR_PATTERNS dict\n2. **Self-documenting**: The dict clearly shows all patterns for each error type\n3. **Maintainable**: Patterns are grouped by error type, not scattered in elif branches\n4. **DRY**: Error types are defined once in ERROR_PATTERNS, used via iteration\n5. **Testable**: Can easily test individual patterns without mocking the whole method\n\n## Location\n\n- File: `progress_tracker.py`\n- Lines to modify: 93-97 (add ERROR_PATTERNS), 541-560 (refactor method)",
        "testStrategy": "## Test Strategy\n\n### 1. Syntax and Import Verification\n```bash\npython -c \"from progress_tracker import ProgressTracker; print('Import successful')\"\n```\n\n### 2. Verify ERROR_PATTERNS Constant Exists\n```bash\npython -c \"\nfrom progress_tracker import ProgressTracker\nprint('ERROR_PATTERNS:', ProgressTracker.ERROR_PATTERNS)\nprint('Keys match NON_RETRYABLE_ERRORS:', set(ProgressTracker.ERROR_PATTERNS.keys()) == ProgressTracker.NON_RETRYABLE_ERRORS)\n\"\n```\n\n### 3. Unit Test All Error Classifications\n```bash\npython -c \"\nfrom progress_tracker import ProgressTracker\n\ntracker = ProgressTracker('test_progress.csv')\n\n# Test each error type with various patterns\ntest_cases = [\n    # Suspended\n    ('Your account has been suspended', 'suspended'),\n    ('Account suspended', 'suspended'),\n    \n    # Captcha\n    ('Please complete the captcha', 'captcha'),\n    ('Verify your identity', 'captcha'),\n    \n    # Logged out\n    ('Please log in to continue', 'loggedout'),\n    ('You have been logged out', 'loggedout'),\n    ('Sign up to continue', 'loggedout'),\n    \n    # Action blocked\n    ('Action blocked. Please try again later', 'actionblocked'),\n    ('Try again later', 'actionblocked'),\n    \n    # Banned\n    ('Your account has been banned', 'banned'),\n    ('Account disabled for violating terms', 'banned'),\n    \n    # Retryable (empty string)\n    ('Connection timeout', ''),\n    ('Network error', ''),\n    ('', ''),\n    (None, ''),\n]\n\nall_passed = True\nfor error_msg, expected in test_cases:\n    result = tracker._classify_error(error_msg)\n    status = '✓' if result == expected else '✗'\n    if result != expected:\n        all_passed = False\n    print(f'{status} \\\"{error_msg}\\\" -> \\\"{result}\\\" (expected \\\"{expected}\\\")')\n\nprint(f'\\nAll tests passed: {all_passed}')\n\n# Cleanup\nimport os\nif os.path.exists('test_progress.csv'):\n    os.remove('test_progress.csv')\nif os.path.exists('test_progress.csv.lock'):\n    os.remove('test_progress.csv.lock')\n\"\n```\n\n### 4. Integration Test with update_job_status()\n```bash\npython -c \"\nfrom progress_tracker import ProgressTracker\nimport os\n\ntracker = ProgressTracker('test_integration.csv')\n\n# Seed a test job\ntracker.seed_jobs([{\n    'job_id': 'test_job_1',\n    'account': 'test_account',\n    'video_path': '/fake/video.mp4',\n    'caption': 'Test caption'\n}])\n\n# Claim the job\njob = tracker.claim_next_job(worker_id=0)\nprint(f'Claimed job: {job[\\\"job_id\\\"]}')\n\n# Fail with a non-retryable error\ntracker.update_job_status('test_job_1', 'failed', worker_id=0, error='Account suspended')\n\n# Verify error_type was set correctly\njobs = tracker._read_all_jobs()\njob = next(j for j in jobs if j['job_id'] == 'test_job_1')\nprint(f'Status: {job[\\\"status\\\"]}')\nprint(f'Error type: {job[\\\"error_type\\\"]}')\nassert job['error_type'] == 'suspended', f'Expected suspended, got {job[\\\"error_type\\\"]}'\nprint('Integration test passed!')\n\n# Cleanup\nfor f in ['test_integration.csv', 'test_integration.csv.lock']:\n    if os.path.exists(f):\n        os.remove(f)\n\"\n```\n\n### 5. Verify No Regression in Live System\n```bash\n# Check current progress file still works\npython -c \"\nfrom progress_tracker import ProgressTracker\ntracker = ProgressTracker('parallel_progress.csv')\nstats = tracker.get_statistics()\nprint(f'Progress file loads correctly: {stats}')\n\"\n```",
        "status": "done",
        "dependencies": [
          "40"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T11:49:23.905Z"
      },
      {
        "id": "47",
        "title": "Convert action dispatch if/elif chain to ACTION_HANDLERS dispatch table",
        "description": "Refactor the 8-action if/elif chain in the post() method (lines 801-854) to use an ACTION_HANDLERS class constant dict mapping action names to handler methods, following the Command pattern established in Task 43's humanize dispatch table.",
        "details": "## Current State Analysis\n\nThe `post()` method in `post_reel_smart.py` (lines 801-854) contains an 8-condition if/elif chain for dispatching actions:\n\n```python\n# Execute action\nif action['action'] == 'done':\n    print(\"\\n[SUCCESS] Share initiated!\")\n    # ... 10 lines of success handling\n    return True\n\nelif action['action'] == 'home':\n    print(\"  [HOME] Going to home screen...\")\n    self.press_key('KEYCODE_HOME')\n    time.sleep(2)\n\nelif action['action'] == 'open_instagram':\n    print(\"  [OPEN] Opening Instagram...\")\n    self.adb(\"am force-stop com.instagram.android\")\n    time.sleep(1)\n    self.adb(\"monkey -p com.instagram.android 1\")\n    time.sleep(4)\n\nelif action['action'] == 'tap':\n    idx = action.get('element_index', 0)\n    if 0 <= idx < len(elements):\n        elem = elements[idx]\n        self.tap(elem['center'][0], elem['center'][1])\n    else:\n        print(f\"  Invalid element index: {idx}\")\n\nelif action['action'] == 'tap_and_type':\n    if self._handle_tap_and_type(action, elements, caption):\n        continue  # Helper handled it and wants to skip to next step\n\nelif action['action'] == 'back':\n    self.press_key('KEYCODE_BACK')\n\nelif action['action'] == 'scroll_down':\n    self.adb(\"input swipe 360 900 360 400 300\")\n\nelif action['action'] == 'scroll_up':\n    self.adb(\"input swipe 360 400 360 900 300\")\n```\n\n## Target Implementation Pattern\n\nFollow Task 43's pattern (lines 245-251 in `humanize_before_post()`):\n\n```python\n# Dispatch table for humanize actions\naction_handlers = {\n    'scroll_feed': self._humanize_scroll_feed,\n    'view_story': self._humanize_view_story,\n    'scroll_reels': self._humanize_scroll_reels,\n    'check_notifications': self._humanize_check_notifications,\n}\n```\n\n## Implementation Steps\n\n### Step 1: Create Handler Methods\n\nExtract each action into a private handler method. Handlers will receive context via a dataclass:\n\n```python\n@dataclass\nclass ActionContext:\n    \"\"\"Context passed to action handlers during post() execution.\"\"\"\n    action: Dict[str, Any]\n    elements: List[Dict]\n    caption: str\n    humanize: bool\n\nclass SmartInstagramPoster:\n    # ... existing code ...\n    \n    def _action_done(self, ctx: ActionContext) -> Optional[bool]:\n        \"\"\"Handle 'done' action - posting complete.\"\"\"\n        print(\"\\n[SUCCESS] Share initiated!\")\n        if self.wait_for_upload_complete(timeout=60):\n            print(\"[SUCCESS] Upload confirmed complete!\")\n        else:\n            print(\"[WARNING] Upload confirmation timeout - may still be processing\")\n        if ctx.humanize:\n            self.humanize_after_post()\n        return True  # Return value signals post() to return True\n    \n    def _action_home(self, ctx: ActionContext) -> None:\n        \"\"\"Handle 'home' action - go to home screen.\"\"\"\n        print(\"  [HOME] Going to home screen...\")\n        self.press_key('KEYCODE_HOME')\n        time.sleep(2)\n    \n    def _action_open_instagram(self, ctx: ActionContext) -> None:\n        \"\"\"Handle 'open_instagram' action - restart Instagram app.\"\"\"\n        print(\"  [OPEN] Opening Instagram...\")\n        self.adb(\"am force-stop com.instagram.android\")\n        time.sleep(1)\n        self.adb(\"monkey -p com.instagram.android 1\")\n        time.sleep(4)\n    \n    def _action_tap(self, ctx: ActionContext) -> None:\n        \"\"\"Handle 'tap' action - tap an element by index.\"\"\"\n        idx = ctx.action.get('element_index', 0)\n        if 0 <= idx < len(ctx.elements):\n            elem = ctx.elements[idx]\n            self.tap(elem['center'][0], elem['center'][1])\n        else:\n            print(f\"  Invalid element index: {idx}\")\n    \n    def _action_tap_and_type(self, ctx: ActionContext) -> Optional[str]:\n        \"\"\"Handle 'tap_and_type' action - tap field and type caption.\"\"\"\n        if self._handle_tap_and_type(ctx.action, ctx.elements, ctx.caption):\n            return 'continue'  # Signal to skip to next iteration\n        return None\n    \n    def _action_back(self, ctx: ActionContext) -> None:\n        \"\"\"Handle 'back' action - press back key.\"\"\"\n        self.press_key('KEYCODE_BACK')\n    \n    def _action_scroll_down(self, ctx: ActionContext) -> None:\n        \"\"\"Handle 'scroll_down' action - swipe up to scroll down.\"\"\"\n        self.adb(\"input swipe 360 900 360 400 300\")\n    \n    def _action_scroll_up(self, ctx: ActionContext) -> None:\n        \"\"\"Handle 'scroll_up' action - swipe down to scroll up.\"\"\"\n        self.adb(\"input swipe 360 400 360 900 300\")\n```\n\n### Step 2: Define ACTION_HANDLERS Dispatch Table\n\nCreate a class-level constant mapping action names to handler methods:\n\n```python\nclass SmartInstagramPoster:\n    # Class constant for action dispatch (Command pattern)\n    # Keys match action names from ClaudeUIAnalyzer (claude_analyzer.py:103)\n    ACTION_HANDLERS = {\n        'done': '_action_done',\n        'home': '_action_home', \n        'open_instagram': '_action_open_instagram',\n        'tap': '_action_tap',\n        'tap_and_type': '_action_tap_and_type',\n        'back': '_action_back',\n        'scroll_down': '_action_scroll_down',\n        'scroll_up': '_action_scroll_up',\n    }\n```\n\nNote: Use method name strings since we can't reference instance methods at class definition time.\n\n### Step 3: Refactor post() Method\n\nReplace the if/elif chain with dispatch table lookup:\n\n```python\ndef post(self, video_path, caption, max_steps=30, humanize=False):\n    # ... existing setup code (lines 722-800) ...\n    \n    # Create context for handlers\n    ctx = ActionContext(\n        action=action,\n        elements=elements,\n        caption=caption,\n        humanize=humanize\n    )\n    \n    # Dispatch action using handler table\n    action_name = action['action']\n    handler_name = self.ACTION_HANDLERS.get(action_name)\n    \n    if handler_name is None:\n        print(f\"  Unknown action: {action_name}\")\n        time.sleep(1)\n        continue\n    \n    # Get and call handler method\n    handler = getattr(self, handler_name)\n    result = handler(ctx)\n    \n    # Handle special return values\n    if result is True:\n        return True  # 'done' handler signals success\n    elif result is False:\n        return False  # Handler signals failure\n    elif result == 'continue':\n        continue  # Skip to next loop iteration\n    \n    # ... existing loop detection code (lines 846-854) ...\n```\n\n### Step 4: Handle ActionContext Import\n\nAdd the dataclass import and definition at the top of the file:\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Any, Optional\n\n@dataclass\nclass ActionContext:\n    \"\"\"Context passed to action handlers during post() execution.\"\"\"\n    action: Dict[str, Any]\n    elements: List[Dict]\n    caption: str\n    humanize: bool\n```\n\n## Benefits\n\n1. **Consistency**: Matches the dispatch table pattern from Task 43 (`humanize_before_post()`)\n2. **Maintainability**: Adding new actions requires only: (1) add handler method, (2) add entry to dict\n3. **Testability**: Each handler method can be unit tested independently\n4. **Readability**: The `post()` method becomes shorter and clearer\n5. **Extensibility**: Easy to add new actions without modifying dispatch logic\n6. **Self-documenting**: The ACTION_HANDLERS dict serves as documentation of supported actions\n\n## Files Modified\n\n- `post_reel_smart.py`: Add ActionContext dataclass, 8 handler methods, ACTION_HANDLERS constant, refactor post() dispatch logic\n\n## Estimated Line Changes\n\n- Remove: ~53 lines (if/elif chain)\n- Add: ~70 lines (dataclass + 8 handlers + dict + dispatch logic)\n- Net: +17 lines, but much better organization",
        "testStrategy": "## Test Strategy\n\n### 1. Syntax and Import Verification\n```bash\n# Verify the file has no syntax errors and imports correctly\npython -c \"from post_reel_smart import SmartInstagramPoster; print('Import successful')\"\n```\n\n### 2. Verify ACTION_HANDLERS Constant Exists\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nprint('ACTION_HANDLERS:', SmartInstagramPoster.ACTION_HANDLERS)\nprint('Keys:', list(SmartInstagramPoster.ACTION_HANDLERS.keys()))\nexpected = ['done', 'home', 'open_instagram', 'tap', 'tap_and_type', 'back', 'scroll_down', 'scroll_up']\nassert set(SmartInstagramPoster.ACTION_HANDLERS.keys()) == set(expected), 'Missing handlers!'\nprint('All 8 handlers present')\n\"\n```\n\n### 3. Verify Handler Methods Exist\n```bash\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nposter = SmartInstagramPoster.__new__(SmartInstagramPoster)  # Create without __init__\nhandlers = ['_action_done', '_action_home', '_action_open_instagram', '_action_tap', \n            '_action_tap_and_type', '_action_back', '_action_scroll_down', '_action_scroll_up']\nfor handler in handlers:\n    assert hasattr(poster, handler), f'Missing handler: {handler}'\n    assert callable(getattr(poster, handler)), f'Handler not callable: {handler}'\nprint('All 8 handler methods exist and are callable')\n\"\n```\n\n### 4. Verify ActionContext Dataclass\n```bash\npython -c \"\nfrom post_reel_smart import ActionContext\nctx = ActionContext(\n    action={'action': 'tap', 'element_index': 0},\n    elements=[{'center': (100, 200)}],\n    caption='Test caption',\n    humanize=False\n)\nprint('ActionContext created:', ctx)\nprint('action:', ctx.action)\nprint('elements:', ctx.elements)\nprint('caption:', ctx.caption)\nprint('humanize:', ctx.humanize)\n\"\n```\n\n### 5. Static Analysis - No if/elif Chain for Actions\n```bash\n# Verify the old if/elif chain is removed from post()\npython -c \"\nimport inspect\nfrom post_reel_smart import SmartInstagramPoster\nsource = inspect.getsource(SmartInstagramPoster.post)\n# Should NOT have the old pattern\nassert \\\"elif action['action'] == 'back'\\\" not in source, 'Old if/elif chain still present!'\nassert \\\"elif action['action'] == 'scroll_down'\\\" not in source, 'Old if/elif chain still present!'\n# SHOULD have new dispatch pattern\nassert 'ACTION_HANDLERS' in source or 'handler_name' in source, 'New dispatch pattern not found!'\nprint('Dispatch table pattern confirmed')\n\"\n```\n\n### 6. Integration Test - Full Posting Flow (Dry Run)\n```bash\n# Test with a mock scenario to verify dispatch works\n# This requires the phone infrastructure but verifies the refactor\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\n\n# Check that the class can be instantiated (basic sanity)\ntry:\n    poster = SmartInstagramPoster('test_phone')\n    # Verify ACTION_HANDLERS is accessible\n    assert hasattr(poster, 'ACTION_HANDLERS')\n    # Verify all handler methods resolve\n    for action_name, handler_name in poster.ACTION_HANDLERS.items():\n        handler = getattr(poster, handler_name)\n        print(f'{action_name} -> {handler_name} OK')\nexcept Exception as e:\n    # May fail due to missing credentials/phone, but dispatch should be configured\n    print(f'Expected init error (no phone): {type(e).__name__}')\n\"\n```\n\n### 7. Live Test (Full Integration)\n```bash\n# Run an actual post to verify behavior is identical\n# Use a test account and video\npython post_reel_smart.py <test_phone> <test_video.mp4> \"Test caption #test\"\n```\n\n### 8. Verify Parallel Orchestrator Still Works\n```bash\n# The orchestrator uses SmartInstagramPoster internally\npython parallel_orchestrator.py --status\n```\n\n### 9. Code Quality Checks\n```bash\n# Check for any remaining hardcoded action strings in dispatch area\ngrep -n \"action\\['action'\\] ==\" post_reel_smart.py | head -20\n# Should only show the handler return value checks, not the old dispatch\n```\n\n### Success Criteria\n1. All 8 actions in ACTION_HANDLERS constant\n2. All 8 handler methods (_action_*) exist and are callable\n3. ActionContext dataclass properly stores all fields\n4. No if/elif chain for action dispatch remains\n5. Same behavior: posting works identically before and after refactor\n6. Existing tests pass (parallel_orchestrator.py --status)\n7. Live post test succeeds with same output pattern",
        "status": "done",
        "dependencies": [
          "43",
          "3"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T11:51:53.525Z"
      },
      {
        "id": "48",
        "title": "Fix bare except clauses in core modules",
        "description": "Replace bare 'except:' clauses in device_connection.py and post_reel_smart.py with 'except Exception:' and add optional debug logging to improve error traceability without changing runtime behavior.",
        "details": "## Current State Analysis\n\nFound 4 bare except clauses across the two core modules:\n\n### device_connection.py (3 occurrences):\n\n**1. Line 448 - reconnect_appium() method:**\n```python\ntry:\n    if self.appium_driver:\n        self.appium_driver.quit()\nexcept:\n    pass\n```\nContext: Cleanup during Appium reconnection - silently ignores driver quit failures.\n\n**2. Line 482 - disconnect() method (Appium cleanup):**\n```python\ntry:\n    if self.appium_driver:\n        self.appium_driver.quit()\n        print(\"  Appium driver closed\")\nexcept:\n    pass\n```\nContext: Cleanup during disconnect - silently ignores driver quit failures.\n\n**3. Line 487 - disconnect() method (ADB cleanup):**\n```python\ntry:\n    self.client.disable_adb(self.phone_id)\nexcept:\n    pass\n```\nContext: Cleanup during disconnect - silently ignores ADB disable failures.\n\n### post_reel_smart.py (1 occurrence):\n\n**4. Line 897 - cleanup() method:**\n```python\ntry:\n    self.adb(\"rm -f /sdcard/Download/*.mp4\")\nexcept:\n    pass\n```\nContext: Cleanup after posting - silently ignores video deletion failures.\n\n## Implementation Steps\n\n### Step 1: Add optional logging infrastructure to device_connection.py\n\nSince device_connection.py doesn't currently import logging, add a minimal optional logger:\n\n```python\n# At top of file, after existing imports\nimport logging\n\n# Create module-level logger (only used when explicitly configured)\n_logger = logging.getLogger(__name__)\n```\n\n### Step 2: Fix bare except in reconnect_appium() (line 448)\n\n```python\n# Before\nexcept:\n    pass\n\n# After\nexcept Exception as e:\n    _logger.debug(\"Appium driver quit during reconnect failed: %s\", e)\n```\n\n### Step 3: Fix bare excepts in disconnect() (lines 482, 487)\n\n```python\n# Line 482 - Appium cleanup\nexcept Exception as e:\n    _logger.debug(\"Appium driver quit during disconnect failed: %s\", e)\n\n# Line 487 - ADB cleanup  \nexcept Exception as e:\n    _logger.debug(\"disable_adb during disconnect failed: %s\", e)\n```\n\n### Step 4: Add optional logging to post_reel_smart.py cleanup() (line 897)\n\nSince post_reel_smart.py also doesn't import logging at module level:\n\n```python\n# At top of file, after existing imports\nimport logging\n_logger = logging.getLogger(__name__)\n\n# Line 897 fix\nexcept Exception as e:\n    _logger.debug(\"Video cleanup rm command failed: %s\", e)\n```\n\n## Why 'except Exception:' Instead of More Specific Types\n\n1. **Preserves original behavior**: Catches the same errors (all exceptions except SystemExit, KeyboardInterrupt, GeneratorExit)\n2. **Best practice**: PEP 8 recommends avoiding bare except; `except Exception:` is the standard broad catch\n3. **Still catches everything needed**: Subprocess errors, Appium WebDriver exceptions, network errors, etc.\n4. **Doesn't catch control flow exceptions**: Allows KeyboardInterrupt to propagate (important for Ctrl+C handling during cleanup)\n\n## Why Optional Debug Logging\n\n1. **Zero overhead in production**: Debug logging is disabled by default\n2. **Helps debugging**: When issues occur, enabling debug logging reveals silently-swallowed errors\n3. **No behavior change**: The pass statement is effectively preserved (exception is caught, logged at debug level, then continues)\n4. **Consistent pattern**: Establishes a pattern for other cleanup code in the codebase",
        "testStrategy": "## Test Strategy\n\n### 1. Syntax and Import Verification\n```bash\n# Verify files have no syntax errors after changes\npython -c \"from device_connection import DeviceConnectionManager; print('device_connection.py OK')\"\npython -c \"from post_reel_smart import SmartInstagramPoster; print('post_reel_smart.py OK')\"\n```\n\n### 2. Verify No Bare Except Clauses Remain\n```bash\n# Search for bare except patterns - should return nothing\ngrep -n \"except:\" device_connection.py post_reel_smart.py | grep -v \"except Exception\"\n\n# Expected: No output (all bare excepts replaced)\n```\n\n### 3. Verify Logging Import Added\n```bash\npython -c \"\nimport ast\nwith open('device_connection.py', 'r') as f:\n    tree = ast.parse(f.read())\nimports = [node.names[0].name for node in ast.walk(tree) if isinstance(node, ast.Import)]\nassert 'logging' in imports, 'logging not imported in device_connection.py'\nprint('device_connection.py: logging import present')\n\"\n\npython -c \"\nimport ast\nwith open('post_reel_smart.py', 'r') as f:\n    tree = ast.parse(f.read())\nimports = [node.names[0].name for node in ast.walk(tree) if isinstance(node, ast.Import)]\nassert 'logging' in imports, 'logging not imported in post_reel_smart.py'\nprint('post_reel_smart.py: logging import present')\n\"\n```\n\n### 4. Functional Test - Disconnect Cleanup Works\n```bash\npython -c \"\nfrom device_connection import DeviceConnectionManager\n\n# Create manager with dummy phone name - will fail to connect but disconnect should work\nmgr = DeviceConnectionManager('nonexistent_test_phone')\nmgr.phone_id = 'fake_id'  # Set fake ID\nmgr.appium_driver = None  # No driver\n\n# Call disconnect - should not raise even though operations will fail\ntry:\n    mgr.disconnect()\n    print('SUCCESS: disconnect() completes without raising')\nexcept Exception as e:\n    print(f'FAILURE: disconnect() raised: {e}')\n\"\n```\n\n### 5. Verify Debug Logging Works When Enabled\n```bash\npython -c \"\nimport logging\n\n# Enable debug logging for device_connection module\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger('device_connection')\nlogger.setLevel(logging.DEBUG)\n\nfrom device_connection import DeviceConnectionManager\n\n# Create manager and trigger cleanup path\nmgr = DeviceConnectionManager('test_phone_for_logging')\nmgr.phone_id = 'fake'\nmgr.appium_driver = None\n\n# This should produce debug log output about cleanup failures\nprint('--- Debug output should appear below if logging works ---')\nmgr.disconnect()\nprint('--- End debug output ---')\n\"\n```\n\n### 6. Verify Original Behavior Preserved (Exceptions Still Swallowed)\n```bash\npython -c \"\nfrom device_connection import DeviceConnectionManager\n\n# Create a mock driver that raises on quit\nclass MockDriverThatRaises:\n    def quit(self):\n        raise RuntimeError('Simulated driver failure')\n\nmgr = DeviceConnectionManager('test_phone')\nmgr.phone_id = 'fake'\nmgr.appium_driver = MockDriverThatRaises()\n\n# disconnect should NOT raise despite the driver raising\ntry:\n    mgr.disconnect()\n    print('SUCCESS: Exceptions still properly swallowed in cleanup')\nexcept Exception as e:\n    print(f'FAILURE: Exception escaped cleanup: {e}')\n\"\n```",
        "status": "done",
        "dependencies": [
          "37",
          "38"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T12:02:16.484Z"
      },
      {
        "id": "49",
        "title": "Extract screen coordinate constants in post_reel_smart.py",
        "description": "Define named constants for magic numbers like SCREEN_CENTER_X=360, FEED_TOP_Y=400, FEED_BOTTOM_Y=900 used in swipe/tap operations. This makes the code self-documenting and easier to adjust for different screen sizes.",
        "details": "## Current State Analysis\n\nMagic numbers are scattered throughout `post_reel_smart.py` for screen coordinate operations. These numbers appear in swipe and tap calls but lack semantic meaning:\n\n### Magic Numbers Found (multi-use candidates):\n\n**Horizontal coordinates:**\n- `360` - Screen center X (12+ occurrences across swipes and taps)\n- `650` - Right side X (used for story skip tap at line 180)\n\n**Vertical coordinates:**\n- `400` - Feed top Y / scroll destination (used in scroll_down swipes)\n- `640` - Screen center Y (used for double-tap like at lines 205-207)\n- `800` - Notifications scroll Y\n- `900` - Feed bottom Y / scroll start (used in scroll_up swipes)\n- `1000` - Reels bottom Y (used in reels swipe at line 210)\n- `300` - Reels top Y (used in reels swipe at line 210)\n\n**Duration constants:**\n- `200, 300, 400` - Swipe durations in ms (some via `random.randint(200, 400)`)\n\n### Implementation Plan\n\n**Step 1: Define constants at class level or module level**\n\nAdd a new `ScreenCoordinates` dataclass or class-level constants in `post_reel_smart.py`:\n\n```python\n# Screen coordinate constants for 720x1280 resolution\n# These values are calibrated for Geelark cloud phones\nclass ScreenCoords:\n    \"\"\"Screen coordinate constants for UI interactions.\"\"\"\n    # Horizontal\n    SCREEN_CENTER_X = 360  # Center of 720px screen\n    STORY_SKIP_X = 650     # Right side for story skip tap\n    \n    # Vertical\n    FEED_TOP_Y = 400       # Top of scrollable feed area\n    SCREEN_CENTER_Y = 640  # Center of 1280px screen\n    NOTIFICATIONS_Y = 800  # Notifications scroll position\n    FEED_BOTTOM_Y = 900    # Bottom of scrollable feed area\n    REELS_TOP_Y = 300      # Top Y for reels swipe\n    REELS_BOTTOM_Y = 1000  # Bottom Y for reels swipe\n    \n    # Swipe durations (ms)\n    SWIPE_FAST_MS = 200\n    SWIPE_NORMAL_MS = 300\n    SWIPE_SLOW_MS = 400\n```\n\n**Step 2: Update usages in `post_reel_smart.py`**\n\nReplace magic numbers with constants:\n\n```python\n# Before (line 156):\nself.swipe(360, 900, 360, 400, random.randint(200, 400))\n\n# After:\nself.swipe(ScreenCoords.SCREEN_CENTER_X, ScreenCoords.FEED_BOTTOM_Y,\n           ScreenCoords.SCREEN_CENTER_X, ScreenCoords.FEED_TOP_Y,\n           random.randint(ScreenCoords.SWIPE_FAST_MS, ScreenCoords.SWIPE_SLOW_MS))\n```\n\n**Lines to update in `post_reel_smart.py`:**\n- Line 156: `_humanize_scroll_feed()` - scroll down swipe\n- Line 160: `_humanize_scroll_feed()` - scroll up swipe\n- Line 180: `_humanize_view_story()` - story skip tap (650, 640)\n- Lines 205-207: `_humanize_scroll_reels()` - double-tap like (360, 640)\n- Line 210: `_humanize_scroll_reels()` - reels swipe (360, 1000, 360, 300)\n- Line 232: `_humanize_check_notifications()` - notifications swipe\n- Line 283: `humanize_after_post()` - feed scroll\n- Line 564: `_action_scroll_down()` - ADB swipe command\n- Line 568: `_action_scroll_up()` - ADB swipe command\n\n**Step 3: Update `appium_ui_controller.py`**\n\nThe `scroll_down()` and `scroll_up()` methods (lines 221-227) also use these magic numbers. Either:\n1. Import `ScreenCoords` from `post_reel_smart.py` (creates import dependency)\n2. Define constants in a shared module (e.g., `config.py`)\n3. Define locally in `appium_ui_controller.py` (duplicate but isolated)\n\n**Recommended approach:** Add constants to `config.py` since it's already the centralized config:\n\n```python\n# In config.py, add:\nclass ScreenCoords:\n    \"\"\"Screen coordinate constants for 720x1280 Geelark phones.\"\"\"\n    SCREEN_CENTER_X = 360\n    FEED_TOP_Y = 400\n    SCREEN_CENTER_Y = 640\n    STORY_SKIP_X = 650\n    NOTIFICATIONS_Y = 800\n    FEED_BOTTOM_Y = 900\n    REELS_TOP_Y = 300\n    REELS_BOTTOM_Y = 1000\n    SWIPE_FAST_MS = 200\n    SWIPE_NORMAL_MS = 300\n    SWIPE_SLOW_MS = 400\n```\n\nThen import in both files:\n```python\nfrom config import Config, ScreenCoords, setup_environment\n```\n\n**Step 4: Exclude single-use magic numbers**\n\nOnly extract constants for values used in **multiple places**. Single-use coordinates like element centers from UI dumps should remain as-is since they're dynamically determined.\n\n### Files to Modify\n\n1. `config.py` - Add `ScreenCoords` class/dataclass\n2. `post_reel_smart.py` - Import and use `ScreenCoords` constants (9 locations)\n3. `appium_ui_controller.py` - Import and use `ScreenCoords` constants (2 locations)",
        "testStrategy": "## Test Strategy\n\n### 1. Syntax and Import Verification\n```bash\n# Verify all files have no syntax errors after changes\npython -c \"from config import Config, ScreenCoords; print('config.py OK')\"\npython -c \"from post_reel_smart import SmartInstagramPoster; print('post_reel_smart.py OK')\"\npython -c \"from appium_ui_controller import AppiumUIController; print('appium_ui_controller.py OK')\"\n```\n\n### 2. Verify ScreenCoords Constants Exist\n```bash\npython -c \"\nfrom config import ScreenCoords\nprint('SCREEN_CENTER_X:', ScreenCoords.SCREEN_CENTER_X)\nprint('FEED_TOP_Y:', ScreenCoords.FEED_TOP_Y)\nprint('FEED_BOTTOM_Y:', ScreenCoords.FEED_BOTTOM_Y)\nprint('All constants defined correctly')\n\"\n```\n\n### 3. Verify No Magic Numbers Remain in Multi-Use Locations\n```bash\n# Check that 360 is not used as a raw literal in swipe/tap calls\n# (should be replaced with ScreenCoords.SCREEN_CENTER_X)\ngrep -n \"swipe(360\" post_reel_smart.py\ngrep -n \"tap(360\" post_reel_smart.py\ngrep -n \"swipe(360\" appium_ui_controller.py\n# Expected: No matches (all replaced with constants)\n```\n\n### 4. Verify Constant Values Match Original\n```bash\npython -c \"\nfrom config import ScreenCoords\n# Verify the constants have the correct values\nassert ScreenCoords.SCREEN_CENTER_X == 360, 'SCREEN_CENTER_X wrong'\nassert ScreenCoords.FEED_TOP_Y == 400, 'FEED_TOP_Y wrong'\nassert ScreenCoords.FEED_BOTTOM_Y == 900, 'FEED_BOTTOM_Y wrong'\nassert ScreenCoords.SCREEN_CENTER_Y == 640, 'SCREEN_CENTER_Y wrong'\nassert ScreenCoords.REELS_BOTTOM_Y == 1000, 'REELS_BOTTOM_Y wrong'\nassert ScreenCoords.REELS_TOP_Y == 300, 'REELS_TOP_Y wrong'\nprint('All constant values verified')\n\"\n```\n\n### 5. Behavior Verification (No Code Breakage)\n```bash\n# Quick instantiation test to ensure the class still works\npython -c \"\nfrom post_reel_smart import SmartInstagramPoster\nposter = SmartInstagramPoster('test_phone')\nprint('SmartInstagramPoster instantiation OK')\n\"\n```\n\n### 6. Unit Test for Swipe Methods\n```bash\npython -c \"\nfrom config import ScreenCoords\nfrom appium_ui_controller import AppiumUIController\n\n# Verify scroll_down and scroll_up use correct values\n# (Check source code for ScreenCoords usage)\nimport inspect\nsource = inspect.getsource(AppiumUIController.scroll_down)\nassert 'ScreenCoords' in source or 'SCREEN_CENTER_X' in source, 'scroll_down should use constants'\nprint('AppiumUIController methods use constants')\n\"\n```\n\n### 7. Full Integration Test (Optional - requires running phone)\n```bash\n# Only run if a test phone is available\n# python post_reel_smart.py test_phone test_video.mp4 \"Test caption\"\n```\n\n### 8. Code Review Checklist\n- [ ] All multi-use magic numbers (360, 400, 640, 900, etc.) replaced with named constants\n- [ ] Constants defined in `config.py` ScreenCoords class\n- [ ] `post_reel_smart.py` imports and uses ScreenCoords\n- [ ] `appium_ui_controller.py` imports and uses ScreenCoords\n- [ ] Single-use numbers (from element bounds) NOT extracted\n- [ ] Code behavior unchanged (same coordinates used)",
        "status": "done",
        "dependencies": [
          "39",
          "43",
          "47"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-13T12:04:36.361Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-12-13T12:04:36.363Z",
      "taskCount": 49,
      "completedCount": 46,
      "tags": [
        "posting"
      ]
    }
  }
}
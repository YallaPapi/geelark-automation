{
  "master": {
    "tasks": [
      {
        "id": 11,
        "title": "Project skeleton, configuration, and input/output handling",
        "description": "Set up the Python project, config management, and basic CLI to load inputs (photos, bios, follow list, proxies, API keys) and write CSV output for created accounts.",
        "details": "Implementation details:\n- Use Python 3.11+ with a simple, single-package structure (e.g., `geelark_ig_bot/`).\n- Create `config.py` to load configuration from a `.env` file (using `python-dotenv`) or a `config.yaml` file (using `pyyaml`). Keys: DAISY_SMS_KEY, TWO_CAPTCHA_KEY, ANTHROPIC_KEY, PROXY_ROTATE_URL, GEELARK_DEVICE_ID or connection params, paths for PHOTOS_DIR, BIOS_FILE, FOLLOW_FILE, OUTPUT_CSV.\n- Implement a small `models.py` with dataclasses such as `AccountProfile(photo_path, bio, follow_targets)` and `RunContext(proxy_url, device_id, session_id, logs_path)`.\n- Implement `io_inputs.py`:\n  - Load all image paths from the photos folder (validate file extensions and existence).\n  - Load bios from a text file, one bio per non-empty line.\n  - Load accounts-to-follow from a text file, one username per non-empty line.\n- Implement `io_outputs.py` with function `append_created_account(csv_path, username, password, phone, status, extra=None)` that appends a row; ensure the CSV is created with a header if missing.\n- Implement `main.py` with a CLI (using `argparse`) that supports parameters like `--accounts N`, `--device-id`, `--start-index`, `--output-csv`.\n- Add logging (built-in `logging` module) with INFO for high-level steps and DEBUG for low-level details; log to both console and a rotating file handler.\n- Ensure paths and config values are validated at startup, with clear error messages and non-zero exit codes on failure.\n- Keep architecture minimal: a main loop that calls a `create_single_account(profile: AccountProfile)` function implemented in later tasks.\n\nPseudo-code sketch:\n```python\n# main.py\nfrom config import load_config\nfrom io_inputs import load_photos, load_bios, load_follow_targets\nfrom io_outputs import append_created_account\nfrom workflow import create_single_account\n\nif __name__ == \"__main__\":\n    cfg = load_config()\n    photos = load_photos(cfg.PHOTOS_DIR)\n    bios = load_bios(cfg.BIOS_FILE)\n    follows = load_follow_targets(cfg.FOLLOW_FILE)\n\n    for i in range(cfg.NUM_ACCOUNTS):\n        profile = build_profile(photos, bios, follows, i)\n        result = create_single_account(profile, cfg)\n        append_created_account(\n            cfg.OUTPUT_CSV,\n            result.username,\n            result.password,\n            result.phone,\n            result.status,\n        )\n```",
        "testStrategy": "- Unit test config loading with missing/invalid keys.\n- Unit test input loaders with temporary directories and sample files.\n- Unit test CSV writer: create temp file, append multiple rows, verify header and data.\n- Run a dry-run mode (no device interaction) that uses mock `create_single_account` to verify CLI, logging, and CSV pipeline behave correctly.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Geelark device control abstraction (RPA/ADB/API)",
        "description": "Research and implement a minimal Python abstraction to control Geelark cloud phones (screenshot, tap, type, scroll) using the most reliable available method (RPA, ADB, or API).",
        "details": "Implementation details:\n- Investigate GeeLark’s RPA feature and any documented APIs from their dashboard/help center.[1]\n- Decide on a practical option:\n  - **Option A (preferred, if accessible):** Use GeeLark RPA/Custom tasks via HTTP or WebSocket if they expose an API to trigger actions on a running device (tap, input text, wait), or via a local bridge component.\n  - **Option B:** Connect via ADB over TCP to the cloud phone (if GeeLark exposes an ADB endpoint per phone). Use `adbutils` or `pure-python-adb` for screenshots and input events.\n  - **Option C:** If GeeLark has an official REST API to interact with cloud phones, wrap the relevant endpoints.\n- Define a Python interface `GeelarkDeviceController` in `geelark_device.py` with methods:\n  - `screenshot() -> bytes` (PNG/JPEG data)\n  - `tap(x: int, y: int)`\n  - `type_text(text: str)`\n  - `scroll(direction: Literal[\"up\",\"down\",\"left\",\"right\"], amount: int=500)`\n  - `back()` to press back button\n  - `home()` to go home\n  - `wait(seconds: float)` for simple delays.\n- Implement at least one concrete subclass, e.g., `AdbGeelarkDeviceController` or `RpaGeelarkDeviceController`, depending on what is feasible with GeeLark.\n- Include a simple device discovery/attachment function: `connect_device(device_id_or_host) -> GeelarkDeviceController`.\n- Ensure screenshot capturing is performant (e.g., ADB `exec-out screencap -p`), and images are in a format accepted by Claude Vision.\n\nExample using ADB-style pseudo-code:\n```python\nclass AdbGeelarkDeviceController(GeelarkDeviceController):\n    def __init__(self, serial: str):\n        self.adb = adbutils.AdbDevice(serial=serial)\n\n    def screenshot(self) -> bytes:\n        return self.adb.screencap()\n\n    def tap(self, x, y):\n        self.adb.shell(f\"input tap {x} {y}\")\n\n    def type_text(self, text):\n        safe = text.replace(\" \", \"%s\")\n        self.adb.shell(f\"input text '{safe}'\")\n\n    def scroll(self, direction, amount=500):\n        if direction == \"up\":\n            self.adb.shell(f\"input swipe 500 1000 500 {1000-amount}\")\n        # etc.\n```",
        "testStrategy": "- If ADB is used, test against a local Android emulator: verify that screenshot bytes are non-empty and tapping/types produce visible effects.\n- If GeeLark RPA/API is used, integration test on a disposable cloud phone: tap a known coordinate (e.g., Settings icon) and verify manually.\n- Add a `--test-device` CLI option that runs a quick health-check: take screenshot, tap a test area, log success/failure.\n- Use mocks in unit tests to assert high-level code calls `tap`, `type_text`, etc., with expected parameters.",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Claude Vision screen understanding client",
        "description": "Implement a Python client that sends device screenshots and the current context to Claude Vision, parses its response into actionable steps with coordinates and text.",
        "details": "Implementation details:\n- Use Anthropic’s Python SDK or plain HTTP with API key from config.\n- Define a prompt template that instructs Claude Vision explicitly:\n  - Provide **screen description**.\n  - Provide **next action** in a strict JSON format with fields like `{\"action\": \"tap\"|\"type\"|\"scroll\"|\"done\"|\"wait\",\"coordinates\": {\"x\": int, \"y\": int},\"text\": \"...\", \"reason\": \"...\"}`.\n  - Ask it to always respond with a single JSON object and no extra text.\n  - Instruct it that the goal is to create and fully set up an Instagram account according to the step list (birthday, phone, SMS, username, password, skip optional, photo, bio, creator, follow accounts).\n- Implement `claude_vision.py` with:\n  - `class ClaudeVisionClient:`\n    - `propose_action(image_bytes: bytes, state: dict) -> dict` where `state` includes progress markers (e.g., `has_entered_birthday`, `has_verified_phone`).\n- Implement robust JSON parsing:\n  - Strip any non-JSON prefix/suffix if Claude accidentally adds text.\n  - Validate that required keys exist; if not, log error and request again with a clarifying system message.\n- Include rate limiting/backoff and simple retry for network errors or malformed responses.\n- Maintain a small `state` object that encodes goal progress to share with Claude in the system/user message so it can choose the next step more reliably.\n\nPseudo-code:\n```python\nSYSTEM_PROMPT = \"\"\"You are controlling an Android phone to create a new Instagram account...\"\"\"\n\ndef propose_action(self, img, state):\n    msg = self._build_message(state)\n    resp = self.client.messages.create(\n        model=\"claude-3.5-sonnet\",  # or latest vision-capable model\n        max_tokens=300,\n        temperature=0.1,\n        messages=[\n          {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n          {\"role\": \"user\", \"content\": [\n              {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": base64.b64encode(img).decode()}},\n              {\"type\": \"text\", \"text\": msg},\n          ]},\n        ],\n    )\n    json_str = extract_json(resp)\n    return json.loads(json_str)\n```",
        "testStrategy": "- Unit test prompt-building and JSON parsing with canned Claude-like responses.\n- Add an offline mode that uses a fake vision client returning predetermined actions for known test screenshots to validate the loop without spending API credits.\n- Log each request/response pair to a file (with redaction of secrets) and manually inspect a few runs to ensure action JSON is consistent.",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "DaisySMS integration for phone number and SMS code retrieval",
        "description": "Implement integration with DaisySMS to rent phone numbers and poll for Instagram verification SMS codes.",
        "details": "Implementation details:\n- Review DaisySMS API docs to identify endpoints for:\n  - Requesting a number for a specific service/country.\n  - Checking SMS status and retrieving the code.\n  - Canceling/finishing an activation.\n- Implement `daisysms_client.py` with:\n  - `request_number(service=\"instagram\", country=None) -> Activation` where `Activation` holds `id`, `phone_number`.\n  - `wait_for_sms(activation_id, timeout=300, poll_interval=5) -> str` returning the numeric code.\n  - `cancel_activation(activation_id)` and `finish_activation(activation_id)`.\n- Handle common failure cases: no numbers, timeout waiting for SMS, banned/invalid numbers.\n- Mask phone number in logs for privacy.\n- Provide helper to format phone for entering on the device (e.g., strip `+` if needed, or let Claude decide how to input it given the screenshot).\n\nPseudo-code sketch:\n```python\nclass DaisySmsClient:\n    def request_number(self):\n        # call API, parse JSON\n        return Activation(id=act_id, phone=phone)\n\n    def wait_for_sms(self, act_id, timeout=300):\n        # loop: GET status, parse text, extract 6-digit code via regex\n```",
        "testStrategy": "- Unit test JSON parsing with sample DaisySMS responses.\n- Use a mock HTTP server (e.g., `responses` or `httpretty`) for DaisySMS endpoints to validate retry and timeout behavior.\n- In a staging run, manually request a number and send a test SMS from another phone to verify code extraction logic.\n- Simulate failure modes (no number, timeout, malformed SMS) and confirm the calling workflow handles them gracefully (marks account as failed, logs reason, releases activation).",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "2Captcha integration for solving Instagram captchas",
        "description": "Integrate with 2Captcha to submit Instagram captchas (image or possibly hCaptcha/ReCaptcha) and retrieve solutions when they appear during signup.",
        "details": "Implementation details:\n- Check 2Captcha docs for supported captcha types on Instagram flows (likely image captcha or hCaptcha/ReCaptcha). Implement at least generic image captcha support; leave hooks for sitekey-based captchas if needed.\n- Implement `twocaptcha_client.py` with:\n  - `submit_image_captcha(image_bytes) -> captcha_id`.\n  - `wait_for_solution(captcha_id, timeout=180, poll_interval=5) -> str`.\n- Integrate with the main flow via a simple contract: when Claude identifies a captcha on the screen and indicates an `action: \"captcha\"` (we can define this), capture a high-resolution screenshot and crop if necessary:\n  - Either ask Claude to provide bounding box coordinates, then crop the relevant region before sending to 2Captcha.\n- After receiving the solution string, pass it back to the device using `type_text` or `tap`/`type` sequences as directed by Claude.\n- Implement error handling: if 2Captcha returns an error or times out, mark run as failed and log details.\n\nPseudo-code:\n```python\nclass TwoCaptchaClient:\n    def submit_image_captcha(self, img):\n        # POST multipart/form-data to 2Captcha\n\n    def wait_for_solution(self, cap_id, timeout):\n        # poll /res.php until status=1\n```",
        "testStrategy": "- Unit test polling and response parsing using mocked 2Captcha HTTP endpoints.\n- Manual integration test with a known captcha image to confirm that 2Captcha returns the expected text.\n- Simulate failures such as `ERROR_CAPTCHA_UNSOLVABLE` and ensure workflow either retries with a new captcha or aborts with a clear status.",
        "priority": "medium",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Mobile proxy rotation and network setup",
        "description": "Implement proxy rotation via a mobile proxy rotation URL before each new account and ensure all external calls from the device go through the rotated IP.",
        "details": "Implementation details:\n- Use the provided `PROXY_ROTATE_URL` config: before starting each new account creation, send a simple HTTP GET to this URL and wait a short delay (e.g., 5–10 seconds) for IP to change.\n- If GeeLark supports per-device proxy assignment, ensure the cloud phone is configured to use the mobile proxy; otherwise, rely on proxy at network edge.\n- Implement `proxy.py` with:\n  - `rotate_proxy() -> bool` which returns True on HTTP 2xx, False otherwise.\n- Add logging to record rotation attempts and results.\n- Optionally verify IP change using a cheap `https://api.ipify.org` style service via the device’s browser or host network (config-driven; disabled by default to avoid extra calls).\n- Integrate into `create_single_account` workflow: call `rotate_proxy()` once at the very beginning of each account run.\n\nPseudo-code:\n```python\ndef rotate_proxy(url, timeout=10):\n    try:\n        r = requests.get(url, timeout=timeout)\n        r.raise_for_status()\n        logger.info(\"Proxy rotated\")\n        time.sleep(8)\n        return True\n    except Exception as e:\n        logger.error(f\"Proxy rotation failed: {e}\")\n        return False\n```",
        "testStrategy": "- Unit test `rotate_proxy` with mocked HTTP responses (success, timeout, non-200).\n- In staging, call rotation multiple times and verify IP change manually using an external IP-check service.\n- Add a debug flag to log detected IPs (host-level) before and after rotation for manual verification.",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Core action loop: screenshot → Claude → device actions",
        "description": "Implement the main control loop that repeatedly screenshots the Geelark device, asks Claude Vision what to do, executes the returned action (tap/type/scroll), and tracks progress toward account creation.",
        "details": "Implementation details:\n- Implement `workflow.py` with a function `run_screen_loop(device: GeelarkDeviceController, vision: ClaudeVisionClient, state: dict, max_steps=200) -> state`.\n- Loop behavior:\n  - For each step:\n    - Take screenshot via `device.screenshot()`.\n    - Call `vision.propose_action(image_bytes, state)`.\n    - Parse action JSON and execute:\n      - `action == \"tap\"`: call `device.tap(x, y)`.\n      - `action == \"type\"`: call `device.type_text(text)`.\n      - `action == \"scroll\"`: call `device.scroll(direction, amount)`.\n      - `action == \"wait\"`: call `device.wait(seconds)`.\n      - `action == \"back\"`/`\"home\"`: call corresponding methods.\n      - `action == \"done\"`: break loop and return.\n      - `action == \"captcha\"`: delegate to 2Captcha handler (Task 15) then feed solution back.\n    - Update `state` with any progress hints returned (e.g., `state[\"phase\"] = resp[\"phase\"]`).\n    - Add random small delays (0.5–1.5 s) to mimic human interaction and let UI update.\n- Implement safety guards:\n  - If `max_steps` reached without `done`, mark run as failed.\n  - Detect repeated identical actions (same tap coordinates for many steps) and break to avoid loops.\n- Ensure the state encodes key information for later steps (e.g., whether phone number has been used, SMS verified, username set, account switched to creator, followed 20 accounts).\n\nPseudo-code:\n```python\ndef run_screen_loop(device, vision, state, max_steps=200):\n    for i in range(max_steps):\n        img = device.screenshot()\n        action = vision.propose_action(img, state)\n        if action[\"action\"] == \"done\":\n            state[\"status\"] = \"done\"\n            break\n        execute_action(device, action, state)\n    return state\n```",
        "testStrategy": "- Implement unit tests for `execute_action` using a mock `GeelarkDeviceController` to verify correct calls for each action type.\n- Use an offline fake-vision client (from Task 13 tests) returning a deterministic series of actions to validate that the loop terminates correctly and state progresses.\n- On a test device with Instagram already on a simple form screen, run a short loop and confirm taps and typing correspond roughly to what Claude suggests (manual spot check using logs and video capture).",
        "priority": "high",
        "dependencies": [
          12,
          13,
          15
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Account setup orchestration and Instagram-specific flow",
        "description": "Orchestrate a full Instagram account setup run, coordinating proxy rotation, device control, Claude-driven steps, DaisySMS phone/SMS handling, captchas, and the business logic for username/password, bio, photo, creator switch, and following accounts.",
        "details": "Implementation details:\n- Implement `create_single_account(profile: AccountProfile, cfg) -> AccountResult` in `workflow.py`.\n- High-level sequence:\n  1. Rotate proxy using Task 16.\n  2. Connect to Geelark device (Task 12) and ensure Instagram app is launched (via explicit launch intent or by tapping icon; you can teach Claude to tap the Instagram icon from home screen as part of loop).\n  3. Initialize `state` with:\n     - `target_bio`, `target_photo_path`, `follow_targets`.\n     - Flags: `birthday_entered`, `phone_requested`, `sms_verified`, `username_set`, `password_set`, `creator_switched`, `followed_count`.\n  4. Request DaisySMS number when the flow reaches phone entry stage:\n     - Either pre-request the number before starting, or better, when `state` indicates phone will be needed (e.g., when Claude says \"now enter phone number\").\n     - Store number and activation id in `state`.\n  5. Run `run_screen_loop` until `state[\"status\"] == \"done\"` or error.\n  6. In the loop integration, insert hooks based on `state`:\n     - When a screen expects the phone number, programmatically supply the DaisySMS number (you may give Claude the number in the context so it types it itself).\n     - After submitting phone, start a background `wait_for_sms` and when code is received, provide it to Claude in the next prompt so it can type it.\n     - For username/password, either auto-generate values in Python (e.g., random letters+digits) and provide them to Claude, or let Claude propose them but ensure Python records them in `state` so they can be output to CSV.\n  7. Ensure optional steps (such as contacts, notifications, etc.) are skipped—rely on Claude’s screen understanding but mention this explicitly in the prompt.\n  8. After reaching home feed, direct Claude (via state goal) to:\n     - Add profile photo from gallery: upload `target_photo_path` to the device or ensure the device already has a set of photos (outside of script scope) and instruct Claude accordingly.\n     - Add bio using `target_bio`.\n     - Switch to Creator account via settings (state flag `creator_switched=True` when done).\n     - Follow ~20 accounts from `follow_targets` list (give the list or next target to Claude in context, track `followed_count`).\n- Implement `AccountResult(username, password, phone, status, error_message=None)` dataclass.\n- On any unrecoverable error (DaisySMS/2Captcha failure, loop timeout, device disconnection), set `status=\"failed\"` and include `error_message`.\n",
        "testStrategy": "- Unit test orchestration logic with mocks for DaisySMS, 2Captcha, device controller, and Claude client to ensure correct call ordering and state changes.\n- Implement a dry-run mode that skips actual external calls and produces synthetic `AccountResult` to verify CSV output and control flow.\n- Run an end-to-end test on a single GeeLark device with manual observation, logging all key decisions; verify that a full account is created and appears in Instagram.\n- After a successful single-account run, test a small batch (e.g., 3 accounts) in series to validate that proxy rotation and resource cleanup between runs behave correctly.",
        "priority": "high",
        "dependencies": [
          12,
          13,
          14,
          15,
          16,
          17
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Profile data assignment and username/password generation",
        "description": "Implement logic to map input photos, bios, and follow lists to each new account and deterministically generate secure usernames and passwords.",
        "details": "Implementation details:\n- Implement `profiles.py` with:\n  - `build_profile(photos, bios, follows, index) -> AccountProfile` using round-robin or randomized selection.\n  - `generate_username(index, base=None) -> str` using a configurable pattern (e.g., random adjectives+noun+digits) and allowed Instagram constraints.\n  - `generate_password() -> str` with 12–16 chars including letters, digits, and symbols.\n- Ensure that for each account run, `AccountProfile` includes:\n  - `photo_path`: may be None if fewer photos than accounts; handle gracefully (skip photo step).\n  - `bio`: may be randomly chosen or selected sequentially.\n  - `follow_targets`: either the full list or a subset of ~20 selected per account.\n- Pass generated username and password into `state` to be shared with Claude so it types them when appropriate.\n- Avoid reusing the same username; if Instagram rejects a username, have Claude propose alternatives but keep track in state and update `AccountResult` accordingly.\n\nPseudo-code:\n```python\n@dataclass\nclass AccountProfile:\n    username: str\n    password: str\n    photo_path: Optional[str]\n    bio: Optional[str]\n    follow_targets: list[str]\n```",
        "testStrategy": "- Unit test profile building to ensure fair rotation of bios/photos and correct slicing of follow targets (~20 per account).\n- Unit test username/password generation for uniqueness and complexity constraints.\n- Use a mock Claude client to simulate username rejection; verify that state and `AccountResult` update to the new accepted username.",
        "priority": "medium",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Logging, error handling, and basic scaling for multiple accounts",
        "description": "Harden the script with structured logging, error handling, cleanup between runs, and simple sequential multi-account execution.",
        "details": "Implementation details:\n- Extend logging from Task 11:\n  - Include per-account correlation ID in all logs.\n  - Log key milestones (proxy rotated, number acquired, SMS received, captcha solved, account created, failures).\n- Implement a central exception handler in `main.py` that catches unexpected errors per account, records a failed `AccountResult`, and continues to the next account instead of crashing the whole batch.\n- Add cleanup hooks:\n  - Release DaisySMS activations on error.\n  - Optionally reset Instagram app state between runs (e.g., clear data or log out via Claude instructions at end of run).\n- For scaling:\n  - Keep initial implementation strictly sequential (one account after another) to minimize complexity.\n  - Design the code to allow future parallelization (e.g., by making `create_single_account` stateless other than its arguments and return value), but do not add concurrency yet.\n- Expose a few runtime knobs via CLI/config: `MAX_STEPS`, `SMS_TIMEOUT`, `CAPTCHA_TIMEOUT`, `RETRY_LIMIT`.\n",
        "testStrategy": "- Simulate multiple account runs with mocks where some accounts succeed and others fail; verify that all results are written to CSV and script exits cleanly.\n- Inject failures (e.g., raise exceptions from DaisySMS/2Captcha/Claude clients) and confirm they are caught and logged and do not stop subsequent accounts.\n- Manual multi-account test (2–3 accounts) to verify logs are readable and correlated with account IDs.",
        "priority": "medium",
        "dependencies": [
          18
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-12-10T03:59:28.494Z",
      "updated": "2025-12-10T04:21:12.826Z",
      "description": "Tasks for master context"
    }
  },
  "posting": {
    "tasks": [
      {
        "id": "64",
        "title": "Update dependencies and environment for OpenAI GPT-5 mini usage",
        "description": "Replace Anthropic SDK dependency with OpenAI Python SDK, ensure correct environment variables for OpenAI, and prepare the project for GPT-5 mini and vision calls without changing any business logic or prompts.",
        "details": "Implementation details:\n- Dependencies:\n  - Uninstall Anthropic SDK if no longer needed elsewhere in the project (or scope changes to this module only):\n    - `pip uninstall anthropic` (only if safe for the rest of the codebase).\n  - Install latest stable OpenAI Python SDK following current docs:\n    - `pip install --upgrade openai`.[6]\n  - Confirm Python version compatibility (OpenAI SDK currently supports modern 3.x; ensure your runtime matches docs).[6]\n- Environment configuration:\n  - Use the standard `OPENAI_API_KEY` environment variable; do NOT hardcode the key.[2][6]\n  - If the project currently uses an `ANTHROPIC_API_KEY`, add `OPENAI_API_KEY` alongside it to avoid breaking existing deployments.\n  - Example initialization snippet (top-level utility module or in claude_analyzer.py):\n    ```python\n    import os\n    from openai import OpenAI\n\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n    ```\n- Global configuration:\n  - Define constants for the new model IDs so later tasks can reuse them without magic strings:\n    ```python\n    GPT5_MINI_MODEL = \"gpt-5-mini\"      # text/chat\n    GPT5_MINI_VISION_MODEL = \"gpt-5-mini\"  # if same model supports vision via image content\n    ```\n  - Keep max_tokens, temperature, and other parameters identical to existing Anthropic usage unless the OpenAI API requires renaming or minimal format changes.\n- Security & best practices:\n  - Ensure `.env` / config files are git-ignored.\n  - If using a settings module, centralize provider choice there, but keep logic identical for now as required by the PRD.\n- Pseudo-code summary:\n  ```python\n  # config_models.py\n  GPT_TEXT_MODEL = \"gpt-5-mini\"\n  GPT_VISION_MODEL = \"gpt-5-mini\"  # same id if unified\n\n  from openai import OpenAI\n  import os\n\n  client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n  ```",
        "testStrategy": "- Install & import tests:\n  - Run `pip show openai` to confirm installation and version.\n  - Run the test suite or a minimal script that imports `OpenAI` and creates a dummy client.\n- Environment tests:\n  - Temporarily add a small script:\n    ```python\n    from openai import OpenAI\n    import os\n\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n    print(\"client ok\")\n    ```\n    and run it in the target environment to verify the key is loaded and no ImportError/KeyError occurs.\n- Regression risk checks:\n  - Confirm no other modules in the project that still depend on `anthropic` are broken by any dependency cleanup.\n  - If Anthropic usage exists elsewhere, ensure this task only adds OpenAI without removing Anthropic.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-19T06:59:24.189Z"
      },
      {
        "id": "65",
        "title": "Migrate claude_analyzer.py from Anthropic messages API to OpenAI GPT-5 mini chat API",
        "description": "Refactor claude_analyzer.py to call OpenAI GPT-5 mini via the OpenAI Python SDK, preserving prompts, parameters (where possible), and all logic, while adapting to the OpenAI chat completion message and response formats.",
        "details": "Implementation details:\n- Identify current Anthropic usage pattern in `claude_analyzer.py` similar to:\n  ```python\n  import anthropic\n\n  client = anthropic.Anthropic()\n  response = client.messages.create(\n      model=\"claude-sonnet-4-20250514\",\n      max_tokens=500,\n      messages=[{\"role\": \"user\", \"content\": prompt}]\n  )\n  text = response.content[0].text\n  ```\n- Replace with OpenAI chat completions using GPT-5 mini, matching the provided example pattern:\n  ```python\n  from openai import OpenAI\n  from config_models import GPT_TEXT_MODEL\n\n  client = OpenAI()  # key from env\n\n  def analyze_ui(prompt: str, max_tokens: int = 500) -> str:\n      # IMPORTANT: do not change `prompt` text content\n      response = client.chat.completions.create(\n          model=GPT_TEXT_MODEL,          # \"gpt-5-mini\"\n          max_tokens=max_tokens,\n          messages=[{\"role\": \"user\", \"content\": prompt}],\n      )\n      return response.choices[0].message.content\n  ```\n- Keep all surrounding logic identical:\n  - Do NOT change how `prompt` is constructed.\n  - Do NOT re-order steps or add system prompts unless the current code already uses them.\n  - Keep temperatures, top_p, etc., equivalent where the OpenAI API supports them; only rename parameters to the OpenAI equivalents.\n- Handle message format differences:\n  - Anthropic `messages.create` may accept role/content differently; map to OpenAI's `[{\"role\": \"user\", \"content\": prompt}]` format as shown.[6]\n  - If the existing code uses multiple turns, map each to `{ \"role\": \"user\" | \"assistant\" | \"system\", \"content\": ... }` preserving roles and order.\n- Handle response structure differences:\n  - Anthropic: `response.content[0].text`.\n  - OpenAI: `response.choices[0].message.content`.\n  - Update all call sites that rely on the returned text to expect a simple string.\n- Error handling & timeouts:\n  - Wrap calls in try/except for `openai` HTTP errors (if the project has a common error wrapper, reuse it).\n  - Consider adding a short timeout via client or HTTP config if there was some equivalent before, but avoid behavioral changes unless necessary.\n- Pseudo-code (core refactor):\n  ```python\n  # claude_analyzer.py (renaming optional; keep filename if referenced elsewhere)\n\n  from openai import OpenAI\n  from config_models import GPT_TEXT_MODEL\n\n  client = OpenAI()\n\n  def call_llm(prompt: str) -> str:\n      # DO NOT modify prompt text\n      resp = client.chat.completions.create(\n          model=GPT_TEXT_MODEL,\n          messages=[{\"role\": \"user\", \"content\": prompt}],\n          max_tokens=500,\n      )\n      return resp.choices[0].message.content\n  ```\n- Keep function names and signatures unchanged so all callers behave identically.\n- If any logging stores the model name, update logs to reflect `gpt-5-mini` without altering log formats consumed elsewhere.",
        "testStrategy": "- Unit/functional tests for claude_analyzer:\n  - Create or update tests that mock OpenAI client responses, asserting:\n    - The function sends the exact same `prompt` string content.\n    - `model` is set to `\"gpt-5-mini\"`.\n    - `messages` structure is `[{\"role\": \"user\", \"content\": prompt}]` (plus any existing system messages).\n    - The returned value equals `response.choices[0].message.content` from the mock.\n  - Add tests for multi-turn prompts if used.\n- Regression tests on existing flows using claude_analyzer:\n  - Run the full posting pipeline for a small sample and compare key behavior:\n    - Same number of calls.\n    - No change in control-flow (e.g., decision branches based on analyzer output still produce valid next steps).\n- Manual validation:\n  - For a few existing prompts, capture the old Claude output and the new GPT-5 mini output:\n    - Verify they are semantically equivalent for the downstream logic (accepting natural wording differences).\n  - Monitor logs for any parsing errors or unexpected `None`/empty strings.\n- Failure-path tests:\n  - Simulate OpenAI API error and ensure the same retry/fallback logic used with Anthropic is still invoked.",
        "priority": "high",
        "dependencies": [
          "64"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-19T07:00:52.377Z"
      },
      {
        "id": "66",
        "title": "Migrate post_reel_smart.py vision-based screenshot analysis to OpenAI GPT-5 mini vision API",
        "description": "Refactor vision-related calls in post_reel_smart.py to use the OpenAI GPT-5 mini vision capabilities, preserving all prompts and control logic while adapting to OpenAI’s multimodal message format and response parsing.",
        "details": "Implementation details:\n- Identify current vision usage in `post_reel_smart.py` (likely calling Claude vision via Anthropic SDK):\n  - Find functions that analyze screenshots or failure images.\n  - Note how images are currently passed (e.g., as bytes, file paths, or URLs) and how the prompt is structured.\n- Update to OpenAI multimodal chat format (current pattern is to pass images in `messages` content as a list of text+image parts):[6]\n  - Typical OpenAI multimodal message format:\n    ```python\n    response = client.chat.completions.create(\n        model=\"gpt-5-mini\",  # vision-capable variant\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},  # keep prompt text identical\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n                ],\n            }\n        ],\n        max_tokens=500,\n    )\n    text = response.choices[0].message.content\n    ```\n  - If images are local files, you may need to either:\n    - Upload them to a temporary secure URL, or\n    - Use the latest OpenAI SDK support for sending image bytes directly if available; follow the current SDK docs for the simplest supported method.[6]\n- Preserve prompts and logic:\n  - Do NOT alter the textual instructions; just change how the image is attached.\n  - Keep parameters like `max_tokens`, `temperature`, etc., aligned with previous Claude calls.\n- Implement a helper function in `post_reel_smart.py` (or a shared utility) for screenshot analysis:\n  ```python\n  from openai import OpenAI\n  from config_models import GPT_VISION_MODEL\n\n  client = OpenAI()\n\n  def analyze_screenshot(prompt: str, image_url: str, max_tokens: int = 500) -> str:\n      resp = client.chat.completions.create(\n          model=GPT_VISION_MODEL,\n          messages=[{\n              \"role\": \"user\",\n              \"content\": [\n                  {\"type\": \"text\", \"text\": prompt},\n                  {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n              ],\n          }],\n          max_tokens=max_tokens,\n      )\n      return resp.choices[0].message.content\n  ```\n- Replace all existing Claude vision calls in `post_reel_smart.py` with the helper, keeping signatures and caller behavior the same.\n- Response structure:\n  - As with text, expect `response.choices[0].message.content`.\n  - Ensure any downstream parsing that assumed Claude formatting is robust to natural language variation.\n- Performance & cost:\n  - Reuse the same GPT-5 mini model for both text and vision if supported (per PRD assumption), to keep configuration simple and cost predictable.\n- Error handling:\n  - Implement the same retry/fallback behavior used for previous vision calls.\n  - Guard against missing/invalid image URLs and log appropriately.",
        "testStrategy": "- Unit tests for the vision helper:\n  - Mock the OpenAI client’s `chat.completions.create` to verify:\n    - `model` is set to GPT_VISION_MODEL (`\"gpt-5-mini\"`).\n    - `messages[0][\"content\"]` is a list including both a text part with the exact prompt and an `image_url` part with the given URL.\n    - The function returns `response.choices[0].message.content`.\n- Integration tests in `post_reel_smart.py`:\n  - Run the failure-handling / screenshot-analysis path with a sample image (test or staging environment) and ensure:\n    - No exceptions from the OpenAI call.\n    - The control-flow (e.g., decisions based on analysis) remains valid.\n- Manual validation:\n  - Use a few representative failure screenshots to compare old Claude output vs new GPT-5 mini vision output for semantic equivalence.\n  - Check logs for any repeated errors or malformed payloads.\n- Negative-path tests:\n  - Simulate missing image URL or invalid image and ensure behavior matches the old implementation (same error messages or fallbacks where applicable).",
        "priority": "high",
        "dependencies": [
          "64",
          "65"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-19T07:05:00.391Z"
      },
      {
        "id": "67",
        "title": "Refine configuration, error handling, and logging for OpenAI calls",
        "description": "Standardize configuration, error handling, and logging around the new OpenAI GPT-5 mini text and vision calls to ensure reliable operation, debuggability, and cost visibility while preserving existing control-flow.",
        "details": "Implementation details:\n- Centralize configuration:\n  - Create or update a config module to hold:\n    - Model names (`GPT_TEXT_MODEL`, `GPT_VISION_MODEL`).\n    - Default `max_tokens`, `temperature`, and any other parameters needed by both `claude_analyzer.py` and `post_reel_smart.py`.\n  - Use these constants in both files to avoid drift.\n- Error handling:\n  - Wrap all OpenAI calls in a small common helper that:\n    ```python\n    import logging\n    from openai import APIError, RateLimitError\n\n    logger = logging.getLogger(__name__)\n\n    def safe_chat_completion(client, **kwargs):\n        try:\n            return client.chat.completions.create(**kwargs)\n        except RateLimitError as e:\n            logger.warning(\"OpenAI rate limit: %s\", e)\n            # propagate or trigger existing retry logic\n            raise\n        except APIError as e:\n            logger.error(\"OpenAI API error: %s\", e)\n            raise\n    ```\n  - Integrate with existing retry/backoff logic if present, keeping behavior equivalent to previous Anthropic handling.\n- Logging and cost visibility:\n  - Add structured logs around each OpenAI call including:\n    - Model name, type (text vs vision), and high-level purpose (e.g., `ui_analysis`, `failure_screenshot_analysis`).\n    - `max_tokens` and maybe a rough estimate of tokens used if the response object exposes it.\n  - Ensure logs do not include API keys or sensitive user content.\n- Timeout and robustness:\n  - If current Anthropic calls used timeouts, replicate them via HTTP client or SDK options, following best practices in current OpenAI docs.[6]\n- Maintain behavior parity:\n  - Any new abstractions must preserve existing signatures and return values so the posting flow logic is unchanged.\n- Pseudo-code for shared wrapper usage:\n  ```python\n  def call_text_llm(prompt: str) -> str:\n      resp = safe_chat_completion(\n          client,\n          model=GPT_TEXT_MODEL,\n          messages=[{\"role\": \"user\", \"content\": prompt}],\n          max_tokens=500,\n      )\n      return resp.choices[0].message.content\n  ```",
        "testStrategy": "- Unit tests for error wrapper:\n  - Mock `client.chat.completions.create` to raise `RateLimitError` and `APIError` and assert that:\n    - The exceptions propagate as expected by existing retry logic or higher-level handlers.\n    - Appropriate log messages are written (using a log-capture fixture).\n- Configuration tests:\n  - Verify that `GPT_TEXT_MODEL` and `GPT_VISION_MODEL` are used consistently in both `claude_analyzer.py` and `post_reel_smart.py`.\n  - If model IDs are changed in one place, tests should fail where hard-coded strings remain.\n- Observability tests:\n  - Run a short end-to-end test and inspect logs to confirm:\n    - Each OpenAI call logs model, call type, and basic metadata.\n    - No secrets or full prompts are logged unless already allowed by existing standards.\n- Regression tests:\n  - Ensure adding the wrapper does not change return values by comparing outputs from direct vs wrapped calls using mocks.",
        "priority": "medium",
        "dependencies": [
          "65",
          "66"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-19T07:06:02.719Z"
      },
      {
        "id": "68",
        "title": "End-to-end regression and cost verification for GPT-5 mini migration",
        "description": "Run full posting flows using the new GPT-5 mini text and vision integrations, verify behavior equivalence to the previous Claude Sonnet implementation, and confirm that estimated API costs meet the target reduction.",
        "details": "Implementation details:\n- End-to-end functional runs:\n  - Execute the complete posting flow for at least the same scale used in the PRD baseline (e.g., 127 successful posts) in a staging or controlled environment.\n  - Ensure:\n    - All 19 steps per successful post complete without errors.\n    - No new timeouts or unexpected failures appear in analyzer or vision steps.\n- Behavioral equivalence checks:\n  - For a representative subset of posts:\n    - Compare key intermediate decisions and outputs between the old Claude-based logs and the new GPT-5 mini logs.\n    - Confirm that “equivalent quality responses” are produced (semantic equivalence and no breakage of downstream parsing).\n- Cost verification:\n  - Using OpenAI usage metrics and your own logging, compute:\n    - Average tokens per API call (prompt + completion) around the target 2,200 tokens.\n    - Total tokens used for 127 successful posts.\n  - Estimate cost with GPT-5 mini pricing from the PRD (~$0.25/MTok) and confirm it is ≈$1.67 for 127 posts versus the previous ~$20.\n- Performance & scalability:\n  - Measure latency per call and ensure there is no unacceptable slowdown versus Claude Sonnet.\n  - If necessary, adjust concurrency settings while ensuring no logic changes.\n- Rollout plan:\n  - Prepare a simple feature flag or config toggle (if not already present) allowing switching between Anthropic and OpenAI during rollout, while defaulting to OpenAI once validated.\n  - Document how to roll back to Claude Sonnet quickly if issues arise, without touching prompt text.\n- Documentation updates:\n  - Update internal README/ops docs explaining:\n    - New environment variables (`OPENAI_API_KEY`).\n    - New model IDs used.\n    - Any new monitoring dashboards or log fields relevant to the migration.",
        "testStrategy": "- Automated regression suite:\n  - Run all existing tests after the migration and ensure they pass.\n  - Add integration tests that simulate the entire posting pipeline at least once end-to-end using mocked or low-cost real calls.\n- Manual QA:\n  - Have a human reviewer inspect a sample of generated posts and intermediate analyzer outputs to confirm quality equivalence.\n- Cost and performance validation:\n  - After running a batch close to 127 posts, export token usage and compute effective cost.\n  - Compare latency and error rates before vs after migration using logs or monitoring tools.\n- Rollout verification:\n  - Test the configuration toggle (if implemented) by switching between providers in staging and confirming that behavior flips accordingly without code changes.\n  - Once stable, enable OpenAI by default in production and monitor for 24–48 hours for any anomalies.",
        "priority": "medium",
        "dependencies": [
          "65",
          "66",
          "67"
        ],
        "status": "in-progress",
        "subtasks": [],
        "updatedAt": "2025-12-19T07:06:02.844Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-12-19T07:06:02.845Z",
      "taskCount": 5,
      "completedCount": 4,
      "tags": [
        "posting"
      ]
    }
  }
}